{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Path Setup\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add TrajectoryCrafter to Python path\n",
    "trajcrafter_path = \"/home/azhuravl/work/TrajectoryCrafter\"\n",
    "sys.path.insert(0, trajcrafter_path)\n",
    "\n",
    "# Change working directory to TrajectoryCrafter\n",
    "os.chdir(trajcrafter_path)\n",
    "\n",
    "# Now import TrajectoryCrafter modules\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames\n",
    "from models.infer import DepthCrafterDemo\n",
    "import inference_orbits\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Argument Setup\n",
    "# Create opts manually for notebook use\n",
    "parser = inference_orbits.get_parser()\n",
    "opts_base = parser.parse_args([\n",
    "    '--video_path', './test/videos/0-NNvgaTcVzAG0-r.mp4',  # Change this path\n",
    "    '--radius', '1.0',\n",
    "    '--device', 'cuda:0'\n",
    "])\n",
    "\n",
    "# Set common parameters\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.camera = \"target\"\n",
    "opts_base.mode = \"gradual\"\n",
    "opts_base.mask = True\n",
    "opts_base.target_pose = [0, 90, opts_base.radius, 0, 0]  # right_90 example\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_vis\"\n",
    "\n",
    "print(f\"Video: {opts_base.video_path}\")\n",
    "print(f\"Target pose: {opts_base.target_pose}\")\n",
    "print(f\"Device: {opts_base.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Classes\n",
    "class VisualizationWarper(Warper):\n",
    "    \"\"\"Extended Warper class for 3D visualization\"\"\"\n",
    "    \n",
    "    def extract_3d_points_with_colors(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "        subsample_step: int = 10\n",
    "    ):\n",
    "        \"\"\"Extract 3D world points and their corresponding colors for visualization\"\"\"\n",
    "        b, c, h, w = frame1.shape\n",
    "        \n",
    "        # Move tensors to device\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation1 = transformation1.to(self.device).to(self.dtype)\n",
    "        intrinsic1 = intrinsic1.to(self.device).to(self.dtype)\n",
    "        \n",
    "        # Create subsampled pixel coordinates for performance\n",
    "        x_coords = torch.arange(0, w, subsample_step, dtype=torch.float32)\n",
    "        y_coords = torch.arange(0, h, subsample_step, dtype=torch.float32)\n",
    "        x2d, y2d = torch.meshgrid(x_coords, y_coords, indexing='xy')\n",
    "        x2d = x2d.to(depth1.device)\n",
    "        y2d = y2d.to(depth1.device)\n",
    "        ones_2d = torch.ones_like(x2d)\n",
    "        \n",
    "        # Stack into homogeneous coordinates\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[None, :, :, :, None]\n",
    "        \n",
    "        # Subsample depth and colors\n",
    "        depth_sub = depth1[:, 0, ::subsample_step, ::subsample_step]\n",
    "        colors_sub = frame1[:, :, ::subsample_step, ::subsample_step]\n",
    "        \n",
    "        # Unproject to 3D camera coordinates\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]\n",
    "        depth_4d = depth_sub[:, :, :, None, None]\n",
    "        \n",
    "        unnormalized_pos = torch.matmul(intrinsic1_inv_4d, pos_vectors_homo)\n",
    "        camera_points = depth_4d * unnormalized_pos\n",
    "        \n",
    "        # Transform to world coordinates\n",
    "        ones_4d = torch.ones(b, camera_points.shape[1], camera_points.shape[2], 1, 1).to(depth1)\n",
    "        world_points_homo = torch.cat([camera_points, ones_4d], dim=3)\n",
    "        trans_4d = transformation1[:, None, None]\n",
    "        world_points_homo = torch.matmul(trans_4d, world_points_homo)\n",
    "        world_points = world_points_homo[:, :, :, :3, 0]  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Prepare colors\n",
    "        colors = colors_sub.permute(0, 2, 3, 1)  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Filter valid points (positive depth)\n",
    "        valid_mask = depth_sub > 0  # (b, h_sub, w_sub)\n",
    "        \n",
    "        # Flatten and filter\n",
    "        points_3d = world_points[valid_mask]  # (N, 3)\n",
    "        colors_rgb = colors[valid_mask]       # (N, 3)\n",
    "        \n",
    "        return points_3d, colors_rgb\n",
    "\n",
    "\n",
    "class TrajCrafterVisualization(TrajCrafter):\n",
    "    \"\"\"Lightweight TrajCrafter subclass for camera trajectory visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, opts):\n",
    "        # Only initialize what we need for pose generation and depth estimation\n",
    "        self.device = opts.device\n",
    "        self.depth_estimater = DepthCrafterDemo(\n",
    "            unet_path=opts.unet_path,\n",
    "            pre_train_path=opts.pre_train_path,\n",
    "            cpu_offload=opts.cpu_offload,\n",
    "            device=opts.device,\n",
    "        )\n",
    "        print(\"TrajCrafterVisualization initialized (diffusion pipeline skipped)\")\n",
    "    \n",
    "    def extract_scene_data(self, opts):\n",
    "        \"\"\"Extract all data needed for 3D visualization\"\"\"\n",
    "        print(\"Reading video frames...\")\n",
    "        frames = read_video_frames(\n",
    "            opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "        )\n",
    "        \n",
    "        print(\"Estimating depth...\")\n",
    "        depths = self.depth_estimater.infer(\n",
    "            frames,\n",
    "            opts.near,\n",
    "            opts.far,\n",
    "            opts.depth_inference_steps,\n",
    "            opts.depth_guidance_scale,\n",
    "            window_size=opts.window_size,\n",
    "            overlap=opts.overlap,\n",
    "        ).to(opts.device)\n",
    "        \n",
    "        print(\"Converting frames to tensors...\")\n",
    "        frames_tensor = (\n",
    "            torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "        )\n",
    "        \n",
    "        print(\"Generating camera poses...\")\n",
    "        pose_s, pose_t, K = self.get_poses(opts, depths, num_frames=opts.video_length)\n",
    "        \n",
    "        # Calculate scene radius\n",
    "        radius = (\n",
    "            depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "            * opts.radius_scale\n",
    "        )\n",
    "        radius = min(radius, 5)\n",
    "        \n",
    "        return {\n",
    "            'frames_numpy': frames,\n",
    "            'frames_tensor': frames_tensor,\n",
    "            'depths': depths,\n",
    "            'pose_source': pose_s,\n",
    "            'pose_target': pose_t,\n",
    "            'intrinsics': K,\n",
    "            'radius': radius,\n",
    "            'trajectory_params': opts.target_pose if hasattr(opts, 'target_pose') else None\n",
    "        }\n",
    "\n",
    "print(\"Classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run Visualization\n",
    "# Initialize visualization TrajCrafter\n",
    "print(\"Initializing TrajCrafter for visualization...\")\n",
    "vis_crafter = TrajCrafterVisualization(opts_base)\n",
    "\n",
    "# Extract scene data\n",
    "print(\"Extracting scene data...\")\n",
    "scene_data = vis_crafter.extract_scene_data(opts_base)\n",
    "\n",
    "# Create warper for 3D point extraction\n",
    "print(\"Creating 3D point cloud from all frames...\")\n",
    "vis_warper = VisualizationWarper(device=opts_base.device)\n",
    "\n",
    "# Extract points from all frames\n",
    "all_points_3d = []\n",
    "all_colors_rgb = []\n",
    "\n",
    "num_frames = scene_data['frames_tensor'].shape[0]\n",
    "for i in tqdm(range(num_frames), desc=\"Processing frames\"):\n",
    "    frame_data = {\n",
    "        'frame': scene_data['frames_tensor'][i:i+1],\n",
    "        'depth': scene_data['depths'][i:i+1], \n",
    "        'pose_source': scene_data['pose_source'][i:i+1],\n",
    "        'intrinsics': scene_data['intrinsics'][i:i+1],\n",
    "    }\n",
    "    \n",
    "    points_3d_frame, colors_rgb_frame = vis_warper.extract_3d_points_with_colors(\n",
    "        frame_data['frame'],\n",
    "        frame_data['depth'], \n",
    "        frame_data['pose_source'],\n",
    "        frame_data['intrinsics'],\n",
    "        subsample_step=20  # Increased for performance with multiple frames\n",
    "    )\n",
    "    \n",
    "    if points_3d_frame.shape[0] > 0:  # Only add if we have valid points\n",
    "        all_points_3d.append(points_3d_frame)\n",
    "        all_colors_rgb.append(colors_rgb_frame)\n",
    "\n",
    "# Concatenate all points\n",
    "if all_points_3d:\n",
    "    points_3d = torch.cat(all_points_3d, dim=0)\n",
    "    colors_rgb = torch.cat(all_colors_rgb, dim=0)\n",
    "    print(f\"Generated {points_3d.shape[0]} 3D points from {len(all_points_3d)} frames\")\n",
    "else:\n",
    "    print(\"No valid 3D points extracted!\")\n",
    "    points_3d = None\n",
    "    colors_rgb = None\n",
    "\n",
    "print(f\"Camera trajectory: {scene_data['pose_target'].shape[0]} poses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Create Viser Server (run once)\n",
    "import viser\n",
    "\n",
    "# Create server once\n",
    "viser_server = viser.ViserServer(port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viser_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Update Viser Content with Camera Direction Arrows (Previous working version)\n",
    "def update_viser_content(server, scene_data, points_3d, colors_rgb, max_points=50000):\n",
    "    \"\"\"Update viser server content without recreating server\"\"\"\n",
    "    \n",
    "    # Clear existing content\n",
    "    server.scene.reset()\n",
    "    \n",
    "    # Add point cloud with max points limit\n",
    "    if points_3d is not None and colors_rgb is not None:\n",
    "        points_np = points_3d.cpu().numpy()\n",
    "        colors_np = colors_rgb.cpu().numpy()\n",
    "        \n",
    "        # Limit number of points\n",
    "        if len(points_np) > max_points:\n",
    "            indices = np.random.choice(len(points_np), max_points, replace=False)\n",
    "            points_np = points_np[indices]\n",
    "            colors_np = colors_np[indices]\n",
    "        \n",
    "        if colors_np.min() < 0:\n",
    "            colors_np = (colors_np + 1) / 2\n",
    "        server.scene.add_point_cloud(\n",
    "            \"/scene_points\", \n",
    "            points=points_np, \n",
    "            colors=colors_np, \n",
    "            point_size=0.05\n",
    "        )\n",
    "    \n",
    "    poses_np = scene_data['pose_target'].cpu().numpy()\n",
    "    positions = poses_np[:, :3, 3]\n",
    "    \n",
    "    # Add trajectory\n",
    "    server.scene.add_spline_catmull_rom(\n",
    "        \"/trajectory\", \n",
    "        positions=positions, \n",
    "        color=(1.0, 0.0, 0.0), \n",
    "        line_width=3.0\n",
    "    )\n",
    "    \n",
    "    # Convert radius to numpy float\n",
    "    arrow_length = float(scene_data['radius']) * 0.4\n",
    "    \n",
    "    for i, pose in enumerate(poses_np[::5]):\n",
    "        position = pose[:3, 3]\n",
    "        rotation_matrix = pose[:3, :3]\n",
    "        \n",
    "        # Flip Z-axis (180° rotation around Y) - your working correction\n",
    "        flip_z = np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]])\n",
    "        corrected_rotation = rotation_matrix @ flip_z\n",
    "        wxyz = viser.transforms.SO3.from_matrix(corrected_rotation).wxyz\n",
    "        \n",
    "        # Add camera frustum\n",
    "        server.scene.add_camera_frustum(\n",
    "            f\"/camera_{i}\",\n",
    "            fov=60,\n",
    "            aspect=16/9,\n",
    "            scale=0.2,\n",
    "            position=position,\n",
    "            wxyz=wxyz,\n",
    "            color=(0.8, 0.2, 0.2)\n",
    "        )\n",
    "        \n",
    "        # Calculate camera look direction (negative Z after correction)\n",
    "        original_forward = rotation_matrix[:, 2]  # Z column\n",
    "        look_direction = -original_forward\n",
    "        \n",
    "        # Create arrow endpoint\n",
    "        arrow_end = position + look_direction * arrow_length\n",
    "        \n",
    "        # Add direction arrow using thick line\n",
    "        server.scene.add_spline_catmull_rom(\n",
    "            f\"/camera_direction_{i}\",\n",
    "            positions=np.array([position, arrow_end]),\n",
    "            color=(0.0, 1.0, 1.0),  # Cyan for visibility\n",
    "            line_width=6.0\n",
    "        )\n",
    "        \n",
    "        # Add arrowhead using a small sphere (simpler than cone)\n",
    "        server.scene.add_icosphere(\n",
    "            f\"/camera_arrowhead_{i}\",\n",
    "            radius=0.05,\n",
    "            position=arrow_end,\n",
    "            color=(0.0, 1.0, 1.0)  # Same cyan color\n",
    "        )\n",
    "        \n",
    "        # Optional: Add a smaller sphere at camera position for clarity\n",
    "        server.scene.add_icosphere(\n",
    "            f\"/camera_center_{i}\",\n",
    "            radius=0.03,\n",
    "            position=position,\n",
    "            color=(1.0, 0.5, 0.0)  # Orange\n",
    "        )\n",
    "    \n",
    "    # Add markers\n",
    "    server.scene.add_icosphere(\n",
    "        \"/start\", \n",
    "        radius=0.1, \n",
    "        position=positions[0], \n",
    "        color=(0.0, 1.0, 0.0)\n",
    "    )\n",
    "    server.scene.add_icosphere(\n",
    "        \"/end\", \n",
    "        radius=0.1, \n",
    "        position=positions[-1], \n",
    "        color=(1.0, 0.0, 1.0)\n",
    "    )\n",
    "    server.scene.add_frame(\n",
    "        \"/world\", \n",
    "        axes_length=0.5, \n",
    "        axes_radius=0.02,\n",
    "        position=(0, 0, 0),\n",
    "        wxyz=(1, 0, 0, 0)\n",
    "    )\n",
    "    \n",
    "    # Print info\n",
    "    path_length = np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1))\n",
    "    print(f\"Updated scene: {points_3d.shape[0] if points_3d is not None else 0} points, {len(poses_np)} poses\")\n",
    "    print(f\"Trajectory length: {path_length:.3f}, Arrow length: {arrow_length:.3f}\")\n",
    "    if scene_data['trajectory_params']:\n",
    "        dtheta, dphi, dr, dx, dy = scene_data['trajectory_params']\n",
    "        print(f\"Motion: θ={dtheta}°, φ={dphi}°, r={dr}, x={dx}, y={dy}\")\n",
    "\n",
    "# Update the content\n",
    "update_viser_content(viser_server, scene_data, points_3d, colors_rgb, max_points=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Point Size Control\n",
    "@viser_server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    \n",
    "    # Add simple point size slider\n",
    "    point_size_slider = client.gui.add_slider(\n",
    "        \"Point Size\",\n",
    "        min=0.005,\n",
    "        max=0.1,\n",
    "        step=0.005,\n",
    "        initial_value=0.015,\n",
    "    )\n",
    "    \n",
    "    # Update point size when slider changes\n",
    "    @point_size_slider.on_update\n",
    "    def _(_) -> None:\n",
    "        if points_3d is not None:\n",
    "            points_np = points_3d.cpu().numpy()\n",
    "            colors_np = colors_rgb.cpu().numpy()\n",
    "            if colors_np.min() < 0:\n",
    "                colors_np = (colors_np + 1) / 2\n",
    "            viser_server.scene.add_point_cloud(\n",
    "                \"/scene_points\",\n",
    "                points=points_np,\n",
    "                colors=colors_np,\n",
    "                point_size=point_size_slider.value\n",
    "            )\n",
    "\n",
    "print(\"Point size control added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Clean Server Restart\n",
    "import viser\n",
    "import asyncio\n",
    "\n",
    "# Clean up any existing server\n",
    "if 'viser_server' in globals():\n",
    "    try:\n",
    "        viser_server.stop()\n",
    "        print(\"Stopped existing server\")\n",
    "    except:\n",
    "        pass\n",
    "    del viser_server\n",
    "\n",
    "# Clear any existing event loops\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_closed():\n",
    "        asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "except:\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "\n",
    "# Create fresh server\n",
    "viser_server = viser.ViserServer(port=8080)\n",
    "print(f\"Fresh viser server created at http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: To test different trajectories (example)\n",
    "# Change your trajectory and re-run the update\n",
    "opts_test = copy.deepcopy(opts_base)\n",
    "opts_test.target_pose = [0, 90, 1, 0, 0]  # 180° rotation\n",
    "\n",
    "# Generate new scene data\n",
    "scene_data_test = vis_crafter.extract_scene_data(opts_test)\n",
    "\n",
    "# Update the same server with new content\n",
    "update_viser_content(viser_server, scene_data_test, points_3d, colors_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import viser\n",
    "\n",
    "# Add sliders for camera control\n",
    "theta_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Theta (deg)\",\n",
    "    min=0, max=360, step=1, initial_value=0,\n",
    ")\n",
    "\n",
    "phi_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Phi (deg)\", \n",
    "    min=-90, max=90, step=1, initial_value=0,\n",
    ")\n",
    "\n",
    "radius_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Distance\",\n",
    "    min=1, max=10, step=0.1, initial_value=5,\n",
    ")\n",
    "\n",
    "roll_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Roll (deg)\",\n",
    "    min=-180, max=180, step=1, initial_value=0,\n",
    ")\n",
    "\n",
    "def update_camera_position():\n",
    "    theta = math.radians(theta_slider.value)\n",
    "    phi = math.radians(phi_slider.value)\n",
    "    r = radius_slider.value\n",
    "    roll = math.radians(roll_slider.value)\n",
    "    \n",
    "    # Convert spherical to cartesian\n",
    "    x = r * math.cos(phi) * math.cos(theta)\n",
    "    y = r * math.cos(phi) * math.sin(theta) \n",
    "    z = r * math.sin(phi)\n",
    "    \n",
    "    position = np.array([x, y, z])\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    \n",
    "    # Calculate camera's forward direction\n",
    "    forward = (look_at - position)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    # Calculate initial right and up vectors\n",
    "    world_up = np.array([0, 0, 1])\n",
    "    right = np.cross(forward, world_up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    up_initial = np.cross(right, forward)\n",
    "    up_initial = up_initial / np.linalg.norm(up_initial)\n",
    "    \n",
    "    # Apply roll rotation around the forward axis\n",
    "    cos_roll = np.cos(roll)\n",
    "    sin_roll = np.sin(roll)\n",
    "    up = cos_roll * up_initial + sin_roll * right\n",
    "    \n",
    "    # Set camera using the correct API\n",
    "    for client in viser_server.get_clients().values():\n",
    "        client.camera.position = position\n",
    "        client.camera.look_at = look_at\n",
    "        client.camera.up_direction = up\n",
    "\n",
    "@theta_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "    \n",
    "@phi_slider.on_update \n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "    \n",
    "@radius_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "\n",
    "@roll_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
