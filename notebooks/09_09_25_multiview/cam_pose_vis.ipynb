{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Path Setup\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add TrajectoryCrafter to Python path\n",
    "trajcrafter_path = \"/home/azhuravl/work/TrajectoryCrafter\"\n",
    "sys.path.insert(0, trajcrafter_path)\n",
    "\n",
    "# Change working directory to TrajectoryCrafter\n",
    "os.chdir(trajcrafter_path)\n",
    "\n",
    "# Now import TrajectoryCrafter modules\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames\n",
    "from models.infer import DepthCrafterDemo\n",
    "import inference_orbits\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Camera Setup for run_w_cam_poses workflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Create opts manually for notebook use (matching run_w_cam_poses structure)\n",
    "class Opts:\n",
    "    def __init__(self):\n",
    "        # Video settings\n",
    "        self.video_path = '/home/azhuravl/work/panoptic-toolbox/150821_dance4/vgaVideos/vga_05_08.mp4'\n",
    "        self.video_length = 49\n",
    "        self.fps = 10\n",
    "        self.stride = 1\n",
    "        self.max_res = 1024\n",
    "        \n",
    "        # Device\n",
    "        self.device = 'cuda:0'\n",
    "        self.weight_dtype = torch.bfloat16\n",
    "        \n",
    "        # Output\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        video_basename = os.path.splitext(os.path.basename(self.video_path))[0]\n",
    "        self.exp_name = f\"{video_basename}_{timestamp}_cam_pose_vis\"\n",
    "        \n",
    "        # Depth\n",
    "        self.near = 0.0001\n",
    "        self.far = 10000.0\n",
    "        self.depth_inference_steps = 5\n",
    "        self.depth_guidance_scale = 1.0\n",
    "        self.window_size = 110\n",
    "        self.overlap = 25\n",
    "        \n",
    "        # Camera\n",
    "        self.mask = False\n",
    "        self.seed = 43\n",
    "        \n",
    "        # Paths (matching run_w_cam_poses)\n",
    "        self.blip_path = \"checkpoints/blip2-opt-2.7b\"\n",
    "        self.unet_path = \"checkpoints/DepthCrafter\"\n",
    "        self.pre_train_path = \"checkpoints/stable-video-diffusion-img2vid\"\n",
    "        self.cpu_offload = 'model'\n",
    "\n",
    "opts_base = Opts()\n",
    "\n",
    "# Define source and target cameras (matching your run_w_cam_poses example)\n",
    "source_camera = {\n",
    "    \"name\": \"05_08\",\n",
    "    \"type\": \"vga\",\n",
    "    \"resolution\": [640,480],\n",
    "    \"panel\": 5,\n",
    "    \"node\": 8,\n",
    "    \"K\": [\n",
    "        [748.194573,0.403304,388.156644],\n",
    "        [0,747.455308,257.075025],\n",
    "        [0,0,1]\n",
    "    ],\n",
    "    \"distCoef\": [-0.352118,0.186737,0,0,-0.119772],\n",
    "    \"R\": [\n",
    "        [-0.871497831,-0.004279560553,0.4903806847],\n",
    "        [0.09322575792,0.980281239,0.1742344701],\n",
    "        [-0.4814566321,0.1975610738,-0.8539140083]\n",
    "    ],\n",
    "    \"t\": [\n",
    "        [-0.03934843898], #[-39.34843898],\n",
    "        [0.09250008112], #[92.50008112],\n",
    "        [0.3049007109] #[304.9007109]\n",
    "    ]\n",
    "}\n",
    "\n",
    "target_camera = {\n",
    "    \"name\": \"01_01\",\n",
    "    \"type\": \"vga\",\n",
    "    \"resolution\": [640,480],\n",
    "    \"panel\": 1,\n",
    "    \"node\": 1,\n",
    "    \"K\": [\n",
    "        [748.561374,0.083459,378.041653],\n",
    "        [0,748.351299,223.336713],\n",
    "        [0,0,1]\n",
    "    ],\n",
    "    \"distCoef\": [-0.32211,0.02854,0,0,0.101902],\n",
    "    \"R\": [\n",
    "        [-0.9610410199,0.02955079861,-0.2748215937],\n",
    "        [0.005847346208,0.9962196747,0.08667276504],\n",
    "        [0.2763439281,0.08168910551,-0.957580766]\n",
    "    ],\n",
    "    \"t\": [\n",
    "        [-0.04625903829], #[-46.25903829],\n",
    "        [0.1435237551], #[143.5237551],\n",
    "        [0.2871962273] #[287.1962273]\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Video: {opts_base.video_path}\")\n",
    "print(f\"Source camera: {source_camera['name']} (panel {source_camera['panel']}, node {source_camera['node']})\")\n",
    "print(f\"Target camera: {target_camera['name']} (panel {target_camera['panel']}, node {target_camera['node']})\")\n",
    "print(f\"Device: {opts_base.device}\")\n",
    "print(f\"Video length: {opts_base.video_length} frames\")\n",
    "print(f\"Experiment name: {opts_base.exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from run_w_cam_poses import CameraPoseTrajCrafter\n",
    "\n",
    "# Cell 3: Visualization Classes\n",
    "class VisualizationWarper(Warper):\n",
    "    \"\"\"Extended Warper class for 3D visualization\"\"\"\n",
    "    \n",
    "    def extract_3d_points_with_colors(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        depth1: torch.Tensor,\n",
    "        transformation_source: torch.Tensor,\n",
    "        transformation_target: torch.Tensor,\n",
    "        intrinsic_source: torch.Tensor,\n",
    "        intrinsic_target: Optional[torch.Tensor] = None,\n",
    "        subsample_step: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract 3D world points and their corresponding colors for visualization\n",
    "        \n",
    "        Args:\n",
    "            frame1: (b, c, h, w) input frame\n",
    "            depth1: (b, 1, h, w) depth map\n",
    "            transformation_source: (b, 4, 4) source camera extrinsic matrix [R, t; 0, 1]\n",
    "            transformation_target: (b, 4, 4) target camera extrinsic matrix [R, t; 0, 1]\n",
    "            intrinsic_source: (b, 3, 3) source camera intrinsic matrix\n",
    "            intrinsic_target: (b, 3, 3) target camera intrinsic matrix (optional, defaults to source)\n",
    "            subsample_step: int, subsampling step for performance\n",
    "        \"\"\"\n",
    "        b, c, h, w = frame1.shape\n",
    "        \n",
    "        # Move tensors to device\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation_source = transformation_source.to(self.device).to(self.dtype)\n",
    "        transformation_target = transformation_target.to(self.device).to(self.dtype)\n",
    "        intrinsic_source = intrinsic_source.to(self.device).to(self.dtype)\n",
    "        \n",
    "        if intrinsic_target is None:\n",
    "            intrinsic_target = intrinsic_source.clone()\n",
    "        intrinsic_target = intrinsic_target.to(self.device).to(self.dtype)\n",
    "        \n",
    "        # Create subsampled pixel coordinates for performance\n",
    "        x_coords = torch.arange(0, w, subsample_step, dtype=torch.float32)\n",
    "        y_coords = torch.arange(0, h, subsample_step, dtype=torch.float32)\n",
    "        x2d, y2d = torch.meshgrid(x_coords, y_coords, indexing='xy')\n",
    "        x2d = x2d.to(depth1.device)\n",
    "        y2d = y2d.to(depth1.device)\n",
    "        ones_2d = torch.ones_like(x2d)\n",
    "        \n",
    "        # Stack into homogeneous coordinates\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[None, :, :, :, None]  # (1, h_sub, w_sub, 3, 1)\n",
    "        \n",
    "        # Subsample depth and colors\n",
    "        depth_sub = depth1[:, 0, ::subsample_step, ::subsample_step]  # (b, h_sub, w_sub)\n",
    "        colors_sub = frame1[:, :, ::subsample_step, ::subsample_step]  # (b, c, h_sub, w_sub)\n",
    "        \n",
    "        # Unproject to 3D camera coordinates (source camera space)\n",
    "        intrinsic_source_inv = torch.linalg.inv(intrinsic_source)  # (b, 3, 3)\n",
    "        intrinsic_source_inv_4d = intrinsic_source_inv[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        depth_4d = depth_sub[:, :, :, None, None]  # (b, h_sub, w_sub, 1, 1)\n",
    "        \n",
    "        # Get 3D points in source camera coordinate system\n",
    "        unnormalized_pos = torch.matmul(intrinsic_source_inv_4d, pos_vectors_homo)  # (b, h_sub, w_sub, 3, 1)\n",
    "        camera_points_source = depth_4d * unnormalized_pos  # (b, h_sub, w_sub, 3, 1)\n",
    "        \n",
    "        # Transform to world coordinates using source camera transformation\n",
    "        ones_4d = torch.ones(b, camera_points_source.shape[1], camera_points_source.shape[2], 1, 1, device=depth1.device)\n",
    "        camera_points_source_homo = torch.cat([camera_points_source, ones_4d], dim=3)  # (b, h_sub, w_sub, 4, 1)\n",
    "        transformation_source_4d = transformation_source[:, None, None]  # (b, 1, 1, 4, 4)\n",
    "        \n",
    "        # Transform from source camera space to world space\n",
    "        world_points_homo = torch.matmul(transformation_source_4d, camera_points_source_homo)  # (b, h_sub, w_sub, 4, 1)\n",
    "        world_points = world_points_homo[:, :, :, :3, 0]  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Prepare colors (convert from [-1,1] to [0,1] if needed)\n",
    "        colors = colors_sub.permute(0, 2, 3, 1)  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Filter valid points (positive depth)\n",
    "        valid_mask = depth_sub > 0  # (b, h_sub, w_sub)\n",
    "        \n",
    "        # Flatten and filter\n",
    "        points_3d = world_points[valid_mask]  # (N, 3)\n",
    "        colors_rgb = colors[valid_mask]       # (N, 3)\n",
    "        \n",
    "        return points_3d, colors_rgb\n",
    "\n",
    "\n",
    "class TrajCrafterVisualization(CameraPoseTrajCrafter):\n",
    "    \"\"\"Lightweight TrajCrafter subclass for camera trajectory visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, opts):\n",
    "        # Only initialize what we need for pose generation and depth estimation\n",
    "        self.device = opts.device\n",
    "        self.depth_estimater = DepthCrafterDemo(\n",
    "            unet_path=opts.unet_path,\n",
    "            pre_train_path=opts.pre_train_path,\n",
    "            cpu_offload=opts.cpu_offload,\n",
    "            device=opts.device,\n",
    "        )\n",
    "        print(\"TrajCrafterVisualization initialized (diffusion pipeline skipped)\")\n",
    "    \n",
    "    def extract_scene_data(self, opts, source_camera, target_camera):\n",
    "        \"\"\"Extract all data needed for 3D visualization following CameraPoseTrajCrafter workflow\"\"\"\n",
    "        \n",
    "        print(\"Reading video frames...\")\n",
    "        frames = self.read_video_frames(\n",
    "            opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "        )\n",
    "        \n",
    "        # Pad frames if necessary\n",
    "        if frames.shape[0] < opts.video_length:\n",
    "            last_frame = frames[-1:]\n",
    "            num_pad = opts.video_length - frames.shape[0]\n",
    "            pad_frames = np.repeat(last_frame, num_pad, axis=0)\n",
    "            frames = np.concatenate([frames, pad_frames], axis=0)\n",
    "            print(f\"Padding video from {frames.shape[0]} to {opts.video_length} frames\")\n",
    "            \n",
    "        # Undistort the frames using source camera distortion coefficients\n",
    "        print(\"Undistorting frames using source camera distortion coefficients...\")\n",
    "        frames, undistorted_K = self.undistort_frames(frames, source_camera)\n",
    "        \n",
    "        # Update source camera with undistorted intrinsics\n",
    "        source_camera_undistorted = source_camera.copy()\n",
    "        source_camera_undistorted[\"K\"] = undistorted_K.tolist()\n",
    "        source_camera_undistorted[\"distCoef\"] = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "        print(\"Estimating depth...\")\n",
    "        depths = self.depth_estimater.infer(\n",
    "            frames,\n",
    "            opts.near,\n",
    "            opts.far,\n",
    "            opts.depth_inference_steps,\n",
    "            opts.depth_guidance_scale,\n",
    "            window_size=opts.window_size,\n",
    "            overlap=opts.overlap,\n",
    "        ).to(opts.device)\n",
    "        \n",
    "        print(\"Converting frames to tensors...\")\n",
    "        frames_tensor = (\n",
    "            torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "        )\n",
    "        \n",
    "        print(\"Converting camera poses...\")\n",
    "        # Use undistorted source camera and target camera\n",
    "        source_c2w, source_K = self.convert_camera_format(source_camera_undistorted)\n",
    "        target_c2w, target_K = self.convert_camera_format(target_camera)\n",
    "\n",
    "        pose_s = source_c2w.to(opts.device).unsqueeze(0).repeat(opts.video_length, 1, 1)\n",
    "        pose_t = target_c2w.to(opts.device).unsqueeze(0).repeat(opts.video_length, 1, 1)\n",
    "        \n",
    "        # Use target intrinsics for all frames (following your workflow)\n",
    "        K_matrices_s = source_K.to(opts.device).unsqueeze(0).repeat(opts.video_length, 1, 1)\n",
    "        K_matrices_t = target_K.to(opts.device).unsqueeze(0).repeat(opts.video_length, 1, 1)\n",
    "        print(f\"Using target camera intrinsics for all frames.\")\n",
    "        \n",
    "        return {\n",
    "            'frames_numpy': frames,\n",
    "            'frames_tensor': frames_tensor,\n",
    "            'depths': depths,\n",
    "            'pose_source': pose_s,\n",
    "            'pose_target': pose_t,\n",
    "            'intrinsics_source': K_matrices_s,\n",
    "            'intrinsics_target': K_matrices_t,\n",
    "            'source_camera_undistorted': source_camera_undistorted,\n",
    "            'target_camera': target_camera,\n",
    "            'video_length': opts.video_length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run Visualization\n",
    "# Initialize visualization TrajCrafter\n",
    "print(\"Initializing TrajCrafter for visualization...\")\n",
    "vis_crafter = TrajCrafterVisualization(opts_base)\n",
    "\n",
    "# Extract scene data\n",
    "print(\"Extracting scene data...\")\n",
    "scene_data = vis_crafter.extract_scene_data(opts_base, source_camera, target_camera)  # Added camera parameters\n",
    "\n",
    "# Create warper for 3D point extraction\n",
    "print(\"Creating 3D point cloud from all frames...\")\n",
    "vis_warper = VisualizationWarper(device=opts_base.device)\n",
    "\n",
    "# Extract points from all frames\n",
    "all_points_3d = []\n",
    "all_colors_rgb = []\n",
    "\n",
    "num_frames = scene_data['frames_tensor'].shape[0]\n",
    "for i in tqdm(range(num_frames), desc=\"Processing frames\"):\n",
    "    frame_data = {\n",
    "        'frame': scene_data['frames_tensor'][i:i+1],\n",
    "        'depth': scene_data['depths'][i:i+1], \n",
    "        'pose_source': scene_data['pose_source'][i:i+1],\n",
    "        'pose_target': scene_data['pose_target'][i:i+1],  # Added target pose\n",
    "        'intrinsics_source': scene_data['intrinsics_source'][i:i+1],  # Updated key name\n",
    "        'intrinsics_target': scene_data['intrinsics_target'][i:i+1],  # Added target intrinsics\n",
    "    }\n",
    "    \n",
    "    # Use the updated method with both source and target cameras\n",
    "    points_3d_frame, colors_rgb_frame = vis_warper.extract_3d_points_with_colors(\n",
    "        frame_data['frame'],\n",
    "        frame_data['depth'], \n",
    "        frame_data['pose_source'],      # Source camera transformation\n",
    "        frame_data['pose_target'],      # Target camera transformation\n",
    "        frame_data['intrinsics_source'], # Source camera intrinsics\n",
    "        frame_data['intrinsics_target'], # Target camera intrinsics\n",
    "        subsample_step=20  # Increased for performance with multiple frames\n",
    "    )\n",
    "    \n",
    "    if points_3d_frame.shape[0] > 0:  # Only add if we have valid points\n",
    "        all_points_3d.append(points_3d_frame)\n",
    "        all_colors_rgb.append(colors_rgb_frame)\n",
    "\n",
    "# Concatenate all points\n",
    "if all_points_3d:\n",
    "    points_3d = torch.cat(all_points_3d, dim=0)\n",
    "    colors_rgb = torch.cat(all_colors_rgb, dim=0)\n",
    "    print(f\"Generated {points_3d.shape[0]} 3D points from {len(all_points_3d)} frames\")\n",
    "else:\n",
    "    print(\"No valid 3D points extracted!\")\n",
    "    points_3d = None\n",
    "    colors_rgb = None\n",
    "\n",
    "print(f\"Source camera trajectory: {scene_data['pose_source'].shape[0]} poses\")\n",
    "print(f\"Target camera trajectory: {scene_data['pose_target'].shape[0]} poses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viser\n",
    "import threading\n",
    "import time\n",
    "\n",
    "print(\"Creating animated viser visualization...\")\n",
    "\n",
    "# Cell 1: Create Viser Server (run once)\n",
    "# Check if server already exists and stop it\n",
    "try:\n",
    "    if 'viser_server' in globals() and viser_server is not None:\n",
    "        print(\"Stopping existing server...\")\n",
    "        viser_server.stop()\n",
    "        del viser_server\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new server\n",
    "print(\"Creating new Viser server on port 8080...\")\n",
    "viser_server = viser.ViserServer(port=8080)\n",
    "print(\"Server started successfully!\")\n",
    "\n",
    "# Cell 2: Animated Viser Content for run_w_cam_poses\n",
    "def setup_viser_scene(server, scene_data):\n",
    "    \"\"\"Setup static scene elements (trajectories and camera poses)\"\"\"\n",
    "    \n",
    "    # Source camera trajectory (green)\n",
    "    source_poses_np = scene_data['pose_source'].cpu().numpy()\n",
    "    source_positions = source_poses_np[:, :3, 3]\n",
    "    \n",
    "    # Target camera trajectory (red) \n",
    "    target_poses_np = scene_data['pose_target'].cpu().numpy()\n",
    "    target_positions = target_poses_np[:, :3, 3]\n",
    "    \n",
    "    # Add trajectories (static)\n",
    "    server.scene.add_spline_catmull_rom(\n",
    "        \"/source_trajectory\", \n",
    "        positions=source_positions, \n",
    "        color=(0.0, 1.0, 0.0),  # Green\n",
    "        line_width=3.0\n",
    "    )\n",
    "    \n",
    "    server.scene.add_spline_catmull_rom(\n",
    "        \"/target_trajectory\", \n",
    "        positions=target_positions, \n",
    "        color=(1.0, 0.0, 0.0),  # Red\n",
    "        line_width=3.0\n",
    "    )\n",
    "    \n",
    "    # Add source camera poses (static, green, every 5th to reduce clutter)\n",
    "    for i, pose in enumerate(source_poses_np[::5]):\n",
    "        position = pose[:3, 3]\n",
    "        rotation_matrix = pose[:3, :3]\n",
    "        \n",
    "        # Convert rotation to quaternion\n",
    "        wxyz = viser.transforms.SO3.from_matrix(rotation_matrix).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            f\"/source_camera_{i}\",\n",
    "            fov=60, aspect=4/3, scale=0.08,  # Smaller scale\n",
    "            position=position, wxyz=wxyz,\n",
    "            color=(0.2, 0.8, 0.2)  # Light green\n",
    "        )\n",
    "    \n",
    "    # Add target camera poses (static, red, every 5th to reduce clutter)\n",
    "    for i, pose in enumerate(target_poses_np[::5]):\n",
    "        position = pose[:3, 3]\n",
    "        rotation_matrix = pose[:3, :3]\n",
    "        \n",
    "        # Convert rotation to quaternion\n",
    "        wxyz = viser.transforms.SO3.from_matrix(rotation_matrix).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            f\"/target_camera_{i}\",\n",
    "            fov=60, aspect=4/3, scale=0.08,  # Smaller scale\n",
    "            position=position, wxyz=wxyz,\n",
    "            color=(0.8, 0.2, 0.2)  # Light red\n",
    "        )\n",
    "    \n",
    "    # Add start/end markers\n",
    "    server.scene.add_icosphere(\"/source_start\", radius=0.05, position=source_positions[0], color=(0.0, 1.0, 0.0))\n",
    "    server.scene.add_icosphere(\"/source_end\", radius=0.05, position=source_positions[-1], color=(0.0, 0.5, 0.0))\n",
    "    server.scene.add_icosphere(\"/target_start\", radius=0.05, position=target_positions[0], color=(1.0, 0.0, 0.0))\n",
    "    server.scene.add_icosphere(\"/target_end\", radius=0.05, position=target_positions[-1], color=(0.5, 0.0, 0.0))\n",
    "    \n",
    "    server.scene.add_frame(\"/world\", axes_length=0.2, position=(0, 0, 0), wxyz=(1, 0, 0, 0))\n",
    "\n",
    "def animate_frame(server, scene_data, frame_idx, max_points=8000):\n",
    "    \"\"\"Update point cloud and current camera positions for given frame\"\"\"\n",
    "    # Clear previous frame\n",
    "    try:\n",
    "        server.scene.remove(\"/current_frame\")\n",
    "        server.scene.remove(\"/current_source_camera\")\n",
    "        server.scene.remove(\"/current_target_camera\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract points for this frame using both cameras\n",
    "    frame_data = {\n",
    "        'frame': scene_data['frames_tensor'][frame_idx:frame_idx+1],\n",
    "        'depth': scene_data['depths'][frame_idx:frame_idx+1], \n",
    "        'pose_source': scene_data['pose_source'][frame_idx:frame_idx+1],\n",
    "        'pose_target': scene_data['pose_target'][frame_idx:frame_idx+1],\n",
    "        'intrinsics_source': scene_data['intrinsics_source'][frame_idx:frame_idx+1],\n",
    "        'intrinsics_target': scene_data['intrinsics_target'][frame_idx:frame_idx+1],\n",
    "    }\n",
    "    \n",
    "    # Use the updated method with both source and target cameras\n",
    "    points_3d, colors_rgb = vis_warper.extract_3d_points_with_colors(\n",
    "        frame_data['frame'],\n",
    "        frame_data['depth'], \n",
    "        frame_data['pose_source'],      # Source camera transformation\n",
    "        frame_data['pose_target'],      # Target camera transformation\n",
    "        frame_data['intrinsics_source'], # Source camera intrinsics\n",
    "        frame_data['intrinsics_target'], # Target camera intrinsics\n",
    "        subsample_step=15\n",
    "    )\n",
    "    \n",
    "    if points_3d.shape[0] > 0:\n",
    "        points_np = points_3d.cpu().numpy()\n",
    "        colors_np = colors_rgb.cpu().numpy()\n",
    "        \n",
    "        # Limit points for performance\n",
    "        if len(points_np) > max_points:\n",
    "            indices = np.random.choice(len(points_np), max_points, replace=False)\n",
    "            points_np = points_np[indices]\n",
    "            colors_np = colors_np[indices]\n",
    "        \n",
    "        # Normalize colors if needed\n",
    "        if colors_np.min() < 0:\n",
    "            colors_np = (colors_np + 1) / 2\n",
    "            \n",
    "        # Update point cloud\n",
    "        server.scene.add_point_cloud(\n",
    "            \"/current_frame\", \n",
    "            points=points_np, \n",
    "            colors=colors_np, \n",
    "            point_size=0.08\n",
    "        )\n",
    "        \n",
    "        # Highlight current source camera (bright green)\n",
    "        source_pos = scene_data['pose_source'][frame_idx, :3, 3].cpu().numpy()\n",
    "        source_rot = scene_data['pose_source'][frame_idx, :3, :3].cpu().numpy()\n",
    "        source_wxyz = viser.transforms.SO3.from_matrix(source_rot).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            \"/current_source_camera\",\n",
    "            fov=60, aspect=4/3, scale=0.15,\n",
    "            position=source_pos, wxyz=source_wxyz,\n",
    "            color=(0.0, 1.0, 0.0)  # Bright green\n",
    "        )\n",
    "        \n",
    "        # Highlight current target camera (bright red)\n",
    "        target_pos = scene_data['pose_target'][frame_idx, :3, 3].cpu().numpy()\n",
    "        target_rot = scene_data['pose_target'][frame_idx, :3, :3].cpu().numpy()\n",
    "        target_wxyz = viser.transforms.SO3.from_matrix(target_rot).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            \"/current_target_camera\",\n",
    "            fov=60, aspect=4/3, scale=0.15,\n",
    "            position=target_pos, wxyz=target_wxyz,\n",
    "            color=(1.0, 0.0, 0.0)  # Bright red\n",
    "        )\n",
    "\n",
    "# Setup scene\n",
    "setup_viser_scene(viser_server, scene_data)\n",
    "\n",
    "# Create GUI controls for animation\n",
    "@viser_server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    # Animation controls\n",
    "    with client.gui.add_folder(\"Animation Controls\"):\n",
    "        play_button = client.gui.add_button(\"▶️ Play/Pause\")\n",
    "        frame_slider = client.gui.add_slider(\n",
    "            \"Frame\", \n",
    "            min=0, \n",
    "            max=scene_data['frames_tensor'].shape[0]-1, \n",
    "            step=1, \n",
    "            initial_value=0\n",
    "        )\n",
    "        speed_slider = client.gui.add_slider(\n",
    "            \"Speed\", \n",
    "            min=0.5, \n",
    "            max=10.0, \n",
    "            step=0.5, \n",
    "            initial_value=2.0\n",
    "        )\n",
    "        max_points_slider = client.gui.add_slider(\n",
    "            \"Max Points\", \n",
    "            min=1000, \n",
    "            max=20000, \n",
    "            step=1000, \n",
    "            initial_value=8000\n",
    "        )\n",
    "    \n",
    "    with client.gui.add_folder(\"Camera Info\"):\n",
    "        source_cam_info = client.gui.add_text(\"Source Camera\", initial_value=scene_data.get('source_camera_undistorted', {}).get('name', 'Unknown'))\n",
    "        target_cam_info = client.gui.add_text(\"Target Camera\", initial_value=scene_data.get('target_camera', {}).get('name', 'Unknown'))\n",
    "        frame_info = client.gui.add_text(\"Current Frame\", initial_value=\"0\")\n",
    "    \n",
    "    # Animation state\n",
    "    is_playing = [False]\n",
    "    \n",
    "    @play_button.on_click\n",
    "    def _(_):\n",
    "        is_playing[0] = not is_playing[0]\n",
    "        play_button.name = \"⏸️ Pause\" if is_playing[0] else \"▶️ Play\"\n",
    "        \n",
    "    @frame_slider.on_update\n",
    "    def _(_):\n",
    "        animate_frame(viser_server, scene_data, frame_slider.value, max_points_slider.value)\n",
    "        frame_info.value = f\"Frame {frame_slider.value}/{scene_data['frames_tensor'].shape[0]-1}\"\n",
    "    \n",
    "    # Animation loop\n",
    "    def animation_loop():\n",
    "        while True:\n",
    "            if is_playing[0]:\n",
    "                current_frame = frame_slider.value\n",
    "                next_frame = (current_frame + 1) % scene_data['frames_tensor'].shape[0]\n",
    "                frame_slider.value = next_frame\n",
    "                animate_frame(viser_server, scene_data, next_frame, max_points_slider.value)\n",
    "                frame_info.value = f\"Frame {next_frame}/{scene_data['frames_tensor'].shape[0]-1}\"\n",
    "            time.sleep(1.0 / speed_slider.value)\n",
    "    \n",
    "    # Start animation thread\n",
    "    animation_thread = threading.Thread(target=animation_loop, daemon=True)\n",
    "    animation_thread.start()\n",
    "\n",
    "# Show first frame\n",
    "animate_frame(viser_server, scene_data, 0)\n",
    "\n",
    "print(f\"Animated Viser server running at: http://localhost:8080\")\n",
    "print(\"Controls:\")\n",
    "print(\"- Green trajectory/cameras = Source camera path\") \n",
    "print(\"- Red trajectory/cameras = Target camera path\")\n",
    "print(\"- Bright green/red frustums = Current frame cameras\")\n",
    "print(\"- Point cloud updates show 3D scene from current frame\")\n",
    "print(\"Use the GUI controls to play/pause animation and adjust settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Animated Viser Content\n",
    "def setup_viser_scene(server, scene_data):\n",
    "    \"\"\"Setup static scene elements (trajectory and camera poses)\"\"\"\n",
    "\n",
    "    poses_np = scene_data['pose_target'].cpu().numpy()\n",
    "    positions = poses_np[:, :3, 3]\n",
    "    \n",
    "    # Add trajectory (static)\n",
    "    server.scene.add_spline_catmull_rom(\n",
    "        \"/trajectory\", \n",
    "        positions=positions, \n",
    "        color=(1.0, 0.0, 0.0), \n",
    "        line_width=3.0\n",
    "    )\n",
    "    \n",
    "    # Add all camera poses (static)\n",
    "    for i, pose in enumerate(poses_np[::2]):  # Every 2nd pose to reduce clutter\n",
    "        position = pose[:3, 3]\n",
    "        rotation_matrix = pose[:3, :3]\n",
    "        \n",
    "        print(position)\n",
    "        \n",
    "        # flip_z = np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]])\n",
    "        # corrected_rotation = rotation_matrix @ flip_z\n",
    "        \n",
    "        corrected_rotation = rotation_matrix  # No correction\n",
    "        wxyz = viser.transforms.SO3.from_matrix(corrected_rotation).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            f\"/camera_{i}\",\n",
    "            fov=60, aspect=16/9, scale=0.15,\n",
    "            position=position, wxyz=wxyz,\n",
    "            color=(0.8, 0.2, 0.2)\n",
    "        )\n",
    "    \n",
    "    # Add start/end markers\n",
    "    server.scene.add_icosphere(\"/start\", radius=0.1, position=positions[0], color=(0.0, 1.0, 0.0))\n",
    "    server.scene.add_icosphere(\"/end\", radius=0.1, position=positions[-1], color=(1.0, 0.0, 1.0))\n",
    "    server.scene.add_frame(\"/world\", axes_length=0.5, position=(0, 0, 0), wxyz=(1, 0, 0, 0))\n",
    "\n",
    "def animate_frame(server, scene_data, frame_idx, max_points=5000):\n",
    "    \"\"\"Update only the point cloud for given frame\"\"\"\n",
    "    # Clear previous frame\n",
    "    try:\n",
    "        server.scene.remove(\"/current_frame\")\n",
    "        server.scene.remove(\"/current_camera\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract points for this frame\n",
    "    frame_data = {\n",
    "        'frame': scene_data['frames_tensor'][frame_idx:frame_idx+1],\n",
    "        'depth': scene_data['depths'][frame_idx:frame_idx+1], \n",
    "        'pose_source': scene_data['pose_source'][frame_idx:frame_idx+1],\n",
    "        'intrinsics': scene_data['intrinsics'][frame_idx:frame_idx+1],\n",
    "    }\n",
    "    \n",
    "    points_3d, colors_rgb = vis_warper.extract_3d_points_with_colors(\n",
    "        frame_data['frame'], frame_data['depth'], \n",
    "        frame_data['pose_source'], frame_data['intrinsics'],\n",
    "        subsample_step=15\n",
    "    )\n",
    "    \n",
    "    if points_3d.shape[0] > 0:\n",
    "        points_np = points_3d.cpu().numpy()\n",
    "        colors_np = colors_rgb.cpu().numpy()\n",
    "        \n",
    "        # Limit points\n",
    "        if len(points_np) > max_points:\n",
    "            indices = np.random.choice(len(points_np), max_points, replace=False)\n",
    "            points_np = points_np[indices]\n",
    "            colors_np = colors_np[indices]\n",
    "        \n",
    "        if colors_np.min() < 0:\n",
    "            colors_np = (colors_np + 1) / 2\n",
    "            \n",
    "        # Update point cloud\n",
    "        server.scene.add_point_cloud(\n",
    "            \"/current_frame\", \n",
    "            points=points_np, \n",
    "            colors=colors_np, \n",
    "            point_size=0.03\n",
    "        )\n",
    "        \n",
    "        # Highlight current camera\n",
    "        pos = scene_data['pose_target'][frame_idx, :3, 3].cpu().numpy()\n",
    "        server.scene.add_icosphere(\n",
    "            \"/current_camera\", \n",
    "            radius=0.08, \n",
    "            position=pos, \n",
    "            color=(1.0, 1.0, 0.0)  # Yellow\n",
    "        )\n",
    "\n",
    "# Setup scene\n",
    "setup_viser_scene(viser_server, scene_data)\n",
    "\n",
    "# Create GUI controls for animation\n",
    "@viser_server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    # Animation controls\n",
    "    play_button = client.gui.add_button(\"Play/Pause\")\n",
    "    frame_slider = client.gui.add_slider(\n",
    "        \"Frame\", \n",
    "        min=0, \n",
    "        max=scene_data['frames_tensor'].shape[0]-1, \n",
    "        step=1, \n",
    "        initial_value=0\n",
    "    )\n",
    "    speed_slider = client.gui.add_slider(\n",
    "        \"Speed\", \n",
    "        min=1, \n",
    "        max=10.0, \n",
    "        step=0.1, \n",
    "        initial_value=3.0\n",
    "    )\n",
    "    \n",
    "    # Animation state\n",
    "    is_playing = [False]\n",
    "    \n",
    "    @play_button.on_click\n",
    "    def _(_):\n",
    "        is_playing[0] = not is_playing[0]\n",
    "        \n",
    "    @frame_slider.on_update\n",
    "    def _(_):\n",
    "        animate_frame(viser_server, scene_data, frame_slider.value)\n",
    "    \n",
    "    # Animation loop\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    def animation_loop():\n",
    "        while True:\n",
    "            if is_playing[0]:\n",
    "                current_frame = frame_slider.value\n",
    "                next_frame = (current_frame + 1) % scene_data['frames_tensor'].shape[0]\n",
    "                frame_slider.value = next_frame\n",
    "                animate_frame(viser_server, scene_data, next_frame)\n",
    "            time.sleep(0.5 / speed_slider.value)\n",
    "    \n",
    "    # Start animation thread\n",
    "    animation_thread = threading.Thread(target=animation_loop, daemon=True)\n",
    "    animation_thread.start()\n",
    "\n",
    "# Show first frame\n",
    "animate_frame(viser_server, scene_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Viser GUI Controls\n",
    "##########################################\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import viser\n",
    "\n",
    "\n",
    "# Simple Point Size Control\n",
    "@viser_server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    \n",
    "    update_camera_position()\n",
    "    \n",
    "    # Add simple point size slider\n",
    "    point_size_slider = client.gui.add_slider(\n",
    "        \"Point Size\",\n",
    "        min=0.005,\n",
    "        max=0.1,\n",
    "        step=0.005,\n",
    "        initial_value=0.015,\n",
    "    )\n",
    "    \n",
    "    # Update point size when slider changes\n",
    "    @point_size_slider.on_update\n",
    "    def _(_) -> None:\n",
    "        if points_3d is not None:\n",
    "            points_np = points_3d.cpu().numpy()\n",
    "            colors_np = colors_rgb.cpu().numpy()\n",
    "            if colors_np.min() < 0:\n",
    "                colors_np = (colors_np + 1) / 2\n",
    "            viser_server.scene.add_point_cloud(\n",
    "                \"/scene_points\",\n",
    "                points=points_np,\n",
    "                colors=colors_np,\n",
    "                point_size=point_size_slider.value\n",
    "            )\n",
    "\n",
    "# Set initial camera position\n",
    "initial_theta = 0\n",
    "initial_phi = 75\n",
    "initial_roll = -90\n",
    "initial_radius = 10\n",
    "\n",
    "# Add sliders for camera control (no global variables needed)\n",
    "theta_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Theta (deg)\",\n",
    "    min=0, max=360, step=1, initial_value=initial_theta,\n",
    ")\n",
    "\n",
    "phi_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Phi (deg)\", \n",
    "    min=-90, max=270, step=1, initial_value=initial_phi,\n",
    ")\n",
    "\n",
    "roll_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Roll (deg)\",\n",
    "    min=-180, max=180, step=1, initial_value=initial_roll,\n",
    ")\n",
    "\n",
    "radius_slider = viser_server.gui.add_slider(\n",
    "    \"Camera Distance\",\n",
    "    min=1, max=20, step=0.1, initial_value=initial_radius,\n",
    ")\n",
    "\n",
    "def update_camera_position():\n",
    "    theta = math.radians(theta_slider.value)\n",
    "    phi = math.radians(phi_slider.value)\n",
    "    r = radius_slider.value\n",
    "    roll = math.radians(roll_slider.value)\n",
    "    \n",
    "    # Convert spherical to cartesian\n",
    "    x = r * math.cos(phi) * math.cos(theta)\n",
    "    y = r * math.cos(phi) * math.sin(theta) \n",
    "    z = r * math.sin(phi)\n",
    "    \n",
    "    position = np.array([x, y, z])\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    \n",
    "    # Calculate camera's forward direction\n",
    "    forward = (look_at - position)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    # Handle world up vector based on phi angle to prevent flipping\n",
    "    if abs(phi) < math.pi/2:  # -90° to +90°: normal \"above horizon\" view\n",
    "        world_up = np.array([0, 0, 1])\n",
    "    else:  # Beyond ±90°: \"below horizon\" or \"upside down\" view\n",
    "        world_up = np.array([0, 0, -1])  # Flip world up\n",
    "    \n",
    "    # Calculate right vector\n",
    "    right = np.cross(forward, world_up)\n",
    "    if np.linalg.norm(right) < 1e-6:  # Handle gimbal lock at poles\n",
    "        # Use a fallback right vector\n",
    "        right = np.array([1, 0, 0]) if abs(theta) < math.pi else np.array([-1, 0, 0])\n",
    "    else:\n",
    "        right = right / np.linalg.norm(right)\n",
    "    \n",
    "    # Calculate up vector\n",
    "    up_initial = np.cross(right, forward)\n",
    "    up_initial = up_initial / np.linalg.norm(up_initial)\n",
    "    \n",
    "    # Apply roll rotation around the forward axis\n",
    "    cos_roll = np.cos(roll)\n",
    "    sin_roll = np.sin(roll)\n",
    "    up = cos_roll * up_initial + sin_roll * right\n",
    "    \n",
    "    # Set camera using the correct API\n",
    "    for client in viser_server.get_clients().values():\n",
    "        client.camera.position = position\n",
    "        client.camera.look_at = look_at\n",
    "        client.camera.up_direction = up\n",
    "        \n",
    "\n",
    "@theta_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "    \n",
    "@phi_slider.on_update \n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "    \n",
    "@radius_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "\n",
    "@roll_slider.on_update\n",
    "def _(_):\n",
    "    update_camera_position()\n",
    "\n",
    "# Apply initial camera position\n",
    "# update_camera_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: To test different trajectories (example)\n",
    "# Change your trajectory and re-run the update\n",
    "opts_test = copy.deepcopy(opts_base)\n",
    "opts_test.target_pose = [0, 90, 1, 0, 0]  # 180° rotation\n",
    "\n",
    "# Generate new scene data\n",
    "scene_data_test = vis_crafter.extract_scene_data(opts_test)\n",
    "\n",
    "# Update the same server with new content\n",
    "update_viser_content(viser_server, scene_data_test, points_3d, colors_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Camera trajectory visualization\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scene data (same as your existing code)\n",
    "scene_data = vis_crafter.extract_scene_data(opts_base)\n",
    "\n",
    "# Generate CIRCULAR trajectory instead of linear\n",
    "def generate_circular_scene_data(crafter, opts, scene_data, circle_type='horizontal'):\n",
    "    \"\"\"Generate new scene data with circular motion\"\"\"\n",
    "    \n",
    "    # Reuse existing depths and frames\n",
    "    pose_s, pose_t, K = crafter.get_poses_circular(\n",
    "        opts, \n",
    "        scene_data['depths'], \n",
    "        num_frames=opts.video_length,\n",
    "        circle_type=circle_type\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'frames_numpy': scene_data['frames_numpy'],\n",
    "        'frames_tensor': scene_data['frames_tensor'],\n",
    "        'depths': scene_data['depths'], \n",
    "        'pose_source': pose_s,\n",
    "        'pose_target': pose_t,\n",
    "        'intrinsics': K,\n",
    "        'radius': scene_data['radius'],\n",
    "        'trajectory_params': f\"circular_{circle_type}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate different circular motions\n",
    "# horizontal_circle = generate_circular_scene_data(vis_crafter, opts_base, scene_data, 'horizontal')\n",
    "vertical_circle = generate_circular_scene_data(vis_crafter, opts_base, scene_data, 'vertical_xz')\n",
    "\n",
    "# Use in your existing Viser visualization\n",
    "# update_trajectory_visualization(viser_server, horizontal_circle)\n",
    "update_trajectory_visualization(viser_server, vertical_circle)\n",
    "\n",
    "print(\"Circular trajectories generated!\")\n",
    "print(\"Available types: horizontal, vertical_xz, vertical_yz, tilted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Camera trajectory visualization - Multiple Trajectories\n",
    "##########################################################################\n",
    "\n",
    "def generate_new_trajectory(vis_crafter, opts_base, target_pose_params, scene_data):\n",
    "    \"\"\"Generate new trajectory without recomputing point clouds\"\"\"\n",
    "    print(f\"Generating trajectory for pose: {target_pose_params}\")\n",
    "    \n",
    "    # Create new opts with different target pose\n",
    "    opts_new = copy.deepcopy(opts_base)\n",
    "    opts_new.target_pose = target_pose_params\n",
    "    \n",
    "    # Only regenerate poses, reuse existing depths\n",
    "    # pose_s, pose_t, K = vis_crafter.get_poses(opts_new, scene_data['depths'], num_frames=opts_new.video_length)\n",
    "    pose_s, pose_t, K = vis_crafter.get_poses_circular(\n",
    "        opts_new, \n",
    "        scene_data['depths'], \n",
    "        num_frames=opts_new.video_length,\n",
    "        circle_type='horizontal'  # or other types based on params\n",
    "    )\n",
    "    \n",
    "    # Create new scene data with same point clouds but new trajectory\n",
    "    new_scene_data = {\n",
    "        'frames_numpy': scene_data['frames_numpy'],\n",
    "        'frames_tensor': scene_data['frames_tensor'], \n",
    "        'depths': scene_data['depths'],\n",
    "        'pose_source': pose_s,\n",
    "        'pose_target': pose_t,\n",
    "        'intrinsics': K,\n",
    "        'radius': scene_data['radius'],\n",
    "        'trajectory_params': target_pose_params\n",
    "    }\n",
    "    \n",
    "    return new_scene_data\n",
    "\n",
    "def update_trajectory_visualization(server, new_scene_data):\n",
    "    \"\"\"Update only the trajectory visualization, keep point clouds\"\"\"\n",
    "    \n",
    "    # Clear existing trajectory elements\n",
    "    try:\n",
    "        server.scene.remove(\"/trajectory\")\n",
    "        for i in range(50):  # Clear up to 50 camera poses\n",
    "            try:\n",
    "                server.scene.remove(f\"/camera_{i}\")\n",
    "            except:\n",
    "                break\n",
    "        server.scene.remove(\"/start\")\n",
    "        server.scene.remove(\"/end\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add new trajectory\n",
    "    poses_np = new_scene_data['pose_target'].cpu().numpy()\n",
    "    positions = poses_np[:, :3, 3]\n",
    "    \n",
    "    # Add trajectory spline\n",
    "    server.scene.add_spline_catmull_rom(\n",
    "        \"/trajectory\", \n",
    "        positions=positions, \n",
    "        color=(1.0, 0.0, 0.0), \n",
    "        line_width=3.0\n",
    "    )\n",
    "    \n",
    "    # Add camera poses (every 2nd to reduce clutter)\n",
    "    for i, pose in enumerate(poses_np[::2]):\n",
    "        position = pose[:3, 3]\n",
    "        rotation_matrix = pose[:3, :3]\n",
    "        \n",
    "        # flip_z = np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]])\n",
    "        # corrected_rotation = rotation_matrix @ flip_z\n",
    "        \n",
    "        corrected_rotation = rotation_matrix  # No correction\n",
    "        wxyz = viser.transforms.SO3.from_matrix(corrected_rotation).wxyz\n",
    "        \n",
    "        server.scene.add_camera_frustum(\n",
    "            f\"/camera_{i}\",\n",
    "            fov=60, aspect=16/9, scale=0.15,\n",
    "            position=position, wxyz=wxyz,\n",
    "            color=(0.8, 0.2, 0.2)\n",
    "        )\n",
    "    \n",
    "    # Add new start/end markers\n",
    "    server.scene.add_icosphere(\"/start\", radius=0.1, position=positions[0], color=(0.0, 1.0, 0.0))\n",
    "    server.scene.add_icosphere(\"/end\", radius=0.1, position=positions[-1], color=(1.0, 0.0, 1.0))\n",
    "\n",
    "# Predefined trajectory presets\n",
    "TRAJECTORY_PRESETS = {\n",
    "    \"Original Right 90°\": [0, 90, 1, 0, 0],\n",
    "    \"Left 90°\": [0, -90, 1, 0, 0], \n",
    "    \"Full Circle\": [0, 360, 1, 0, 0],\n",
    "    \"Up and Right\": [45, 45, 0.5, 0, 1],\n",
    "    \"Pull Back\": [0, 0, 3, 0, 0],\n",
    "    \"Orbit Up\": [30, 180, 0, 0, 0],\n",
    "    \"Dolly Forward\": [0, 0, -2, 0, 0],\n",
    "    \"Rise and Turn\": [60, 120, 1, 0, 2],\n",
    "}\n",
    "\n",
    "# Create trajectory selection GUI\n",
    "@viser_server.on_client_connect  \n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    \n",
    "    # Trajectory selection dropdown\n",
    "    trajectory_dropdown = client.gui.add_dropdown(\n",
    "        \"Trajectory Preset\",\n",
    "        options=list(TRAJECTORY_PRESETS.keys()),\n",
    "        initial_value=\"Original Right 90°\"\n",
    "    )\n",
    "    \n",
    "    # Manual trajectory controls\n",
    "    with client.gui.add_folder(\"Custom Trajectory\"):\n",
    "        theta_input = client.gui.add_slider(\"Theta (pitch)\", min=-90, max=90, step=1, initial_value=0)\n",
    "        phi_input = client.gui.add_slider(\"Phi (yaw)\", min=-360, max=360, step=5, initial_value=90)\n",
    "        dr_input = client.gui.add_slider(\"Distance\", min=-3, max=3, step=0.1, initial_value=1.0)\n",
    "        dx_input = client.gui.add_slider(\"X offset\", min=-2, max=2, step=0.1, initial_value=0.0)\n",
    "        dy_input = client.gui.add_slider(\"Y offset\", min=-2, max=2, step=0.1, initial_value=0.0)\n",
    "        \n",
    "        generate_button = client.gui.add_button(\"Generate Custom Trajectory\")\n",
    "    \n",
    "    # Display current trajectory info\n",
    "    trajectory_info = client.gui.add_text(\"Trajectory Info\", initial_value=\"Current: [0, 90, 1, 0, 0]\")\n",
    "    \n",
    "    # Global reference to current scene data\n",
    "    current_scene_data = [scene_data]  # Use list for mutability\n",
    "    \n",
    "    @trajectory_dropdown.on_update\n",
    "    def _(_):\n",
    "        selected_preset = trajectory_dropdown.value\n",
    "        target_pose = TRAJECTORY_PRESETS[selected_preset]\n",
    "        \n",
    "        # Generate new trajectory\n",
    "        new_scene_data = generate_new_trajectory(vis_crafter, opts_base, target_pose, scene_data)\n",
    "        current_scene_data[0] = new_scene_data\n",
    "        \n",
    "        # Update visualization\n",
    "        update_trajectory_visualization(viser_server, new_scene_data)\n",
    "        \n",
    "        # Update info\n",
    "        trajectory_info.value = f\"Current: {target_pose}\"\n",
    "        \n",
    "        # Update manual controls to match preset\n",
    "        theta_input.value = target_pose[0]\n",
    "        phi_input.value = target_pose[1] \n",
    "        dr_input.value = target_pose[2]\n",
    "        dx_input.value = target_pose[3]\n",
    "        dy_input.value = target_pose[4]\n",
    "        \n",
    "        print(f\"Switched to trajectory: {selected_preset} -> {target_pose}\")\n",
    "    \n",
    "    @generate_button.on_click\n",
    "    def _(_):\n",
    "        custom_pose = [theta_input.value, phi_input.value, dr_input.value, dx_input.value, dy_input.value]\n",
    "        \n",
    "        # Generate new trajectory\n",
    "        new_scene_data = generate_new_trajectory(vis_crafter, opts_base, custom_pose, scene_data)\n",
    "        current_scene_data[0] = new_scene_data\n",
    "        \n",
    "        # Update visualization  \n",
    "        update_trajectory_visualization(viser_server, new_scene_data)\n",
    "        \n",
    "        # Update info\n",
    "        trajectory_info.value = f\"Current: {custom_pose}\"\n",
    "        \n",
    "        print(f\"Generated custom trajectory: {custom_pose}\")\n",
    "    \n",
    "    # Animation controls for current trajectory\n",
    "    with client.gui.add_folder(\"Animation\"):\n",
    "        play_button = client.gui.add_button(\"Play/Pause\")\n",
    "        frame_slider = client.gui.add_slider(\n",
    "            \"Frame\", \n",
    "            min=0, \n",
    "            max=scene_data['frames_tensor'].shape[0]-1, \n",
    "            step=1, \n",
    "            initial_value=0\n",
    "        )\n",
    "        speed_slider = client.gui.add_slider(\"Speed\", min=0.5, max=5.0, step=0.1, initial_value=1.0)\n",
    "    \n",
    "    # Animation state\n",
    "    is_playing = [False]\n",
    "    \n",
    "    @play_button.on_click\n",
    "    def _(_):\n",
    "        is_playing[0] = not is_playing[0]\n",
    "        \n",
    "    @frame_slider.on_update\n",
    "    def _(_):\n",
    "        animate_frame(viser_server, current_scene_data[0], frame_slider.value)\n",
    "    \n",
    "    # Animation loop\n",
    "    import threading\n",
    "    import time\n",
    "    \n",
    "    def animation_loop():\n",
    "        while True:\n",
    "            if is_playing[0]:\n",
    "                current_frame = frame_slider.value\n",
    "                next_frame = (current_frame + 1) % current_scene_data[0]['frames_tensor'].shape[0]\n",
    "                frame_slider.value = next_frame\n",
    "                animate_frame(viser_server, current_scene_data[0], next_frame)\n",
    "            time.sleep(1.0 / speed_slider.value)\n",
    "    \n",
    "    # Start animation thread\n",
    "    animation_thread = threading.Thread(target=animation_loop, daemon=True)\n",
    "    animation_thread.start()\n",
    "\n",
    "print(\"Trajectory visualization setup complete!\")\n",
    "print(\"Available presets:\", list(TRAJECTORY_PRESETS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print trajectory numbers for a few presets\n",
    "target_poses = {\n",
    "    \"Right 90°\": [0, 90, 1, 0, 0],\n",
    "    \"Full Circle\": [0, 360, 1, 0, 0],\n",
    "    \"Pull Back\": [0, 0, 3, 0, 0]\n",
    "}\n",
    "\n",
    "new_data = generate_new_trajectory(\n",
    "    vis_crafter, opts_base,\n",
    "    target_poses[\"Right 90°\"],\n",
    "    scene_data\n",
    "    )\n",
    "\n",
    "positions = new_data['pose_target'].cpu().numpy()[:, :3, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using maplotlib, plot the 0th and 2rd axis\n",
    "# Using matplotlib, plot the 0th and 2nd axis of positions variable\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot X vs Z (0th vs 2nd axis)\n",
    "plt.plot(positions[:, 0], positions[:, 2], 'b-o', linewidth=2, markersize=4)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Z Position') \n",
    "plt.title('Camera Trajectory: X vs Z (Top-down view)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Mark start and end points\n",
    "plt.plot(positions[0, 0], positions[0, 2], 'go', markersize=10, label='Start')\n",
    "plt.plot(positions[-1, 0], positions[-1, 2], 'ro', markersize=10, label='End')\n",
    "\n",
    "# Add frame numbers for reference\n",
    "for i in range(0, len(positions), max(1, len(positions)//8)):\n",
    "    plt.annotate(f'{i}', (positions[i, 0], positions[i, 2]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the trajectory values\n",
    "print(f\"Trajectory shape: {positions.shape}\")\n",
    "print(f\"X range: {positions[:, 0].min():.3f} to {positions[:, 0].max():.3f}\")\n",
    "print(f\"Z range: {positions[:, 2].min():.3f} to {positions[:, 2].max():.3f}\")\n",
    "print(f\"Start (X,Z): ({positions[0, 0]:.3f}, {positions[0, 2]:.3f})\")\n",
    "print(f\"End (X,Z): ({positions[-1, 0]:.3f}, {positions[-1, 2]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the coordinate system by examining a few poses\n",
    "poses = new_data['pose_target'].cpu().numpy()\n",
    "print(\"Shape:\", poses.shape)\n",
    "print(\"\\nFirst pose (Frame 0):\")\n",
    "print(poses[0])\n",
    "print(\"\\nPosition:\", poses[0][:3, 3])\n",
    "print(\"Rotation matrix:\")\n",
    "print(poses[0][:3, :3])\n",
    "\n",
    "# Check if it follows OpenCV or OpenGL convention\n",
    "print(\"\\nCoordinate system analysis:\")\n",
    "print(\"Z-axis (forward direction):\", poses[0][:3, 2])\n",
    "print(\"Y-axis (up direction):\", poses[0][:3, 1]) \n",
    "print(\"X-axis (right direction):\", poses[0][:3, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
