{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Video\n",
    "# Trajectory Planning\n",
    "# Camera_0 = Estimate video camera pose\n",
    "# Target angle: 100, 4 segments, from 1st frame\n",
    "# Camera_1 = 0 to 25 â€“ Camera_0\n",
    "# Camera_2 \n",
    "\n",
    "def pad_video(frames, target_length):\n",
    "    if frames.shape[0] < target_length:\n",
    "        last_frame = frames[-1:]\n",
    "        num_pad = target_length - frames.shape[0]\n",
    "        pad_frames = np.repeat(last_frame, num_pad, axis=0)\n",
    "        frames = np.concatenate([frames, pad_frames], axis=0)\n",
    "    return frames\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 0: set up stage\n",
    "    \n",
    "    # read input video\n",
    "    frames = read_video_frames(\n",
    "            opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "        )\n",
    "    \n",
    "    # pad if too short\n",
    "    frames = pad_video(frames, opts.video_length)\n",
    "    \n",
    "    # prompt    \n",
    "    prompt = self.get_caption(opts, frames[opts.video_length // 2])\n",
    "\n",
    "    # get global cam_pos of input video + n=video_length 3D point clouds\n",
    "    poses_input, pc_input = geometric_fm(frames, opts)\n",
    "    \n",
    "    # plan trajectory\n",
    "    traj_segments = plan_trajectory(\n",
    "        poses_input[0], opts.target_pose, opts.n_splits\n",
    "    )\n",
    "    \n",
    "    # save everything\n",
    "    # Directory Structure\n",
    "    # - exp_name/\n",
    "    \n",
    "    #   - input/\n",
    "    #     + input.mp4\n",
    "    #     + cameras_input.npy\n",
    "    #     + point_cloud_input.ply\n",
    "    #     + prompt.txt\n",
    "    \n",
    "    #   - stage_1/\n",
    "    #     - input.mp4 mask.mp4 render.mp4 gen.mp4\n",
    "    #     - point_cloud_input.ply\n",
    "    #     + cameras_target.npy\n",
    "    setup_exp_directory(opts, frames, poses_input, pc_input, prompt, traj_segments)\n",
    "    \n",
    "    pc_global = pc_input\n",
    "    \n",
    "    # 1: autoregressive generation\n",
    "    for i in range(opts.n_splits):\n",
    "        \n",
    "        # TODO: video reversal for even segments\n",
    "        \n",
    "        inpainted_video = generate_segment(frames, pc_global, traj_segments[i], opts)\n",
    "        \n",
    "        pc_inpainted = geometric_fm(inpainted_video, opts)\n",
    "        pc_global = merge_point_clouds(pc_global, pc_inpainted)\n",
    "        \n",
    "        save_segment_results(inpainted_video, pc_global, traj_segments[i], opts, segment_idx=i)    \n",
    "    \n",
    "    # maybe final global inpainting here\n",
    "        \n",
    "\n",
    "# Inpaint\n",
    "# Project to 3D, save PC_1 in COLMAP format\n",
    "# Inpaint\n",
    "# Save camera_1, gen_1, render_1\n",
    "# Extend\n",
    "# Reverse gen_1\n",
    "# camera_1 are known - Global\n",
    "# Estimate Depth from gen_1_r\n",
    "# Project to 3D\n",
    "# Merge with PC_1, save in COLMAP format\n",
    "# Inpaint\n",
    "# Save camera_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames, sphere2pose, save_video\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.infer import DepthCrafterDemo\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add core.py to path if needed\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/28_08_25_trajectories')\n",
    "from core import VisualizationWarper\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "\n",
    "\n",
    "class TrajCrafterAutoregressive(TrajCrafter):\n",
    "    def __init__(self, opts):\n",
    "        super().__init__(opts)\n",
    "\n",
    "        # self.funwarp = VisualizationWarper(device=opts.device)\n",
    "        self.prompt = None\n",
    "        \n",
    "        self.K = torch.tensor(\n",
    "            [[500, 0.0, 512.], [0.0, 500, 288.], [0.0, 0.0, 1.0]]\n",
    "            ).repeat(opts.video_length, 1, 1).to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS (moved outside class)\n",
    "# ============================================================================\n",
    "\n",
    "def pad_video(frames, target_length):\n",
    "    if frames.shape[0] < target_length:\n",
    "        last_frame = frames[-1:]\n",
    "        num_pad = target_length - frames.shape[0]\n",
    "        pad_frames = np.repeat(last_frame, num_pad, axis=0)\n",
    "        frames = np.concatenate([frames, pad_frames], axis=0)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def generate_traj_specified(c2ws_anchor, target_pose, n_frames, device):\n",
    "    theta, phi, d_r, d_x, d_y = target_pose\n",
    "    \n",
    "    thetas = np.linspace(0, theta, n_frames)  \n",
    "    phis = np.linspace(0, phi, n_frames)          \n",
    "    rs = np.linspace(0, d_r, n_frames)            \n",
    "    xs = np.linspace(0, d_x, n_frames)            \n",
    "    ys = np.linspace(0, d_y, n_frames)            \n",
    "    \n",
    "    c2ws_list = []\n",
    "    for th, ph, r, x, y in zip(thetas, phis, rs, xs, ys):\n",
    "        c2w_new = sphere2pose(\n",
    "            c2ws_anchor,\n",
    "            np.float32(th),\n",
    "            np.float32(ph),\n",
    "            np.float32(r),\n",
    "            device,\n",
    "            np.float32(x),\n",
    "            np.float32(y),\n",
    "        )\n",
    "        c2ws_list.append(c2w_new)\n",
    "    c2ws = torch.cat(c2ws_list, dim=0)\n",
    "    return c2ws\n",
    "\n",
    "\n",
    "def save_poses_torch(c2ws, filepath):\n",
    "    \"\"\"Save camera poses as PyTorch tensor (.pth file)\"\"\"\n",
    "    torch.save(c2ws.cpu(), filepath)\n",
    "\n",
    "def save_point_clouds_torch(pc_list, color_list, dirpath):\n",
    "    \"\"\"Save point clouds as PyTorch tensors (much faster than text files)\"\"\"\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    \n",
    "    # Save as individual tensor files\n",
    "    for idx, (pc, color) in enumerate(zip(pc_list, color_list)):\n",
    "        # Save points and colors as separate tensors\n",
    "        torch.save(pc.cpu(), os.path.join(dirpath, f'points_{idx:03d}.pth'))\n",
    "        torch.save(color.cpu(), os.path.join(dirpath, f'colors_{idx:03d}.pth'))\n",
    "    \n",
    "\n",
    "def save_segment_results(pc_input, color_input, pc_inpainted, color_inpainted, \n",
    "                        pc_merged, color_merged, traj_segment, opts, segment_idx):\n",
    "    # Function to save results for each segment\n",
    "    # Implementation needed\n",
    "    stage_dir = Path(opts.save_dir) / f'stage_{segment_idx+1}'\n",
    "    stage_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    save_point_clouds_torch(pc_input, color_input, stage_dir / 'point_cloud_input')\n",
    "    # save_point_clouds_torch(pc_inpainted, color_inpainted, stage_dir / 'point_cloud_inpainted')\n",
    "    # save_point_clouds_torch(pc_merged, color_merged, stage_dir / 'point_cloud_merged')\n",
    "    save_poses_torch(traj_segment, stage_dir / 'cameras_target.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_point_cloud(frames, c2ws, opts):\n",
    "    \n",
    "    # print('before depth', frames.shape, frames.dtype, min(frames), max(frames))\n",
    "    \n",
    "    depths = vis_crafter.depth_estimater.infer(\n",
    "        frames,\n",
    "        opts.near,\n",
    "        opts.far,\n",
    "        opts.depth_inference_steps,\n",
    "        opts.depth_guidance_scale,\n",
    "        window_size=opts.window_size,\n",
    "        overlap=opts.overlap,\n",
    "    ).to(opts.device)\n",
    "    \n",
    "    radius = (\n",
    "        depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "        * opts.radius_scale\n",
    "    )\n",
    "    radius = min(radius, 5)\n",
    "    \n",
    "    # frames = torch.from_numpy(frames).to(opts.device)\n",
    "    \n",
    "    frames = (\n",
    "        torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "    )  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "    assert frames.shape[0] == opts.video_length\n",
    "\n",
    "    \n",
    "    pc_list = []\n",
    "    color_list = []\n",
    "    for i in range(opts.video_length):\n",
    "        \n",
    "        # print(frames[i:i+1].shape)\n",
    "        # print(depths[i:i+1].shape)\n",
    "        \n",
    "        pc, color = funwarp.extract_3d_points_with_colors(\n",
    "            frames[i:i+1],\n",
    "            depths[i:i+1],\n",
    "            c2ws[i:i+1],\n",
    "            vis_crafter.K[i:i+1],\n",
    "            subsample_step=1\n",
    "        )\n",
    "        # print(pc.device)\n",
    "        # print(color.device)\n",
    "        pc_list.append(pc)\n",
    "        color_list.append(color)\n",
    "    \n",
    "    return pc_list, color_list, radius\n",
    "\n",
    "\n",
    "def generate_segment(frames, pc_input, color_input, traj_segment, segment_dir, opts):\n",
    "    \n",
    "    frames = (\n",
    "        torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "    )  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "    assert frames.shape[0] == opts.video_length\n",
    "\n",
    "    # render the point clouds\n",
    "    warped_images = []\n",
    "    masks = []        \n",
    "    for i in tqdm(range(opts.video_length)):\n",
    "        \n",
    "        # print(pc_input[i].device)\n",
    "        # print(color_input[i].device)\n",
    "        # print(traj_segment[i:i+1].device)\n",
    "        # print(vis_crafter.K[i:i+1].device)\n",
    "        \n",
    "        output_frame, output_mask = funwarp.render_pointcloud_native(\n",
    "            pc_input[i],\n",
    "            color_input[i],\n",
    "            traj_segment[i:i+1],\n",
    "            vis_crafter.K[i:i+1],\n",
    "            image_size=(576, 1024),\n",
    "            mask=opts.mask,\n",
    "        )\n",
    "        warped_images.append(output_frame)\n",
    "        masks.append(output_mask)\n",
    "        \n",
    "        # print(color_input)\n",
    "        \n",
    "    # plot warped images 0, 10, 20\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axs = plt.subplots(3, figsize=(20, 6))\n",
    "    \n",
    "    # torch.Size([1, 3, 576, 1024]) torch.Size([1, 1, 576, 1024])\n",
    "    \n",
    "    # print(warped_images.min(), warped_images.max())\n",
    "    # print(warped_images)\n",
    "    \n",
    "    # print(warped_images[0].shape, masks[0].shape)\n",
    "    axs[0].imshow((frames[30].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0)\n",
    "    axs[1].imshow((warped_images[30][0].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0)\n",
    "    axs[2].imshow(masks[30][0].permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "        \n",
    "    cond_video = (torch.cat(warped_images) + 1.0) / 2.0\n",
    "    cond_masks = torch.cat(masks)\n",
    "\n",
    "    frames = F.interpolate(\n",
    "        frames, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    cond_video = F.interpolate(\n",
    "        cond_video, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    cond_masks = F.interpolate(cond_masks, size=opts.sample_size, mode='nearest')\n",
    "    \n",
    "    save_video(\n",
    "        (frames.permute(0, 2, 3, 1) + 1.0) / 2.0,\n",
    "        os.path.join(segment_dir, 'input.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    save_video(\n",
    "        cond_video.permute(0, 2, 3, 1),\n",
    "        os.path.join(segment_dir, 'render.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    save_video(\n",
    "        cond_masks.repeat(1, 3, 1, 1).permute(0, 2, 3, 1),\n",
    "        os.path.join(segment_dir, 'mask.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "\n",
    "    frames = (frames.permute(1, 0, 2, 3).unsqueeze(0) + 1.0) / 2.0\n",
    "    frames_ref = frames[:, :, :10, :, :]\n",
    "    cond_video = cond_video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "    cond_masks = (1.0 - cond_masks.permute(1, 0, 2, 3).unsqueeze(0)) * 255.0\n",
    "    generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "    # with torch.no_grad():            \n",
    "    #     sample = vis_crafter.pipeline(\n",
    "    #         vis_crafter.prompt,\n",
    "    #         num_frames=opts.video_length,\n",
    "    #         negative_prompt=opts.negative_prompt,\n",
    "    #         height=opts.sample_size[0],\n",
    "    #         width=opts.sample_size[1],\n",
    "    #         generator=generator,\n",
    "    #         guidance_scale=opts.diffusion_guidance_scale,\n",
    "    #         num_inference_steps=opts.diffusion_inference_steps,\n",
    "    #         video=cond_video.to(opts.device),\n",
    "    #         mask_video=cond_masks.to(opts.device),\n",
    "    #         reference=frames_ref,\n",
    "    #     ).videos\n",
    "    \n",
    "    sample = frames\n",
    "        \n",
    "    save_video(\n",
    "        sample[0].permute(1, 2, 3, 0),\n",
    "        os.path.join(segment_dir, 'gen.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    \n",
    "    frames = read_video_frames(\n",
    "        os.path.join(segment_dir, 'gen.mp4'), opts.video_length, opts.stride, opts.max_res\n",
    "    )\n",
    "    return frames\n",
    "    \n",
    "    # print('after diffusion', sample[0].shape, sample[0].min(), sample[0].max())\n",
    "    \n",
    "    # return sample[0].permute(1, 2, 3, 0)\n",
    "\n",
    "    \n",
    "\n",
    "def infer_autoregressive(opts):\n",
    "    \n",
    "    # read input video\n",
    "    frames = read_video_frames(\n",
    "        opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "    )\n",
    "    \n",
    "    # pad if too short\n",
    "    frames = pad_video(frames, opts.video_length)\n",
    "    \n",
    "    # prompt\n",
    "    vis_crafter.prompt = vis_crafter.get_caption(opts, frames[opts.video_length // 2])\n",
    "    \n",
    "    ########################################################\n",
    "    # Geometric FM\n",
    "    ########################################################\n",
    "    \n",
    "    c2ws_init = torch.tensor([\n",
    "                [-1.0, 0.0, 0.0, 0.0],\n",
    "                [0.0, 1.0, 0.0, 0.0],\n",
    "                [0.0, 0.0, -1.0, 0.0],\n",
    "                [0.0, 0.0, 0.0, 1.0],\n",
    "        ]).repeat(opts.video_length, 1, 1).to(opts.device)\n",
    "    \n",
    "    # c2ws_init = torch.tensor([\n",
    "    #             [1.0, 0.0, 0.0, 0.0],\n",
    "    #             [0.0, 1.0, 0.0, 0.0],\n",
    "    #             [0.0, 0.0, 1.0, 0.0],\n",
    "    #             [0.0, 0.0, 0.0, 1.0],\n",
    "    #     ]).repeat(opts.video_length, 1, 1).to(opts.device)\n",
    "    \n",
    "    # radius = (\n",
    "    #         depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "    #         * opts.radius_scale\n",
    "    #     )\n",
    "    #     radius = min(radius, 5)\n",
    "        # poses[:, 2, 3] = poses[:, 2, 3] + radius\n",
    "        \n",
    "    \n",
    "    \n",
    "    pc_input, color_input, radius = extract_point_cloud(frames, c2ws_init, opts)\n",
    "    \n",
    "    ########################################################\n",
    "    # Camera Pose Planning\n",
    "    ########################################################\n",
    "            \n",
    "    c2ws_target = generate_traj_specified(\n",
    "        c2ws_init[0:1], \n",
    "        opts.target_pose, \n",
    "        opts.video_length * opts.n_splits, \n",
    "        opts.device\n",
    "    )\n",
    "    \n",
    "    # c2ws_target[:, 2, 3] = c2ws_target[:, 2, 3] + radius\n",
    "    \n",
    "    # take inverse\n",
    "    c2ws_target = torch.inverse(c2ws_target)\n",
    "    \n",
    "    # split into segments\n",
    "    traj_segments = c2ws_target.view(opts.n_splits, opts.video_length, 4, 4)\n",
    "    \n",
    "    ########################################################\n",
    "    # Autoregressive Generation\n",
    "    ########################################################\n",
    "    \n",
    "    for i in range(opts.n_splits):\n",
    "        \n",
    "        segment_dir = os.path.join(opts.save_dir, f'stage_{i+1}')\n",
    "        os.makedirs(segment_dir, exist_ok=True)\n",
    "        \n",
    "        inpainted_video = generate_segment(\n",
    "            frames, pc_input, color_input, traj_segments[i], segment_dir, opts\n",
    "            )\n",
    "        \n",
    "        pc_inpainted = pc_input\n",
    "        color_inpainted = color_input\n",
    "        # pc_inpainted, color_inpainted = extract_point_cloud(inpainted_video, traj_segments[i], opts)\n",
    "        \n",
    "        # pc_merged, color_merged = merge_point_clouds(pc_input, color_input, pc_inpainted, color_inpainted)\n",
    "        pc_merged, color_merged = pc_inpainted, color_inpainted\n",
    "                    \n",
    "        save_segment_results(\n",
    "            pc_input,\n",
    "            color_input,\n",
    "            pc_inpainted,\n",
    "            color_inpainted,            \n",
    "            pc_merged,\n",
    "            color_merged,\n",
    "            traj_segments[i],\n",
    "            opts, segment_idx=i)\n",
    "        \n",
    "        frames = inpainted_video\n",
    "        pc_input = pc_merged\n",
    "        color_input = color_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autoregressive generation for large trajectories\n",
    "final_video = infer_autoregressive(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/rhino.mp4\",\n",
    "    \"--n_splits\", \"2\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "name = \"right_90\"\n",
    "pose = [0, 90, radius, 0, 0]\n",
    "\n",
    "print(f\"\\n=== Running Autoregressive {name} ===\")\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/28_08_25_trajectories')\n",
    "import core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "funwarp = core.VisualizationWarper(device=opts.device)\n",
    "funwarp.device = opts.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_crafter = TrajCrafterAutoregressive(opts_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
