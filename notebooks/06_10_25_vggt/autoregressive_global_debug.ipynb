{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames, sphere2pose, save_video\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.infer import DepthCrafterDemo\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add core.py to path if needed\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/28_08_25_trajectories')\n",
    "from core import VisualizationWarper\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "\n",
    "\n",
    "class TrajCrafterAutoregressive(TrajCrafter):\n",
    "    def __init__(self, opts):\n",
    "        super().__init__(opts)\n",
    "\n",
    "        # self.funwarp = VisualizationWarper(device=opts.device)\n",
    "        self.prompt = None\n",
    "        \n",
    "        self.K = torch.tensor(\n",
    "            [[500, 0.0, 512.], [0.0, 500, 288.], [0.0, 0.0, 1.0]]\n",
    "            ).repeat(opts.video_length, 1, 1).to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/rhino.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "# name = \"right_90\"\n",
    "# pose = [0, 90, radius, 0, 0]\n",
    "\n",
    "# name = \"top_90\"\n",
    "# pose = [90, 0, radius, 0, 0]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "# name = '120_0_0_0_3', make it infer values from pose\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "\n",
    "print(f\"\\n=== Running Autoregressive {name} ===\")\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrajCrafterVisualization(TrajCrafter):\n",
    "    \"\"\"Lightweight TrajCrafter subclass for camera trajectory visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, opts):\n",
    "        # Only initialize what we need for pose generation and depth estimation\n",
    "        self.device = opts.device\n",
    "        self.depth_estimater = DepthCrafterDemo(\n",
    "            unet_path=opts.unet_path,\n",
    "            pre_train_path=opts.pre_train_path,\n",
    "            cpu_offload=opts.cpu_offload,\n",
    "            device=opts.device,\n",
    "        )\n",
    "        print(\"TrajCrafterVisualization initialized (diffusion pipeline skipped)\")\n",
    "    \n",
    "    def extract_scene_data(self, opts):\n",
    "        \"\"\"Extract all data needed for 3D visualization\"\"\"\n",
    "        print(\"Reading video frames...\")\n",
    "        frames = read_video_frames(\n",
    "            opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "        )\n",
    "        \n",
    "        print(\"Estimating depth...\")\n",
    "        depths = self.depth_estimater.infer(\n",
    "            frames,\n",
    "            opts.near,\n",
    "            opts.far,\n",
    "            opts.depth_inference_steps,\n",
    "            opts.depth_guidance_scale,\n",
    "            window_size=opts.window_size,\n",
    "            overlap=opts.overlap,\n",
    "        ).to(opts.device)\n",
    "        \n",
    "        print(\"Converting frames to tensors...\")\n",
    "        frames_tensor = (\n",
    "            torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "        )\n",
    "        \n",
    "        print(\"Generating camera poses...\")\n",
    "        pose_s, pose_t, K = self.get_poses(opts, depths, num_frames=opts.video_length)\n",
    "        \n",
    "        # Calculate scene radius\n",
    "        radius = (\n",
    "            depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "            * opts.radius_scale\n",
    "        )\n",
    "        radius = min(radius, 5)\n",
    "        \n",
    "        return {\n",
    "            'frames_numpy': frames,\n",
    "            'frames_tensor': frames_tensor,\n",
    "            'depths': depths,\n",
    "            'pose_source': pose_s,\n",
    "            'pose_target': pose_t,\n",
    "            'intrinsics': K,\n",
    "            'radius': radius,\n",
    "            'trajectory_params': opts.target_pose if hasattr(opts, 'target_pose') else None\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def infer_gradual(self, opts):\n",
    "        frames = read_video_frames(\n",
    "            opts.video_path, opts.video_length, opts.stride, opts.max_res\n",
    "        )\n",
    "        # depths= self.depth_estimater.infer(frames, opts.near, opts.far).to(opts.device)\n",
    "        depths = self.depth_estimater.infer(\n",
    "            frames,\n",
    "            opts.near,\n",
    "            opts.far,\n",
    "            opts.depth_inference_steps,\n",
    "            opts.depth_guidance_scale,\n",
    "            window_size=opts.window_size,\n",
    "            overlap=opts.overlap,\n",
    "        ).to(opts.device)\n",
    "        frames = (\n",
    "            torch.from_numpy(frames).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "        )  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "        assert frames.shape[0] == opts.video_length\n",
    "        pose_s, pose_t, K = self.get_poses(opts, depths, num_frames=opts.video_length)\n",
    "        warped_images = []\n",
    "        masks = []\n",
    "        for i in tqdm(range(opts.video_length)):\n",
    "            warped_frame2, mask2, warped_depth2, flow12 = self.funwarp.forward_warp(\n",
    "                frames[i : i + 1],\n",
    "                None,\n",
    "                depths[i : i + 1],\n",
    "                pose_s[i : i + 1],\n",
    "                pose_t[i : i + 1],\n",
    "                K[i : i + 1],\n",
    "                None,\n",
    "                opts.mask,\n",
    "                twice=False,\n",
    "            )\n",
    "            warped_images.append(warped_frame2)\n",
    "            masks.append(mask2)\n",
    "        cond_video = (torch.cat(warped_images) + 1.0) / 2.0\n",
    "        cond_masks = torch.cat(masks)\n",
    "\n",
    "        frames = F.interpolate(\n",
    "            frames, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "        )\n",
    "        cond_video = F.interpolate(\n",
    "            cond_video, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "        )\n",
    "        cond_masks = F.interpolate(cond_masks, size=opts.sample_size, mode='nearest')\n",
    "        save_video(\n",
    "            (frames.permute(0, 2, 3, 1) + 1.0) / 2.0,\n",
    "            os.path.join(opts.save_dir, 'input.mp4'),\n",
    "            fps=opts.fps,\n",
    "        )\n",
    "        save_video(\n",
    "            cond_video.permute(0, 2, 3, 1),\n",
    "            os.path.join(opts.save_dir, 'render.mp4'),\n",
    "            fps=opts.fps,\n",
    "        )\n",
    "        save_video(\n",
    "            cond_masks.repeat(1, 3, 1, 1).permute(0, 2, 3, 1),\n",
    "            os.path.join(opts.save_dir, 'mask.mp4'),\n",
    "            fps=opts.fps,\n",
    "        )\n",
    "\n",
    "        frames = (frames.permute(1, 0, 2, 3).unsqueeze(0) + 1.0) / 2.0\n",
    "        frames_ref = frames[:, :, :10, :, :]\n",
    "        cond_video = cond_video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        cond_masks = (1.0 - cond_masks.permute(1, 0, 2, 3).unsqueeze(0)) * 255.0\n",
    "        generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "        return cond_video, cond_masks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_crafter = TrajCrafterAutoregressive(opts_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames\n",
    "from models.infer import DepthCrafterDemo\n",
    "import inference_orbits\n",
    "\n",
    "\n",
    "# Cell 3: Visualization Classes\n",
    "class VisualizationWarper(Warper):\n",
    "    \"\"\"Extended Warper class for 3D visualization\"\"\"\n",
    "    \n",
    "    def extract_3d_points_with_colors(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "        subsample_step: int = 10\n",
    "    ):\n",
    "        \"\"\"Extract 3D world points and their corresponding colors for visualization\"\"\"\n",
    "        b, c, h, w = frame1.shape\n",
    "        \n",
    "        # Move tensors to device\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation1 = transformation1.to(self.device).to(self.dtype)\n",
    "        intrinsic1 = intrinsic1.to(self.device).to(self.dtype)\n",
    "        \n",
    "        # Create subsampled pixel coordinates for performance\n",
    "        x_coords = torch.arange(0, w, subsample_step, dtype=torch.float32)\n",
    "        y_coords = torch.arange(0, h, subsample_step, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        x2d, y2d = torch.meshgrid(x_coords, y_coords, indexing='xy')\n",
    "        # x2d, y2d = torch.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "        x2d = x2d.to(depth1.device)\n",
    "        y2d = y2d.to(depth1.device)\n",
    "        ones_2d = torch.ones_like(x2d)\n",
    "        \n",
    "        # Stack into homogeneous coordinates\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[None, :, :, :, None]\n",
    "        \n",
    "        # Subsample depth and colors\n",
    "        depth_sub = depth1[:, 0, ::subsample_step, ::subsample_step]\n",
    "        colors_sub = frame1[:, :, ::subsample_step, ::subsample_step]\n",
    "        \n",
    "        # Unproject to 3D camera coordinates\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]\n",
    "        depth_4d = depth_sub[:, :, :, None, None]\n",
    "        \n",
    "        unnormalized_pos = torch.matmul(intrinsic1_inv_4d, pos_vectors_homo)\n",
    "        \n",
    "        # print('depth_4d', depth_4d.shape)\n",
    "        # print('unnormalized_pos', unnormalized_pos.shape)\n",
    "        \n",
    "        camera_points = depth_4d * unnormalized_pos\n",
    "        \n",
    "        # Transform to world coordinates\n",
    "        ones_4d = torch.ones(b, camera_points.shape[1], camera_points.shape[2], 1, 1).to(depth1)\n",
    "        world_points_homo = torch.cat([camera_points, ones_4d], dim=3)\n",
    "        trans_4d = transformation1[:, None, None]\n",
    "        world_points_homo = torch.matmul(trans_4d, world_points_homo)\n",
    "        world_points = world_points_homo[:, :, :, :3, 0]  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Prepare colors\n",
    "        colors = colors_sub.permute(0, 2, 3, 1)  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Filter valid points (positive depth)\n",
    "        valid_mask = depth_sub > 0  # (b, h_sub, w_sub)\n",
    "        \n",
    "        # Flatten and filter\n",
    "        points_3d = world_points[valid_mask]  # (N, 3)\n",
    "        colors_rgb = colors[valid_mask]       # (N, 3)\n",
    "        \n",
    "        return points_3d, colors_rgb\n",
    "    \n",
    "\n",
    "    def render_pointcloud_native_v2(\n",
    "        self,\n",
    "        points_3d: torch.Tensor,\n",
    "        colors_3d: torch.Tensor,\n",
    "        transformation_target: torch.Tensor,\n",
    "        intrinsic_target: torch.Tensor,\n",
    "        image_size: tuple = (576, 1024),\n",
    "        mask: bool = False\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Render point cloud by creating synthetic depth/frame and using forward_warp\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"using warper v2!!!\")\n",
    "        \n",
    "        h, w = image_size\n",
    "        b = transformation_target.shape[0]\n",
    "        device = self.device\n",
    "        \n",
    "        # Create a synthetic source frame with our point cloud data\n",
    "        source_frame = torch.full((b, 3, h, w), -1.0, device=device, dtype=self.dtype)\n",
    "        source_depth = torch.zeros(b, 1, h, w, device=device, dtype=self.dtype)\n",
    "        source_mask = torch.zeros(b, 1, h, w, device=device, dtype=self.dtype)\n",
    "        \n",
    "        # Project world points to source camera (identity transformation)\n",
    "        identity_transform = torch.eye(4, device=device, dtype=self.dtype).unsqueeze(0).repeat(b, 1, 1)\n",
    "        \n",
    "        # Convert world points to camera coordinates for the source view\n",
    "        ones = torch.ones(points_3d.shape[0], 1, device=device, dtype=self.dtype)\n",
    "        world_points_homo = torch.cat([points_3d, ones], dim=1)\n",
    "        \n",
    "        # For source camera, we can use identity or extract the original transformation\n",
    "        # Let's assume we're rendering from an identity camera position\n",
    "        camera_points = points_3d  # For identity camera position\n",
    "        \n",
    "        # Project to 2D\n",
    "        projected_homo = torch.matmul(intrinsic_target[0], camera_points.T).T\n",
    "        pixel_coords = projected_homo[:, :2] / projected_homo[:, 2:3]\n",
    "        depths_vals = camera_points[:, 2]\n",
    "        \n",
    "        # Filter valid points\n",
    "        valid_mask = (depths_vals > 0.01) & \\\n",
    "                    (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < w) & \\\n",
    "                    (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < h)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return (torch.full((b, 3, h, w), -1.0, device=device, dtype=self.dtype),\n",
    "                    torch.zeros(b, 1, h, w, device=device, dtype=self.dtype))\n",
    "        \n",
    "        valid_coords = pixel_coords[valid_mask]\n",
    "        valid_colors = colors_3d[valid_mask]\n",
    "        valid_depths = depths_vals[valid_mask]\n",
    "        \n",
    "        # Fill source frame/depth with our data\n",
    "        x_int = torch.clamp(torch.round(valid_coords[:, 0]).long(), 0, w-1)\n",
    "        y_int = torch.clamp(torch.round(valid_coords[:, 1]).long(), 0, h-1)\n",
    "        \n",
    "        for i in range(len(x_int)):\n",
    "            x, y = x_int[i], y_int[i]\n",
    "            source_frame[0, :, y, x] = valid_colors[i]\n",
    "            source_depth[0, 0, y, x] = valid_depths[i]\n",
    "            source_mask[0, 0, y, x] = 1.0\n",
    "        \n",
    "        # Now use the warper's forward_warp method\n",
    "        warped_frame, warped_mask, _, _ = self.forward_warp(\n",
    "            source_frame,\n",
    "            source_mask,\n",
    "            source_depth,\n",
    "            identity_transform,  # Source transformation (identity)\n",
    "            transformation_target,  # Target transformation\n",
    "            intrinsic_target,\n",
    "            None,  # Use same intrinsics\n",
    "            mask=mask,\n",
    "            twice=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Final warped range: [{warped_frame.min():.3f}, {warped_frame.max():.3f}]\")\n",
    "        print(f\"Valid pixels: {warped_mask.sum().item()}\")\n",
    "        \n",
    "        return warped_frame, warped_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import torch\n",
    "\n",
    "class GlobalPointCloudWarper(Warper):\n",
    "    def __init__(self, resolution: tuple = None, device: str = 'cuda', max_points: int = 1000000):\n",
    "        super().__init__(resolution, device)\n",
    "        self.max_points = max_points\n",
    "        self.device = device\n",
    "    \n",
    "    def lift_to_3d_pointcloud(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        mask1: Optional[torch.Tensor],\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Lift 2D image points to 3D world coordinates with colors.\n",
    "        \"\"\"\n",
    "        if self.resolution is not None:\n",
    "            assert frame1.shape[2:4] == self.resolution\n",
    "        b, c, h, w = frame1.shape\n",
    "        if mask1 is None:\n",
    "            mask1 = torch.ones(size=(b, 1, h, w), device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # Move tensors to device and convert to proper dtype\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        mask1 = mask1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation1 = transformation1.to(self.device).to(self.dtype)\n",
    "        intrinsic1 = intrinsic1.to(self.device).to(self.dtype)\n",
    "\n",
    "        # Create pixel coordinates directly on device\n",
    "        x1d = torch.arange(0, w, device=self.device, dtype=self.dtype)[None]\n",
    "        y1d = torch.arange(0, h, device=self.device, dtype=self.dtype)[:, None]\n",
    "        x2d = x1d.repeat([h, 1])  # (h, w)\n",
    "        y2d = y1d.repeat([1, w])  # (h, w)\n",
    "        ones_2d = torch.ones(size=(h, w), device=self.device, dtype=self.dtype)  # (h, w)\n",
    "        \n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[\n",
    "            None, :, :, :, None\n",
    "        ]  # (1, h, w, 3, 1)\n",
    "\n",
    "        # Rest of the function remains the same...\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)  # (b, 3, 3)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        depth_4d = depth1[:, 0][:, :, :, None, None]  # (b, h, w, 1, 1)\n",
    "\n",
    "        unnormalized_pos = torch.matmul(\n",
    "            intrinsic1_inv_4d, pos_vectors_homo\n",
    "        )  # (b, h, w, 3, 1)\n",
    "        \n",
    "        # Get 3D points in camera coordinate system\n",
    "        camera_points = depth_4d * unnormalized_pos  # (b, h, w, 3, 1)\n",
    "        \n",
    "        # Transform to world coordinates\n",
    "        ones_4d = torch.ones(size=(b, h, w, 1, 1), device=self.device, dtype=self.dtype)\n",
    "        camera_points_homo = torch.cat([camera_points, ones_4d], dim=3)  # (b, h, w, 4, 1)\n",
    "        \n",
    "        # Apply inverse transformation to get world coordinates\n",
    "        transformation1_inv = torch.linalg.inv(transformation1)  # (b, 4, 4)\n",
    "        transformation1_inv_4d = transformation1_inv[:, None, None]  # (b, 1, 1, 4, 4)\n",
    "        world_points_homo = torch.matmul(transformation1_inv_4d, camera_points_homo)  # (b, h, w, 4, 1)\n",
    "        world_points = world_points_homo[:, :, :, :3, 0]  # (b, h, w, 3)\n",
    "        \n",
    "        # Get colors (convert from channel-first to spatial layout)\n",
    "        colors = frame1.permute(0, 2, 3, 1)  # (b, h, w, 3)\n",
    "        \n",
    "        # Apply mask to filter out invalid points\n",
    "        valid_mask = mask1[:, 0, :, :].unsqueeze(-1)  # (b, h, w, 1)\n",
    "        world_points = world_points * valid_mask\n",
    "        colors = colors * valid_mask\n",
    "        \n",
    "        return world_points, colors\n",
    "\n",
    "    def create_pointcloud_from_image(\n",
    "        self,\n",
    "        frame: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "        depth: torch.Tensor,\n",
    "        transformation: torch.Tensor,\n",
    "        intrinsic: torch.Tensor,\n",
    "        confidence_weight: float = 1.0\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Create a point cloud from a single image\"\"\"\n",
    "        # Ensure all inputs are on the correct device\n",
    "        frame = frame.to(self.device).to(self.dtype)\n",
    "        depth = depth.to(self.device).to(self.dtype)\n",
    "        transformation = transformation.to(self.device).to(self.dtype)\n",
    "        intrinsic = intrinsic.to(self.device).to(self.dtype)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.to(self.device).to(self.dtype)\n",
    "        \n",
    "        # Lift 2D points to 3D world coordinates\n",
    "        world_points, colors = self.lift_to_3d_pointcloud(\n",
    "            frame, mask, depth, transformation, intrinsic\n",
    "        )\n",
    "        \n",
    "        # Flatten to point cloud format\n",
    "        b, h, w, _ = world_points.shape\n",
    "        if mask is None:\n",
    "            mask = torch.ones(b, 1, h, w, device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        # Only keep valid points\n",
    "        valid_mask = mask[:, 0, :, :].bool()  # (b, h, w)\n",
    "        \n",
    "        points = world_points[valid_mask]  # (N_valid, 3)\n",
    "        point_colors = colors[valid_mask]  # (N_valid, 3)\n",
    "        weights = torch.full(\n",
    "            (points.shape[0], 1), \n",
    "            confidence_weight, \n",
    "            device=self.device, \n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        \n",
    "        return points, point_colors, weights\n",
    "    \n",
    "    def merge_pointclouds(\n",
    "        self,\n",
    "        point_clouds: List[torch.Tensor],\n",
    "        colors_list: List[torch.Tensor], \n",
    "        weights_list: List[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Merge multiple point clouds into one\"\"\"\n",
    "        if not point_clouds:\n",
    "            return None, None, None\n",
    "            \n",
    "        merged_points = torch.cat(point_clouds, dim=0)\n",
    "        merged_colors = torch.cat(colors_list, dim=0)\n",
    "        merged_weights = torch.cat(weights_list, dim=0)\n",
    "        \n",
    "        return merged_points, merged_colors, merged_weights\n",
    "    \n",
    "    def downsample_pointcloud(\n",
    "        self,\n",
    "        points: torch.Tensor,\n",
    "        colors: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "        max_points: Optional[int] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Downsample a point cloud to max_points\"\"\"\n",
    "        if max_points is None:\n",
    "            max_points = self.max_points\n",
    "            \n",
    "        if points.shape[0] <= max_points:\n",
    "            return points, colors, weights\n",
    "            \n",
    "        # Random sampling weighted by confidence\n",
    "        probabilities = (weights / weights.sum()).squeeze()\n",
    "        indices = torch.multinomial(probabilities, max_points, replacement=False)\n",
    "        \n",
    "        return points[indices], colors[indices], weights[indices]\n",
    "    \n",
    "    def render_from_camera(\n",
    "        self,\n",
    "        points: torch.Tensor,\n",
    "        colors: torch.Tensor,\n",
    "        weights: torch.Tensor,\n",
    "        transformation: torch.Tensor,\n",
    "        intrinsic: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        depth_threshold: float = 100000.0,\n",
    "        point_size: float = 2.0,  # Add this parameter\n",
    "        use_bilinear: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Render point cloud from a target camera viewpoint\n",
    "        \n",
    "        :param points: (N, 3) world coordinates  \n",
    "        :param colors: (N, 3) RGB colors\n",
    "        :param weights: (N, 1) confidence weights\n",
    "        :param transformation: (b, 4, 4) target camera extrinsic matrix\n",
    "        :param intrinsic: (b, 3, 3) target camera intrinsic matrix\n",
    "        :param target_height: output image height\n",
    "        :param target_width: output image width\n",
    "        :param depth_threshold: maximum depth to render\n",
    "        :return: rendered_image: (b, 3, h, w), mask: (b, 1, h, w)\n",
    "        \"\"\"\n",
    "        if points is None or points.shape[0] == 0:\n",
    "            b = transformation.shape[0]\n",
    "            return (\n",
    "                torch.zeros(b, 3, target_height, target_width, device=self.device),\n",
    "                torch.zeros(b, 1, target_height, target_width, device=self.device)\n",
    "            )\n",
    "        \n",
    "        b = transformation.shape[0]\n",
    "        point_count = points.shape[0]\n",
    "        \n",
    "        # Transform world points to camera coordinates\n",
    "        points_homo = torch.cat([\n",
    "            points, \n",
    "            torch.ones(point_count, 1, device=self.device)\n",
    "        ], dim=1)  # (N, 4)\n",
    "        \n",
    "        # Apply transformation for each batch item\n",
    "        rendered_images = []\n",
    "        rendered_masks = []\n",
    "        \n",
    "        for batch_idx in range(b):\n",
    "            # Transform to camera space\n",
    "            camera_points_homo = torch.matmul(\n",
    "                transformation[batch_idx], points_homo.T\n",
    "            ).T  # (N, 4)\n",
    "            camera_points = camera_points_homo[:, :3]  # (N, 3)\n",
    "            \n",
    "            # Filter points behind camera and too far\n",
    "            valid_depth = (camera_points[:, 2] > 0.1) & (camera_points[:, 2] < depth_threshold)\n",
    "            valid_points = camera_points[valid_depth]\n",
    "            valid_colors = colors[valid_depth]\n",
    "            valid_weights = weights[valid_depth]\n",
    "            \n",
    "            if valid_points.shape[0] == 0:\n",
    "                rendered_images.append(torch.zeros(3, target_height, target_width, device=self.device))\n",
    "                rendered_masks.append(torch.zeros(1, target_height, target_width, device=self.device))\n",
    "                continue\n",
    "            \n",
    "            # Project to 2D\n",
    "            projected = torch.matmul(intrinsic[batch_idx], valid_points.T).T  # (N, 3)\n",
    "            pixel_coords = projected[:, :2] / projected[:, 2:3]  # (N, 2)\n",
    "            depths = projected[:, 2]  # (N,)\n",
    "            \n",
    "            # Filter points outside image bounds\n",
    "            in_bounds = (\n",
    "                (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < target_width) &\n",
    "                (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < target_height)\n",
    "            )\n",
    "            \n",
    "            final_coords = pixel_coords[in_bounds]\n",
    "            final_colors = valid_colors[in_bounds]\n",
    "            final_depths = depths[in_bounds]\n",
    "            final_weights = valid_weights[in_bounds]\n",
    "            \n",
    "            # Choose splatting method\n",
    "            if use_bilinear:\n",
    "                rendered_image, rendered_mask = self._splat_points_bilinear_efficient(\n",
    "                    final_coords, final_colors, final_depths, final_weights,\n",
    "                    target_height, target_width\n",
    "                )\n",
    "            else:\n",
    "                rendered_image, rendered_mask = self._splat_points_to_image_with_size(\n",
    "                    final_coords, final_colors, final_depths, final_weights,\n",
    "                    target_height, target_width, point_size=point_size\n",
    "                )\n",
    "            \n",
    "            rendered_images.append(rendered_image)\n",
    "            rendered_masks.append(rendered_mask)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(rendered_images, dim=0),\n",
    "            torch.stack(rendered_masks, dim=0)\n",
    "        )\n",
    "    \n",
    "    def _splat_points_to_image_with_size(\n",
    "        self,\n",
    "        coords: torch.Tensor,  # (N, 2)\n",
    "        colors: torch.Tensor,  # (N, 3)\n",
    "        depths: torch.Tensor,  # (N,)\n",
    "        weights: torch.Tensor,  # (N, 1)\n",
    "        height: int,\n",
    "        width: int,\n",
    "        point_size: float = 2.0  # Controllable point size\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Render points with controllable size to avoid holes\"\"\"\n",
    "        \n",
    "        if coords.shape[0] == 0:\n",
    "            return (\n",
    "                torch.zeros(3, height, width, device=self.device),\n",
    "                torch.zeros(1, height, width, device=self.device)\n",
    "            )\n",
    "        \n",
    "        # Depth weighting\n",
    "        normalized_depths = depths / (depths.max() + 1e-8)\n",
    "        depth_weights = torch.exp(-normalized_depths * 2)\n",
    "        total_weights = weights.squeeze() * depth_weights\n",
    "        \n",
    "        # Create splat pattern based on point size\n",
    "        radius = int(point_size / 2)\n",
    "        offsets = []\n",
    "        splat_weights = []\n",
    "        \n",
    "        for dy in range(-radius, radius + 1):\n",
    "            for dx in range(-radius, radius + 1):\n",
    "                dist = (dx**2 + dy**2)**0.5\n",
    "                if dist <= point_size / 2:\n",
    "                    offsets.append([dx, dy])\n",
    "                    # Gaussian falloff\n",
    "                    weight = torch.exp(torch.tensor(-dist**2 / (2 * (point_size/4)**2)))\n",
    "                    splat_weights.append(weight)\n",
    "        \n",
    "        offsets = torch.tensor(offsets, device=self.device)\n",
    "        splat_weights = torch.tensor(splat_weights, device=self.device)\n",
    "        \n",
    "        # Expand coordinates for all splat positions\n",
    "        n_points = coords.shape[0]\n",
    "        n_splats = len(offsets)\n",
    "        \n",
    "        # Repeat coordinates for each splat offset\n",
    "        expanded_coords = coords[:, None, :] + offsets[None, :, :]  # (N, n_splats, 2)\n",
    "        expanded_coords = expanded_coords.view(-1, 2)  # (N*n_splats, 2)\n",
    "        \n",
    "        # Repeat colors and weights\n",
    "        expanded_colors = colors[:, None, :].repeat(1, n_splats, 1).view(-1, 3)\n",
    "        expanded_weights = total_weights[:, None].repeat(1, n_splats).view(-1)\n",
    "        expanded_splat_weights = splat_weights[None, :].repeat(n_points, 1).view(-1)\n",
    "        \n",
    "        # Apply splat weights\n",
    "        final_weights = expanded_weights * expanded_splat_weights\n",
    "        \n",
    "        # Filter out-of-bounds\n",
    "        pixel_coords = torch.round(expanded_coords).long()\n",
    "        valid_mask = (\n",
    "            (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < width) &\n",
    "            (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < height)\n",
    "        )\n",
    "        \n",
    "        if not valid_mask.any():\n",
    "            return (\n",
    "                torch.zeros(3, height, width, device=self.device),\n",
    "                torch.zeros(1, height, width, device=self.device)\n",
    "            )\n",
    "        \n",
    "        valid_coords = pixel_coords[valid_mask]\n",
    "        valid_colors = expanded_colors[valid_mask]\n",
    "        valid_weights = final_weights[valid_mask]\n",
    "        \n",
    "        # Render using scatter\n",
    "        linear_indices = valid_coords[:, 1] * width + valid_coords[:, 0]\n",
    "        \n",
    "        color_buffer = torch.zeros(3, height * width, device=self.device)\n",
    "        weight_buffer = torch.zeros(height * width, device=self.device)\n",
    "        \n",
    "        weighted_colors = valid_colors.T * valid_weights\n",
    "        \n",
    "        for c in range(3):\n",
    "            color_buffer[c].scatter_add_(0, linear_indices, weighted_colors[c])\n",
    "        \n",
    "        weight_buffer.scatter_add_(0, linear_indices, valid_weights)\n",
    "        \n",
    "        # Normalize\n",
    "        valid_pixels = weight_buffer > 1e-6\n",
    "        color_buffer[:, valid_pixels] /= weight_buffer[valid_pixels]\n",
    "        \n",
    "        return color_buffer.view(3, height, width), valid_pixels.float().view(1, height, width)\n",
    "    \n",
    "    \n",
    "    def _splat_points_bilinear_efficient(\n",
    "        self,\n",
    "        coords: torch.Tensor,  # (N, 2)\n",
    "        colors: torch.Tensor,  # (N, 3)\n",
    "        depths: torch.Tensor,  # (N,)\n",
    "        weights: torch.Tensor,  # (N, 1)\n",
    "        height: int,\n",
    "        width: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Efficient bilinear splatting matching original warper behavior\"\"\"\n",
    "        \n",
    "        if coords.shape[0] == 0:\n",
    "            return (\n",
    "                torch.zeros(3, height, width, device=self.device),\n",
    "                torch.zeros(1, height, width, device=self.device)\n",
    "            )\n",
    "        \n",
    "        # Match original depth weighting\n",
    "        # sat_depth = torch.clamp(depths, min=0.1, max=1000)\n",
    "        # log_depth = torch.log(1 + sat_depth)\n",
    "        # depth_weights = torch.exp(log_depth / log_depth.max() * 50)\n",
    "        # point_weights = weights.squeeze() / depth_weights\n",
    "        \n",
    "        # Alternative: gentler depth weighting that preserves distant points\n",
    "        normalized_depths = (depths - depths.min()) / (depths.max() - depths.min() + 1e-8)\n",
    "        depth_weights = torch.exp(-normalized_depths * 2)  # Gentler falloff\n",
    "        point_weights = weights.squeeze() * depth_weights\n",
    "        \n",
    "        \n",
    "        # Bilinear interpolation coordinates (matches original)\n",
    "        coords_floor = torch.floor(coords)\n",
    "        coords_frac = coords - coords_floor\n",
    "        \n",
    "        # Four corner coordinates\n",
    "        x0, y0 = coords_floor[:, 0].long(), coords_floor[:, 1].long()\n",
    "        x1, y1 = x0 + 1, y0 + 1\n",
    "        \n",
    "        # Bilinear weights (exactly matches original implementation)\n",
    "        dx, dy = coords_frac[:, 0], coords_frac[:, 1]\n",
    "        w_nw = (1 - dx) * (1 - dy) * point_weights  # top-left\n",
    "        w_ne = dx * (1 - dy) * point_weights        # top-right  \n",
    "        w_sw = (1 - dx) * dy * point_weights        # bottom-left\n",
    "        w_se = dx * dy * point_weights              # bottom-right\n",
    "        \n",
    "        # Vectorized approach: collect all coordinates and weights\n",
    "        all_x = torch.cat([x0, x1, x0, x1])\n",
    "        all_y = torch.cat([y0, y0, y1, y1])\n",
    "        all_weights = torch.cat([w_nw, w_ne, w_sw, w_se])\n",
    "        all_colors = torch.cat([colors, colors, colors, colors], dim=0)\n",
    "        \n",
    "        # Filter valid coordinates (within bounds and non-zero weight)\n",
    "        valid_mask = (\n",
    "            (all_x >= 0) & (all_x < width) &\n",
    "            (all_y >= 0) & (all_y < height) &\n",
    "            (all_weights > 1e-8)\n",
    "        )\n",
    "        \n",
    "        if not valid_mask.any():\n",
    "            return (\n",
    "                torch.zeros(3, height, width, device=self.device),\n",
    "                torch.zeros(1, height, width, device=self.device)\n",
    "            )\n",
    "        \n",
    "        # Extract valid data\n",
    "        valid_x = all_x[valid_mask]\n",
    "        valid_y = all_y[valid_mask]\n",
    "        valid_weights = all_weights[valid_mask]\n",
    "        valid_colors = all_colors[valid_mask]\n",
    "        \n",
    "        # Convert to linear indices for scatter operations\n",
    "        linear_indices = valid_y * width + valid_x\n",
    "        \n",
    "        # GPU-efficient scatter accumulation\n",
    "        color_buffer = torch.zeros(3, height * width, device=self.device)\n",
    "        weight_buffer = torch.zeros(height * width, device=self.device)\n",
    "        \n",
    "        # Vectorized color accumulation\n",
    "        weighted_colors = valid_colors.T * valid_weights  # (3, N_valid)\n",
    "        \n",
    "        for c in range(3):\n",
    "            color_buffer[c].scatter_add_(0, linear_indices, weighted_colors[c])\n",
    "        \n",
    "        weight_buffer.scatter_add_(0, linear_indices, valid_weights)\n",
    "        \n",
    "        # Normalize colors by accumulated weights\n",
    "        valid_pixels = weight_buffer > 1e-8\n",
    "        color_buffer[:, valid_pixels] /= weight_buffer[valid_pixels]\n",
    "        \n",
    "        # Reshape back to 2D\n",
    "        color_image = color_buffer.view(3, height, width)\n",
    "        mask_image = valid_pixels.float().view(1, height, width)\n",
    "        \n",
    "        return color_image, mask_image\n",
    "        \n",
    "    def render_pointcloud_zbuffer_vectorized(\n",
    "        self,\n",
    "        points_3d: torch.Tensor,\n",
    "        colors_3d: torch.Tensor,\n",
    "        transformation_target: torch.Tensor,\n",
    "        intrinsic_target: torch.Tensor,\n",
    "        image_size: tuple = (576, 1024)\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fully vectorized Z-buffer rendering - no loops!\n",
    "        \"\"\"\n",
    "        \n",
    "        h, w = image_size\n",
    "        device = self.device\n",
    "        \n",
    "        # Transform and project (same as before)\n",
    "        ones = torch.ones(points_3d.shape[0], 1, device=device, dtype=self.dtype)\n",
    "        world_points_homo = torch.cat([points_3d, ones], dim=1)\n",
    "        camera_points_homo = torch.matmul(transformation_target[0], world_points_homo.T).T\n",
    "        camera_points = camera_points_homo[:, :3]\n",
    "        \n",
    "        projected_homo = torch.matmul(intrinsic_target[0], camera_points.T).T\n",
    "        pixel_coords = projected_homo[:, :2] / projected_homo[:, 2:3]\n",
    "        depths_vals = camera_points[:, 2]\n",
    "        \n",
    "        # Filter valid points\n",
    "        valid_mask = (depths_vals > 0.01) & \\\n",
    "                    (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < w) & \\\n",
    "                    (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < h)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return (torch.full((1, 3, h, w), -1.0, device=device),\n",
    "                    torch.zeros(1, 1, h, w, device=device))\n",
    "        \n",
    "        valid_coords = pixel_coords[valid_mask]\n",
    "        valid_colors = colors_3d[valid_mask]\n",
    "        valid_depths = depths_vals[valid_mask]\n",
    "        \n",
    "        # Convert to integer coordinates\n",
    "        x_int = torch.clamp(torch.round(valid_coords[:, 0]).long(), 0, w-1)\n",
    "        y_int = torch.clamp(torch.round(valid_coords[:, 1]).long(), 0, h-1)\n",
    "        linear_indices = y_int * w + x_int\n",
    "        \n",
    "        # Vectorized Z-buffer using scatter_reduce (PyTorch 1.12+)\n",
    "        # This finds the minimum depth per pixel and corresponding colors\n",
    "        \n",
    "        # Method 1: Use unique + scatter for closest point per pixel\n",
    "        unique_indices, inverse_indices = torch.unique(linear_indices, return_inverse=True)\n",
    "        \n",
    "        # For each unique pixel, find the point with minimum depth\n",
    "        min_depths = torch.full((len(unique_indices),), float('inf'), device=device)\n",
    "        min_depths.scatter_reduce_(0, inverse_indices, valid_depths, reduce='amin')\n",
    "        \n",
    "        # Create mask for points that have minimum depth at their pixel\n",
    "        expanded_min_depths = min_depths[inverse_indices]\n",
    "        closest_mask = (valid_depths == expanded_min_depths)\n",
    "        \n",
    "        # Keep only the closest points\n",
    "        final_indices = linear_indices[closest_mask]\n",
    "        final_colors = valid_colors[closest_mask]\n",
    "        final_depths = valid_depths[closest_mask]\n",
    "        \n",
    "        # Render final result\n",
    "        color_buffer = torch.full((3, h * w), -1.0, device=device)\n",
    "        depth_buffer = torch.full((h * w,), 0.0, device=device)\n",
    "        \n",
    "        # Scatter the closest colors (no conflicts now since we filtered to closest only)\n",
    "        color_buffer[:, final_indices] = final_colors.T\n",
    "        depth_buffer[final_indices] = final_depths\n",
    "        \n",
    "        # Reshape\n",
    "        final_frame = color_buffer.view(3, h, w).unsqueeze(0)\n",
    "        final_mask = (depth_buffer > 0).float().view(1, 1, h, w)\n",
    "        \n",
    "        return final_frame, final_mask\n",
    "    \n",
    "    \n",
    "    def render_pointcloud_zbuffer_vectorized_point_size(\n",
    "        self,\n",
    "        points_3d: torch.Tensor,\n",
    "        colors_3d: torch.Tensor,\n",
    "        transformation_target: torch.Tensor,\n",
    "        intrinsic_target: torch.Tensor,\n",
    "        image_size: tuple = (576, 1024),\n",
    "        point_size: int = 1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Vectorized Z-buffer rendering with proper color handling for larger points\n",
    "        \"\"\"\n",
    "        \n",
    "        h, w = image_size\n",
    "        device = self.device\n",
    "        \n",
    "        # Transform and project (same as before)\n",
    "        ones = torch.ones(points_3d.shape[0], 1, device=device, dtype=self.dtype)\n",
    "        world_points_homo = torch.cat([points_3d, ones], dim=1)\n",
    "        camera_points_homo = torch.matmul(transformation_target[0], world_points_homo.T).T\n",
    "        camera_points = camera_points_homo[:, :3]\n",
    "        \n",
    "        projected_homo = torch.matmul(intrinsic_target[0], camera_points.T).T\n",
    "        pixel_coords = projected_homo[:, :2] / projected_homo[:, 2:3]\n",
    "        depths_vals = camera_points[:, 2]\n",
    "        \n",
    "        # Filter valid points\n",
    "        valid_mask = (depths_vals > 0.01) & \\\n",
    "                    (pixel_coords[:, 0] >= 0) & (pixel_coords[:, 0] < w) & \\\n",
    "                    (pixel_coords[:, 1] >= 0) & (pixel_coords[:, 1] < h)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return (torch.full((1, 3, h, w), -1.0, device=device),\n",
    "                    torch.zeros(1, 1, h, w, device=device))\n",
    "        \n",
    "        valid_coords = pixel_coords[valid_mask]\n",
    "        valid_colors = colors_3d[valid_mask]\n",
    "        valid_depths = depths_vals[valid_mask]\n",
    "        \n",
    "        # Generate splat pattern - FIXED VERSION\n",
    "        if point_size == 1:\n",
    "            splat_offsets = torch.tensor([[0, 0]], device=device)\n",
    "            splat_weights = torch.tensor([1.0], device=device)\n",
    "        else:\n",
    "            radius = point_size // 2\n",
    "            offsets = []\n",
    "            weights = []\n",
    "            \n",
    "            for dy in range(-radius, radius + 1):\n",
    "                for dx in range(-radius, radius + 1):\n",
    "                    # Use square pattern with uniform weights\n",
    "                    if abs(dx) <= radius and abs(dy) <= radius:\n",
    "                        offsets.append([dx, dy])\n",
    "                        \n",
    "                        # OPTION 1: Uniform weights (prevents graying)\n",
    "                        weights.append(1.0)\n",
    "                        \n",
    "                        # OPTION 2: Gentle falloff that preserves brightness\n",
    "                        # dist = max(abs(dx), abs(dy))  # Square distance\n",
    "                        # weight = 1.0 - (dist / (radius + 1)) * 0.3  # Only 30% falloff\n",
    "                        # weights.append(weight)\n",
    "            \n",
    "            splat_offsets = torch.tensor(offsets, device=device)\n",
    "            splat_weights = torch.tensor(weights, device=device)\n",
    "        \n",
    "        n_points = len(valid_coords)\n",
    "        n_splats = len(splat_offsets)\n",
    "        \n",
    "        # Expand points for each splat offset\n",
    "        expanded_coords = valid_coords[:, None, :] + splat_offsets[None, :, :]\n",
    "        expanded_coords = expanded_coords.reshape(-1, 2)\n",
    "        \n",
    "        # Repeat colors and depths\n",
    "        expanded_colors = valid_colors[:, None, :].repeat(1, n_splats, 1).reshape(-1, 3)\n",
    "        expanded_depths = valid_depths[:, None].repeat(1, n_splats).reshape(-1)\n",
    "        \n",
    "        # DON'T multiply colors by splat weights - keep original intensity\n",
    "        # expanded_colors = expanded_colors * expanded_splat_weights[:, None]  # REMOVE THIS\n",
    "        \n",
    "        # Convert to integer coordinates\n",
    "        x_int = torch.round(expanded_coords[:, 0]).long()\n",
    "        y_int = torch.round(expanded_coords[:, 1]).long()\n",
    "        \n",
    "        bounds_mask = (x_int >= 0) & (x_int < w) & (y_int >= 0) & (y_int < h)\n",
    "        \n",
    "        if bounds_mask.sum() == 0:\n",
    "            return (torch.full((1, 3, h, w), -1.0, device=device),\n",
    "                    torch.zeros(1, 1, h, w, device=device))\n",
    "        \n",
    "        final_x = x_int[bounds_mask]\n",
    "        final_y = y_int[bounds_mask]\n",
    "        final_colors = expanded_colors[bounds_mask]\n",
    "        final_depths = expanded_depths[bounds_mask]\n",
    "        \n",
    "        linear_indices = final_y * w + final_x\n",
    "        \n",
    "        # Z-buffer logic (same as before)\n",
    "        unique_indices, inverse_indices = torch.unique(linear_indices, return_inverse=True)\n",
    "        \n",
    "        min_depths = torch.full((len(unique_indices),), float('inf'), device=device)\n",
    "        min_depths.scatter_reduce_(0, inverse_indices, final_depths, reduce='amin')\n",
    "        \n",
    "        expanded_min_depths = min_depths[inverse_indices]\n",
    "        closest_mask = (final_depths == expanded_min_depths)\n",
    "        \n",
    "        winner_indices = linear_indices[closest_mask]\n",
    "        winner_colors = final_colors[closest_mask]\n",
    "        winner_depths = final_depths[closest_mask]\n",
    "        \n",
    "        # Render buffers\n",
    "        color_buffer = torch.full((3, h * w), -1.0, device=device)\n",
    "        depth_buffer = torch.full((h * w,), 0.0, device=device)\n",
    "        \n",
    "        color_buffer[:, winner_indices] = winner_colors.T\n",
    "        depth_buffer[winner_indices] = winner_depths\n",
    "        \n",
    "        # Reshape\n",
    "        final_frame = color_buffer.view(3, h, w).unsqueeze(0)\n",
    "        final_mask = (depth_buffer > 0).float().view(1, 1, h, w)\n",
    "        \n",
    "        return final_frame, final_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funwarp = VisualizationWarper(device=opts.device)\n",
    "funwarp = GlobalPointCloudWarper(device=opts.device, max_points=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import models.utils as utils\n",
    "\n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS (moved outside class)\n",
    "# ============================================================================\n",
    "\n",
    "def pad_video(frames, target_length):\n",
    "    if frames.shape[0] < target_length:\n",
    "        last_frame = frames[-1:]\n",
    "        num_pad = target_length - frames.shape[0]\n",
    "        pad_frames = np.repeat(last_frame, num_pad, axis=0)\n",
    "        frames = np.concatenate([frames, pad_frames], axis=0)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def generate_traj_specified(c2ws_anchor, target_pose, n_frames, device):\n",
    "    theta, phi, d_r, d_x, d_y = target_pose\n",
    "    \n",
    "    thetas = np.linspace(0, theta, n_frames)  \n",
    "    phis = np.linspace(0, phi, n_frames)          \n",
    "    rs = np.linspace(0, d_r, n_frames)            \n",
    "    xs = np.linspace(0, d_x, n_frames)            \n",
    "    ys = np.linspace(0, d_y, n_frames)            \n",
    "    \n",
    "    c2ws_list = []\n",
    "    for th, ph, r, x, y in zip(thetas, phis, rs, xs, ys):\n",
    "        c2w_new = sphere2pose(\n",
    "            c2ws_anchor,\n",
    "            np.float32(th),\n",
    "            np.float32(ph),\n",
    "            np.float32(r),\n",
    "            device,\n",
    "            np.float32(x),\n",
    "            np.float32(y),\n",
    "        )\n",
    "        c2ws_list.append(c2w_new)\n",
    "    c2ws = torch.cat(c2ws_list, dim=0)\n",
    "    return c2ws\n",
    "\n",
    "\n",
    "def save_poses_torch(c2ws, filepath):\n",
    "    \"\"\"Save camera poses as PyTorch tensor (.pth file)\"\"\"\n",
    "    torch.save(c2ws.cpu(), filepath)\n",
    "\n",
    "def save_point_clouds_torch(pc_list, color_list, dirpath):\n",
    "    \"\"\"Save point clouds as PyTorch tensors (much faster than text files)\"\"\"\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    \n",
    "    # Save as individual tensor files\n",
    "    for idx, (pc, color) in enumerate(zip(pc_list, color_list)):\n",
    "        # Save points and colors as separate tensors\n",
    "        torch.save(pc.cpu(), os.path.join(dirpath, f'points_{idx:03d}.pth'))\n",
    "        torch.save(color.cpu(), os.path.join(dirpath, f'colors_{idx:03d}.pth'))\n",
    "    \n",
    "\n",
    "def save_segment_results(pc_input, color_input, pc_inpainted, color_inpainted, \n",
    "                        pc_merged, color_merged, traj_segment, opts, segment_idx):\n",
    "    # Function to save results for each segment\n",
    "    # Implementation needed\n",
    "    stage_dir = Path(opts.save_dir) / f'stage_{segment_idx+1}'\n",
    "    stage_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    save_point_clouds_torch(pc_input, color_input, stage_dir / 'point_cloud_input')\n",
    "    # save_point_clouds_torch(pc_inpainted, color_inpainted, stage_dir / 'point_cloud_inpainted')\n",
    "    # save_point_clouds_torch(pc_merged, color_merged, stage_dir / 'point_cloud_merged')\n",
    "    save_poses_torch(traj_segment, stage_dir / 'cameras_target.pth')\n",
    "\n",
    "\n",
    "def clean_single_mask_simple(mask_tensor, kernel_size=3, n_erosion_steps=3, n_dilation_steps=2):\n",
    "    \"\"\"Simpler mask cleaning without size mismatch issues\"\"\"\n",
    "\n",
    "    # Handle different input dimensions\n",
    "    if mask_tensor.dim() == 4:  # (B, C, H, W)\n",
    "        frame_mask = mask_tensor\n",
    "    elif mask_tensor.dim() == 3:  # (C, H, W)\n",
    "        frame_mask = mask_tensor.unsqueeze(0)  # Add batch dim -> (1, C, H, W)\n",
    "    elif mask_tensor.dim() == 2:  # (H, W)\n",
    "        frame_mask = mask_tensor.unsqueeze(0).unsqueeze(0)  # -> (1, 1, H, W)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mask dimensions: {mask_tensor.shape}\")\n",
    "    \n",
    "    # Ensure we have single channel\n",
    "    if frame_mask.shape[1] > 1:\n",
    "        frame_mask = frame_mask[:, 0:1]  # Take first channel\n",
    "    \n",
    "    # Binarize the mask\n",
    "    binary_mask = (frame_mask > 0.5).float()\n",
    "    \n",
    "    # Simple morphological opening (erosion followed by dilation)\n",
    "    padding = kernel_size // 2\n",
    "    \n",
    "    # Erosion\n",
    "    \n",
    "    for _ in range(n_erosion_steps):\n",
    "        binary_mask = -F.max_pool2d(-binary_mask, kernel_size, stride=1, padding=padding)\n",
    "    \n",
    "    # Dilation\n",
    "    for _ in range(n_dilation_steps):\n",
    "        binary_mask = F.max_pool2d(binary_mask, kernel_size, stride=1, padding=padding)\n",
    "    \n",
    "    # print(binary_mask.shape, frame_mask.shape)\n",
    "    \n",
    "    cleaned = binary_mask * (frame_mask > 0.5).float()\n",
    "    \n",
    "    # Return in same format as input\n",
    "    if mask_tensor.dim() == 2:\n",
    "        return cleaned.squeeze()  # (H, W)\n",
    "    elif mask_tensor.dim() == 3:\n",
    "        return cleaned.squeeze(0)  # (C, H, W)\n",
    "    else:\n",
    "        return cleaned  # (B, C, H, W)\n",
    "    \n",
    "    \n",
    "def align_depth_maps(depth1, depth2):\n",
    "    \"\"\"\n",
    "    Align two depth maps from the same viewpoint\n",
    "    \n",
    "    Args:\n",
    "        depth1, depth2: Depth maps (H, W) or (1, H, W)\n",
    "        mask1, mask2: Valid pixel masks\n",
    "    \"\"\"\n",
    "    \n",
    "    # masks = 1 if depth < 10, 0 otherwise\n",
    "    mask1 = (depth1 < 10).float()\n",
    "    mask2 = (depth2 < 10).float()\n",
    "    \n",
    "    \n",
    "    # Flatten depth maps\n",
    "    if depth1.dim() == 3:\n",
    "        depth1 = depth1.squeeze(0)\n",
    "        depth2 = depth2.squeeze(0)\n",
    "    \n",
    "    d1_flat = depth1.flatten()\n",
    "    d2_flat = depth2.flatten()\n",
    "    \n",
    "    \n",
    "    # Apply masks if provided\n",
    "    if mask1 is not None and mask2 is not None:\n",
    "        if mask1.dim() == 3:\n",
    "            mask1 = mask1.squeeze(0)\n",
    "            mask2 = mask2.squeeze(0)\n",
    "        \n",
    "        valid_mask = (mask1.flatten() > 0.5) & (mask2.flatten() > 0.5)\n",
    "        d1_flat = d1_flat[valid_mask]\n",
    "        d2_flat = d2_flat[valid_mask]\n",
    "    \n",
    "    # Remove invalid depths\n",
    "    valid_depths = (d1_flat > 0) & (d2_flat > 0) & torch.isfinite(d1_flat) & torch.isfinite(d2_flat)\n",
    "    d1_valid = d1_flat[valid_depths]\n",
    "    d2_valid = d2_flat[valid_depths]\n",
    "    \n",
    "    if len(d1_valid) == 0:\n",
    "        return depth1, depth2, 1.0\n",
    "    \n",
    "    # Compute scale factor using robust estimation\n",
    "    ratios = d1_valid / (d2_valid + 1e-8)\n",
    "    \n",
    "    # Remove outliers\n",
    "    q25, q75 = torch.quantile(ratios, 0.25), torch.quantile(ratios, 0.75)\n",
    "    iqr = q75 - q25\n",
    "    inlier_mask = (ratios >= q25 - 1.5*iqr) & (ratios <= q75 + 1.5*iqr)\n",
    "    \n",
    "    if inlier_mask.sum() > 0:\n",
    "        scale_factor = torch.median(ratios[inlier_mask])\n",
    "    else:\n",
    "        scale_factor = torch.median(ratios)\n",
    "    \n",
    "    # Scale the second depth map\n",
    "    # depth2_aligned = depth2 * scale_factor\n",
    "    \n",
    "    return scale_factor\n",
    "\n",
    "\n",
    "def load_video_frames(video_path, video_length, stride, max_res, device, reverse=False):\n",
    "    \"\"\"\n",
    "    Load video frames with optional reversal\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        video_length: Number of frames to load\n",
    "        stride: Frame sampling stride\n",
    "        max_res: Maximum resolution\n",
    "        device: Target device\n",
    "        reverse: Whether to reverse the video frames\n",
    "    \n",
    "    Returns:\n",
    "        frames_np: NumPy array of frames (T, H, W, 3) in [0,1]\n",
    "        frames_tensor: PyTorch tensor (T, 3, H, W) in [-1,1]\n",
    "    \"\"\"\n",
    "    # Load frames\n",
    "    frames_np = utils.read_video_frames(\n",
    "        video_path, video_length, stride, max_res,\n",
    "        # height=opts.sample_size[0], width=opts.sample_size[1],\n",
    "    )\n",
    "\n",
    "    # Pad if too short\n",
    "    frames_np = pad_video(frames_np, video_length)\n",
    "    \n",
    "    # Reverse the video if requested\n",
    "    if reverse:\n",
    "        frames_np = frames_np[::-1, ...].copy()\n",
    "\n",
    "    # Convert to tensor version\n",
    "    frames_tensor = (\n",
    "        torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(device) * 2.0 - 1.0\n",
    "    )  # T H W 3 -> T 3 H W, [-1,1]\n",
    "    \n",
    "    assert frames_tensor.shape[0] == video_length\n",
    "    \n",
    "    return frames_np, frames_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_diffusion(\n",
    "    vis_crafter,\n",
    "    frames_tensor,    # [T, 3, H, W], in [-1, 1]\n",
    "    warped_images,    # list of warped images tensors\n",
    "    frames_ref,\n",
    "    masks,            # list of mask tensors\n",
    "    opts,\n",
    "    segment_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run diffusion sampling for a given stage.\n",
    "    \n",
    "    TODO: ALWAYS PASS ORIGINAL FRAMES AS REFERENCE FRAMES\n",
    "    \"\"\"\n",
    "\n",
    "    # --- determine output directory ---\n",
    "    if not segment_dir:\n",
    "        n_subdirs = len([\n",
    "            name for name in os.listdir(opts.save_dir)\n",
    "            if os.path.isdir(os.path.join(opts.save_dir, name))\n",
    "        ])\n",
    "        segment_dir = os.path.join(opts.save_dir, f'stage_{n_subdirs + 1}')\n",
    "        os.makedirs(segment_dir, exist_ok=True)\n",
    "        print(f\"Saving to: {segment_dir}\")\n",
    "\n",
    "    # --- build conditioning tensors ---\n",
    "    cond_video = (torch.cat(warped_images) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "    cond_masks = torch.cat(masks)  # [T, 1, H, W]\n",
    "\n",
    "    # --- resize inputs to diffusion sample size ---\n",
    "    frames_interp = F.interpolate(\n",
    "        frames_tensor, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    cond_video = F.interpolate(\n",
    "        cond_video, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    cond_masks = F.interpolate(cond_masks, size=opts.sample_size, mode='nearest')\n",
    "\n",
    "    # --- save inputs for visualization ---\n",
    "    save_video(\n",
    "        (frames_interp.permute(0, 2, 3, 1) + 1.0) / 2.0,\n",
    "        os.path.join(segment_dir, 'input.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    save_video(\n",
    "        cond_video.permute(0, 2, 3, 1),\n",
    "        os.path.join(segment_dir, 'render.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    save_video(\n",
    "        cond_masks.repeat(1, 3, 1, 1).permute(0, 2, 3, 1),\n",
    "        os.path.join(segment_dir, 'mask.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "\n",
    "    # --- prepare for diffusion pipeline ---\n",
    "    frames_interp = (frames_interp.permute(1, 0, 2, 3).unsqueeze(0) + 1.0) / 2.0\n",
    "    \n",
    "    # frames_ref = frames_interp[:, :, :10, :, :]  # first few frames as reference\n",
    "    frames_ref_interp = F.interpolate(\n",
    "        frames_ref, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    frames_ref = (frames_ref_interp.permute(1, 0, 2, 3).unsqueeze(0) + 1.0) / 2.0\n",
    "    \n",
    "    \n",
    "    cond_video = cond_video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "    cond_masks = (1.0 - cond_masks.permute(1, 0, 2, 3).unsqueeze(0)) * 255.0\n",
    "\n",
    "    generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "    # --- run diffusion model ---\n",
    "    with torch.no_grad():\n",
    "        sample = vis_crafter.pipeline(\n",
    "            vis_crafter.prompt,\n",
    "            num_frames=opts.video_length,\n",
    "            negative_prompt=opts.negative_prompt,\n",
    "            height=opts.sample_size[0],\n",
    "            width=opts.sample_size[1],\n",
    "            generator=generator,\n",
    "            guidance_scale=opts.diffusion_guidance_scale,\n",
    "            num_inference_steps=opts.diffusion_inference_steps,\n",
    "            video=cond_video.to(opts.device),\n",
    "            mask_video=cond_masks.to(opts.device),\n",
    "            reference=frames_ref.to(opts.device),\n",
    "        ).videos\n",
    "\n",
    "    # --- save result ---\n",
    "    save_video(\n",
    "        sample[0].permute(1, 2, 3, 0),\n",
    "        os.path.join(segment_dir, 'gen.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "\n",
    "    return sample, segment_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/rhino.mp4\",\n",
    "    \"--n_splits\", \"2\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "# name = \"right_90\"\n",
    "# pose = [0, 90, radius, 0, 0]\n",
    "\n",
    "# name = \"top_90\"\n",
    "# pose = [90, 0, radius, 0, 0]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "# name = '120_0_0_0_3', make it infer values from pose\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "\n",
    "print(f\"\\n=== Running Autoregressive {name} ===\")\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.utils as utils\n",
    "\n",
    "# read input video\n",
    "\n",
    "frames_np = utils.read_video_frames(\n",
    "    opts.video_path, opts.video_length, opts.stride, opts.max_res,\n",
    "    # height=opts.sample_size[0], width=opts.sample_size[1],\n",
    ")\n",
    "\n",
    "# pad if too short\n",
    "frames_np = pad_video(frames_np, opts.video_length)\n",
    "# frames_np = frames_np[::-1, ...].copy()\n",
    "\n",
    "\n",
    "frames_tensor = (\n",
    "    torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    ")  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "assert frames_tensor.shape[0] == opts.video_length\n",
    "\n",
    "\n",
    "\n",
    "# prompt\n",
    "vis_crafter.prompt = vis_crafter.get_caption(opts, frames_np[opts.video_length // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Input Depth\n",
    "############################################\n",
    "\n",
    "# TODO: this takes frames as 1024 x 576? size\n",
    "# the sample will be about 640 x 360 - is it ok?\n",
    "depths = vis_crafter.depth_estimater.infer(\n",
    "    frames_np,\n",
    "    opts.near,\n",
    "    opts.far,\n",
    "    opts.depth_inference_steps,\n",
    "    opts.depth_guidance_scale,\n",
    "    window_size=opts.window_size,\n",
    "    overlap=opts.overlap,\n",
    ").to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Cameras\n",
    "##########################################\n",
    "\n",
    "radius = (\n",
    "    depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "    * opts.radius_scale\n",
    ")\n",
    "radius = min(radius, 5)\n",
    "\n",
    "# radius = 10\n",
    "\n",
    "\n",
    "c2ws_anchor = torch.tensor([ \n",
    "            [-1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, -1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "    ]).unsqueeze(0).to(opts.device)\n",
    "\n",
    "c2ws_target = generate_traj_specified(\n",
    "    c2ws_anchor, \n",
    "    opts.target_pose, \n",
    "    opts.video_length * opts.n_splits, \n",
    "    opts.device\n",
    ")\n",
    "c2ws_target[:, 2, 3] += radius\n",
    "\n",
    "c2ws_init = c2ws_target[0].repeat(opts.video_length, 1, 1)\n",
    "\n",
    "\n",
    "traj_segments = c2ws_target.view(opts.n_splits, opts.video_length, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "point_clouds = []\n",
    "colors_list = []\n",
    "weights_list = []\n",
    "\n",
    "global_pc = []\n",
    "global_colors = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    with torch.no_grad():\n",
    "        points, colors, weights = funwarp.create_pointcloud_from_image(\n",
    "            frames_tensor[i:i+1],\n",
    "            None,\n",
    "            depths[i:i+1],\n",
    "            c2ws_init[i:i+1],\n",
    "            vis_crafter.K[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "    point_clouds.append(points)\n",
    "    colors_list.append(colors)  \n",
    "    weights_list.append(weights)\n",
    "    \n",
    "    global_pc.append(points)\n",
    "    global_colors.append(colors)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_images = []\n",
    "masks = []        \n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "\n",
    "    warped_image, mask = funwarp.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "        point_clouds[i],\n",
    "        colors_list[i],\n",
    "        c2ws_target[i:i+1],\n",
    "        vis_crafter.K[0:1].to(opts.device),\n",
    "        (576, 1024),\n",
    "        point_size=2,\n",
    "    )\n",
    "    \n",
    "    # single_mask = masks[10][0]  # Shape: (1, H, W)\n",
    "    # print(single_mask.shape)\n",
    "    cleaned_mask = clean_single_mask_simple(\n",
    "        mask[0],\n",
    "        kernel_size=9,\n",
    "        n_erosion_steps=1,\n",
    "        n_dilation_steps=1\n",
    "        )\n",
    "    # should stay in [-1, 1] range\n",
    "    \n",
    "    cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "    \n",
    "    warped_image = warped_image * cleaned_mask\n",
    "    \n",
    "    warped_images.append(warped_image)\n",
    "    masks.append(cleaned_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.imshow((warped_images[10][0].permute(1,2,0).cpu().numpy() + 1) / 2)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "# plt.imshow(masks[10][0].permute(1,2,0).cpu().numpy(), cmap='gray')\n",
    "plt.imshow((frames_tensor[10].permute(1,2,0).cpu().numpy() + 1) / 2, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, segment_dir = sample_diffusion(\n",
    "    vis_crafter,\n",
    "    frames_tensor,\n",
    "    warped_images,\n",
    "    frames_tensor[:10],\n",
    "    masks,\n",
    "    opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Autoregressive Generation\n",
    "########################################################\n",
    "\n",
    "frames_gen_np = utils.read_video_frames(\n",
    "    os.path.join(segment_dir, 'gen.mp4'), opts.video_length, opts.stride, opts.max_res,\n",
    ")\n",
    "\n",
    "# pad if too short\n",
    "frames_gen_np = pad_video(frames_gen_np, opts.video_length)\n",
    "\n",
    "# reverse the 0th axis\n",
    "frames_gen_np = frames_gen_np[::-1, ...].copy()\n",
    "\n",
    "# print(frames_gen_np.shape)\n",
    "\n",
    "frames_gen_tensor = (\n",
    "    torch.from_numpy(frames_gen_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    ")  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "assert frames_gen_tensor.shape[0] == opts.video_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_np.shape, frames_gen_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_gen_source_np = np.concatenate([frames_np[::-1], frames_gen_np[::-1]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    depths_gen = vis_crafter.depth_estimater.infer(\n",
    "        frames_gen_source_np,\n",
    "        opts.near,\n",
    "        opts.far,\n",
    "        opts.depth_inference_steps,\n",
    "        opts.depth_guidance_scale,\n",
    "        window_size=opts.window_size,\n",
    "        overlap=opts.overlap,\n",
    "    ).to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the depth to match the target cameras\n",
    "\n",
    "radius_after = (\n",
    "    depths_gen[opts.video_length - 1, 0, depths_gen.shape[-2] // 2, depths_gen.shape[-1] // 2].cpu()\n",
    "    * opts.radius_scale\n",
    ")\n",
    "depths_gen_aligned = depths_gen / (radius_after / radius)\n",
    "\n",
    "# assert that the radius of aligned depth is equal to radius before\n",
    "radius_after_2 = depths_gen_aligned[opts.video_length - 1, 0, depths_gen_aligned.shape[-2] // 2, depths_gen_aligned.shape[-1] // 2].cpu() * opts.radius_scale\n",
    "assert abs(radius_after_2 - radius) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save depth as video to /home/azhuravl/work/TrajectoryCrafter/experiments/11-10-2025/rhino_20251011_1207_90_0_0_0_1_auto_s2/stage_4\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depths_vis = 1/depths_gen_aligned  # Invert for better visualization\n",
    "# depths_vis = torch.clamp(depths_gen, 0, 2)\n",
    "# depths_vis = (depths_vis - depths_vis.min()) / (depths_vis.max() - depths_vis.min())\n",
    "depths_vis_np = depths_vis.permute(0, 2, 3, 1).cpu().numpy()\n",
    "depths_vis_np = np.repeat(depths_vis_np, 3, axis=3)  # (T, H, W, 3)\n",
    "save_video(\n",
    "    depths_vis_np,\n",
    "    os.path.join(opts.save_dir, 'depths_all.mp4'),\n",
    "    fps=opts.fps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_gen_source_tensor = torch.from_numpy(frames_gen_source_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "\n",
    "point_clouds_source = []\n",
    "colors_list_source = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    with torch.no_grad():\n",
    "        points, colors, _ = funwarp.create_pointcloud_from_image(\n",
    "            frames_gen_source_tensor[i:i+1],\n",
    "            None,\n",
    "            depths_gen_aligned[i:i+1],\n",
    "            c2ws_init[i:i+1],\n",
    "            # traj_segments[0][-i-1].unsqueeze(0),\n",
    "            vis_crafter.K[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "    point_clouds_source.append(points)\n",
    "    colors_list_source.append(colors)  \n",
    "    \n",
    "    \n",
    "point_clouds_gen = []\n",
    "colors_list_gen = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    with torch.no_grad():\n",
    "        points, colors, _ = funwarp.create_pointcloud_from_image(\n",
    "            frames_gen_source_tensor[opts.video_length + i:opts.video_length + i + 1],\n",
    "            None,\n",
    "            depths_gen_aligned[opts.video_length + i:opts.video_length + i + 1],\n",
    "            traj_segments[0][i].unsqueeze(0),\n",
    "            vis_crafter.K[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "    point_clouds_gen.append(points)\n",
    "    colors_list_gen.append(colors)  \n",
    "\n",
    "\n",
    "# merge the pcs\n",
    "global_pc = []\n",
    "global_colors = []\n",
    "\n",
    "for i in range(opts.video_length):\n",
    "    points_gen = point_clouds_gen[i]\n",
    "    colors_gen = colors_list_gen[i]\n",
    "    \n",
    "    points_source = point_clouds_source[-i-1]\n",
    "    colors_source = colors_list_source[-i-1]\n",
    "    \n",
    "    pc_merged = torch.cat([points_source, points_gen], dim=0)\n",
    "    color_merged = torch.cat([colors_source, colors_gen], dim=0)\n",
    "    \n",
    "    # downsample by 2x randomly\n",
    "    indices = torch.randperm(pc_merged.shape[0], device=pc_merged.device)\n",
    "    indices = indices[:len(indices)//2]\n",
    "    \n",
    "    pc_merged = pc_merged[indices]\n",
    "    color_merged = color_merged[indices]\n",
    "    \n",
    "    global_pc.append(pc_merged)\n",
    "    global_colors.append(color_merged)\n",
    "\n",
    "\n",
    "    # # cat to global pc, then downsample\n",
    "    # pc_merged = torch.cat([global_pc[-i-1], points], dim=0)\n",
    "    # color_merged = torch.cat([global_colors[-i-1], colors], dim=0)\n",
    "    \n",
    "    # # downsample by 2x randomly\n",
    "    # indices = torch.randperm(pc_merged.shape[0], device=pc_merged.device)[:2000000]\n",
    "    # pc_merged = pc_merged[indices]\n",
    "    # color_merged = color_merged[indices]\n",
    "    \n",
    "    # global_pc[-i-1] = pc_merged\n",
    "    # global_colors[-i-1] = color_merged\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_images_gen = []\n",
    "masks_gen = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    warped_image, mask = funwarp.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "        # point_clouds_gen[i],\n",
    "        # colors_list_gen[i],\n",
    "        global_pc[-i-1],\n",
    "        global_colors[-i-1],\n",
    "        traj_segments[1][i:i+1],\n",
    "        vis_crafter.K[0:1].to(opts.device),\n",
    "        (576, 1024),\n",
    "        point_size=2,\n",
    "    )\n",
    "    \n",
    "    cleaned_mask = clean_single_mask_simple(\n",
    "        mask[0],\n",
    "        kernel_size=9,\n",
    "        n_erosion_steps=1,\n",
    "        n_dilation_steps=1\n",
    "        )\n",
    "    \n",
    "    cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "    \n",
    "    warped_image = warped_image * cleaned_mask\n",
    "    \n",
    "    warped_images_gen.append(warped_image)\n",
    "    masks_gen.append(cleaned_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "j = 40\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.imshow((warped_images_gen[j][0].permute(1,2,0).cpu().numpy() + 1) / 2)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "# plt.imshow(masks[10][0].permute(1,2,0).cpu().numpy(), cmap='gray')\n",
    "plt.imshow((frames_gen_tensor[j].permute(1,2,0).cpu().numpy() + 1) / 2, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_autoreg, segment_dir_autoreg = sample_diffusion(\n",
    "    vis_crafter,\n",
    "    frames_tensor=frames_gen_tensor,\n",
    "    warped_images=warped_images_gen,\n",
    "    frames_ref=frames_tensor[:10],\n",
    "    masks=masks_gen,\n",
    "    opts=opts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_autoreg_np = utils.read_video_frames(\n",
    "    os.path.join(segment_dir_autoreg, 'gen.mp4'), opts.video_length, opts.stride, opts.max_res,\n",
    ")\n",
    "# pad if too short\n",
    "frames_autoreg_np = pad_video(frames_autoreg_np, opts.video_length)\n",
    "\n",
    "frames_autoreg_np = frames_autoreg_np[::-1, ...].copy()\n",
    "frames_autoreg_tensor = (\n",
    "    torch.from_numpy(frames_autoreg_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    ")  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    depths_autoreg = vis_crafter.depth_estimater.infer(\n",
    "        frames_autoreg_np,\n",
    "        opts.near,\n",
    "        opts.far,\n",
    "        opts.depth_inference_steps,\n",
    "        opts.depth_guidance_scale,\n",
    "        window_size=opts.window_size,\n",
    "        overlap=opts.overlap,\n",
    "    ).to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate scale factor again\n",
    "scale_factor_2 = align_depth_maps(\n",
    "    depths_gen_aligned[0], depths_autoreg[-1]\n",
    ")\n",
    "print(f\"Depth scale factor 2: {scale_factor_2:.3f}\")\n",
    "depths_autoreg_aligned = depths_autoreg * scale_factor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds_autoreg = []\n",
    "colors_list_autoreg = []\n",
    "weights_list_autoreg = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    with torch.no_grad():\n",
    "        points, colors, weights = funwarp.create_pointcloud_from_image(\n",
    "            frames_autoreg_tensor[i:i+1],\n",
    "            None,\n",
    "            depths_autoreg_aligned[i:i+1],\n",
    "            traj_segments[1][-i-1].unsqueeze(0),\n",
    "            vis_crafter.K[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "    point_clouds_autoreg.append(points)\n",
    "    colors_list_autoreg.append(colors)  \n",
    "    weights_list_autoreg.append(weights)\n",
    "    \n",
    "    # global_pc[i] = torch.cat([global_pc[i], points], dim=0)\n",
    "    # global_colors[i] = torch.cat([global_colors[i], colors], dim=0)\n",
    "    \n",
    "    pc_merged = torch.cat([global_pc[i], points], dim=0)\n",
    "    color_merged = torch.cat([global_colors[i], colors], dim=0)\n",
    "    \n",
    "    # downsample by 2x randomly\n",
    "    indices = torch.randperm(pc_merged.shape[0], device=pc_merged.device)[:2000000]\n",
    "    pc_merged = pc_merged[indices]\n",
    "    color_merged = color_merged[indices]\n",
    "    \n",
    "    global_pc[i] = pc_merged\n",
    "    global_colors[i] = color_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viser utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────── <span style=\"font-weight: bold\">viser</span> ────────────────╮\n",
       "│             ╷                         │\n",
       "│   HTTP      │ http://localhost:8080   │\n",
       "│   Websocket │ ws://localhost:8080     │\n",
       "│             ╵                         │\n",
       "╰───────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────── \u001b[1mviser\u001b[0m ────────────────╮\n",
       "│             ╷                         │\n",
       "│   HTTP      │ http://localhost:8080   │\n",
       "│   Websocket │ ws://localhost:8080     │\n",
       "│             ╵                         │\n",
       "╰───────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import viser\n",
    "import numpy as np\n",
    "\n",
    "# Start viser server\n",
    "server = viser.ViserServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://gpu24-h100-01:8080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# print slurm node name\n",
    "\n",
    "node_name=os.environ.get('SLURM_NODELIST', 'localhost')\n",
    "\n",
    "print(f'http://{node_name}:{server.get_port()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_points(\n",
    "    server,\n",
    "    points: np.ndarray,  # (N, 3)\n",
    "    colors: np.ndarray,  # (N, 3)\n",
    "    name: str,\n",
    "):\n",
    "    # ensure colors are in [0, 1]\n",
    "    if colors.min() < -0.1:\n",
    "        colors = (colors + 1.0) / 2.0\n",
    "    \n",
    "    server.scene.add_point_cloud(\n",
    "        name=name,\n",
    "        points=points,\n",
    "        colors=colors,\n",
    "        point_size=0.01\n",
    "    )\n",
    "\n",
    "def add_camera(\n",
    "    server,\n",
    "    pose: np.ndarray,  # (4, 4)\n",
    "    name: str,\n",
    "    color: tuple = (0.2, 0.8, 0.2),\n",
    "):\n",
    "    pose = np.linalg.inv(pose)\n",
    "    \n",
    "    position = pose[:3, 3]\n",
    "    rotation_matrix = pose[:3, :3]\n",
    "    \n",
    "    # Convert rotation to quaternion\n",
    "    wxyz = viser.transforms.SO3.from_matrix(rotation_matrix).wxyz\n",
    "    \n",
    "    server.scene.add_camera_frustum(\n",
    "        name,\n",
    "        fov=60, aspect=4/3, scale=0.1,\n",
    "        position=position, wxyz=wxyz,\n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "# Add this after creating your server and adding point clouds\n",
    "@server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    yaw_slider = client.gui.add_slider(\"Camera Yaw\", min=-180, max=180, step=1, initial_value=0)\n",
    "    \n",
    "    @yaw_slider.on_update\n",
    "    def _(_):\n",
    "        angle_rad = np.deg2rad(yaw_slider.value)\n",
    "        radius = 8.0\n",
    "        \n",
    "        position = np.array([\n",
    "            radius * np.sin(angle_rad),\n",
    "            0,  # height\n",
    "            radius * np.cos(angle_rad)\n",
    "        ])\n",
    "        \n",
    "        client.camera.position = position\n",
    "        client.camera.look_at = np.array([0, 0, 0])\n",
    "        client.camera.up_direction = np.array([0, -1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.scene.reset()\n",
    "\n",
    "# add original point cloud #0, and all cameras from first segment\n",
    "\n",
    "j = 0\n",
    "\n",
    "add_points(\n",
    "    server,\n",
    "    point_clouds_source[j].cpu().numpy(),\n",
    "    colors_list_source[j].cpu().numpy(),\n",
    "    name='input_pc'\n",
    ")\n",
    "for idx in range(opts.video_length):\n",
    "    add_camera(\n",
    "        server,\n",
    "        traj_segments[0][idx].cpu().numpy(),\n",
    "        name=f'input_cam_{idx:02d}',\n",
    "        color=(0.2, 0.8, 0.2)\n",
    "    )\n",
    "    \n",
    "# add inpainted point cloud #1, and all cameras from second segment\n",
    "add_points(\n",
    "    server,\n",
    "    point_clouds_gen[-j-1].cpu().numpy(),\n",
    "    colors_list_gen[-j-1].cpu().numpy(),\n",
    "    name='gen_pc'\n",
    ")\n",
    "for idx in range(opts.video_length):\n",
    "    add_camera(\n",
    "        server,\n",
    "        traj_segments[1][idx].cpu().numpy(),\n",
    "        name=f'gen_cam_{idx:02d}',\n",
    "        color=(0.8, 0.2, 0.2)\n",
    "    )\n",
    "    \n",
    "# add_points(\n",
    "#     server,\n",
    "#     point_clouds_autoreg[j].cpu().numpy(),\n",
    "#     colors_list_autoreg[j].cpu().numpy(),\n",
    "#     name='autoreg_pc'\n",
    "# )\n",
    "\n",
    "# add_points(\n",
    "#     server,\n",
    "#     global_pc[j].cpu().numpy(),\n",
    "#     global_colors[j].cpu().numpy(),\n",
    "#     name='global_pc'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove vis_cam_XX if exists\n",
    "for idx in range(opts.video_length):\n",
    "    cam_name = f'vis_cam_{idx:02d}'\n",
    "    if cam_name in server.scene.__dict__['_handle_from_node_name']:\n",
    "        server.scene.remove_by_name(cam_name)\n",
    "\n",
    "\n",
    "# add some cameras for visualization\n",
    "c2ws_vis = generate_traj_specified(\n",
    "    c2ws_anchor, \n",
    "    [120, 0, 0, 0, 3], \n",
    "    opts.video_length, \n",
    "    opts.device\n",
    ")\n",
    "c2ws_vis[:, 2, 3] += radius\n",
    "\n",
    "# c2ws_vis[:, 2, 1] /= 2\n",
    "\n",
    "# add cameras to viser\n",
    "for idx in range(0, opts.video_length):\n",
    "    add_camera(\n",
    "        server,\n",
    "        c2ws_vis[idx].cpu().numpy(),\n",
    "        name=f'vis_cam_{idx:02d}',\n",
    "        color=(0.2, 0.2, 0.8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Depth estimation on the input+generated sequence for consistency\n",
    "# Can we use their warping code on multiple images?\n",
    "# Implement this as a loop, with arbitraty number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n of subdirs in /home/azhuravl/work/TrajectoryCrafter/experiments/10-10-2025/rhino_20251010_1009_right_90_auto_s2\n",
    "import os\n",
    "\n",
    "\n",
    "n_subdirs = len([name for name in os.listdir(opts.save_dir) if os.path.isdir(os.path.join(opts.save_dir, name))])\n",
    "\n",
    "segment_dir = os.path.join(opts.save_dir, f'stage_{n_subdirs+1}')\n",
    "print('saving to:', segment_dir)\n",
    "\n",
    "os.makedirs(segment_dir, exist_ok=True)\n",
    "\n",
    "cond_video = (torch.cat(warped_images) + 1.0) / 2.0\n",
    "cond_masks = torch.cat(masks)\n",
    "\n",
    "frames_interp = F.interpolate(\n",
    "    frames_tensor, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    ")\n",
    "cond_video = F.interpolate(\n",
    "    cond_video, size=opts.sample_size, mode='bilinear', align_corners=False\n",
    ")\n",
    "# cond_video = frames_interp\n",
    "cond_masks = F.interpolate(cond_masks, size=opts.sample_size, mode='nearest')\n",
    "\n",
    "save_video(\n",
    "    (frames_interp.permute(0, 2, 3, 1) + 1.0) / 2.0,\n",
    "    os.path.join(segment_dir, 'input.mp4'),\n",
    "    fps=opts.fps,\n",
    ")\n",
    "save_video(\n",
    "    cond_video.permute(0, 2, 3, 1),\n",
    "    os.path.join(segment_dir, 'render.mp4'),\n",
    "    fps=opts.fps,\n",
    ")\n",
    "save_video(\n",
    "    cond_masks.repeat(1, 3, 1, 1).permute(0, 2, 3, 1),\n",
    "    os.path.join(segment_dir, 'mask.mp4'),\n",
    "    fps=opts.fps,\n",
    ")\n",
    "\n",
    "# return\n",
    "\n",
    "frames_interp = (frames_interp.permute(1, 0, 2, 3).unsqueeze(0) + 1.0) / 2.0\n",
    "frames_ref = frames_interp[:, :, :10, :, :]\n",
    "cond_video = cond_video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "\n",
    "# cond_video = frames_interp\n",
    "\n",
    "\n",
    "cond_masks = (1.0 - cond_masks.permute(1, 0, 2, 3).unsqueeze(0)) * 255.0\n",
    "generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "with torch.no_grad():            \n",
    "    sample = vis_crafter.pipeline(\n",
    "        vis_crafter.prompt,\n",
    "        num_frames=opts.video_length,\n",
    "        negative_prompt=opts.negative_prompt,\n",
    "        height=opts.sample_size[0],\n",
    "        width=opts.sample_size[1],\n",
    "        generator=generator,\n",
    "        guidance_scale=opts.diffusion_guidance_scale,\n",
    "        num_inference_steps=opts.diffusion_inference_steps,\n",
    "        video=cond_video.to(opts.device),\n",
    "        mask_video=cond_masks.to(opts.device),\n",
    "        reference=frames_ref.to(opts.device),\n",
    "    ).videos\n",
    "\n",
    "# sample = frames\n",
    "    \n",
    "save_video(\n",
    "    sample[0].permute(1, 2, 3, 0),\n",
    "    os.path.join(segment_dir, 'gen.mp4'),\n",
    "    fps=opts.fps,\n",
    ")\n",
    "return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(opts.n_splits):\n",
    "    \n",
    "    segment_dir = os.path.join(opts.save_dir, f'stage_{i+1}')\n",
    "    os.makedirs(segment_dir, exist_ok=True)\n",
    "    \n",
    "    inpainted_video = generate_segment(\n",
    "        frames, pc_input, color_input, traj_segments[i], segment_dir, opts\n",
    "        )\n",
    "    \n",
    "    pc_inpainted = pc_input\n",
    "    color_inpainted = color_input\n",
    "    # pc_inpainted, color_inpainted = extract_point_cloud(inpainted_video, traj_segments[i], opts)\n",
    "    \n",
    "    # pc_merged, color_merged = merge_point_clouds(pc_input, color_input, pc_inpainted, color_inpainted)\n",
    "    pc_merged, color_merged = pc_inpainted, color_inpainted\n",
    "                \n",
    "    save_segment_results(\n",
    "        pc_input,\n",
    "        color_input,\n",
    "        pc_inpainted,\n",
    "        color_inpainted,            \n",
    "        pc_merged,\n",
    "        color_merged,\n",
    "        traj_segments[i],\n",
    "        opts, segment_idx=i)\n",
    "    \n",
    "    frames = inpainted_video\n",
    "    pc_input = pc_merged\n",
    "    color_input = color_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in HyperNeRF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "    \n",
    "\n",
    "def save_hypernerf_dataset(\n",
    "    frames_source_target_np,\n",
    "    poses_source_target,\n",
    "    intrinsics,\n",
    "    depths_source_target,\n",
    "    global_pc,\n",
    "    global_colors,\n",
    "    warped_images,\n",
    "    masks,\n",
    "    opts,\n",
    "    output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Save complete dataset in HyperNeRF format\n",
    "    \n",
    "    Args:\n",
    "        frames_source_target_np: (T, H, W, 3) numpy array of all frames\n",
    "        poses_source_target: (T, 4, 4) tensor of camera poses\n",
    "        intrinsics: (3, 3) tensor of camera intrinsics\n",
    "        depths_source_target: (T, 1, H, W) tensor of depth maps\n",
    "        global_pc: list of point clouds per timestep\n",
    "        global_colors: list of colors per timestep\n",
    "        warped_images: list of warped image tensors\n",
    "        masks: list of mask tensors\n",
    "        opts: options object\n",
    "        output_dir: output directory path\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'camera'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'rgb', '2x'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'depth'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'point_clouds'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'warped', '2x'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'masks', '2x'), exist_ok=True)\n",
    "    \n",
    "    n_frames = len(frames_source_target_np)\n",
    "    h, w = frames_source_target_np.shape[1:3]\n",
    "    \n",
    "    # Convert poses and intrinsics to numpy\n",
    "    poses_np = poses_source_target.cpu().numpy()\n",
    "    intrinsics_np = intrinsics.cpu().numpy()\n",
    "    \n",
    "    # Calculate scene bounds and center\n",
    "    all_positions = poses_np[:, :3, 3]\n",
    "    scene_center = np.mean(all_positions, axis=0).tolist()\n",
    "    scene_scale = np.max(np.linalg.norm(all_positions - scene_center, axis=1))\n",
    "    \n",
    "    # Generate unique IDs for each frame\n",
    "    ids = []\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    metadata = {}\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        frame_id = f\"frame_{i:06d}\"\n",
    "        ids.append(frame_id)\n",
    "        \n",
    "        # Split train/val (alternating for simplicity)\n",
    "        # if i % 2 == 0:\n",
    "        #     train_ids.append(frame_id)\n",
    "        # else:\n",
    "        #     val_ids.append(frame_id)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            val_ids.append(frame_id)\n",
    "        else:\n",
    "            train_ids.append(frame_id)  # All frames for training\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if (i // opts.video_length) % 2 == 1:\n",
    "            # forward\n",
    "            true_id = i % opts.video_length\n",
    "        else:\n",
    "            # backward\n",
    "            true_id = opts.video_length - i % opts.video_length - 1\n",
    "        \n",
    "        # Create metadata entry\n",
    "        metadata[frame_id] = {\n",
    "            \"time_id\": true_id,\n",
    "            \"warp_id\": true_id,\n",
    "            \"appearance_id\": true_id,\n",
    "            \"camera_id\": i  # Single camera setup\n",
    "        }\n",
    "        \n",
    "        # Save camera parameters\n",
    "        pose = poses_np[i]\n",
    "        \n",
    "        pose = np.linalg.inv(pose)  # World to camera\n",
    "        # Convert from TrajectoryCrafter (OpenGL) to HyperNeRF (OpenCV) convention\n",
    "        pose_cv = pose.copy()\n",
    "        # pose_cv[:3, 1:3] *= -1  # Flip Y and Z axes\n",
    "        \n",
    "        # Extract focal length and principal point\n",
    "        fx, fy = intrinsics_np[0, 0], intrinsics_np[1, 1]\n",
    "        cx, cy = intrinsics_np[0, 2], intrinsics_np[1, 2]\n",
    "        \n",
    "        camera_data = {\n",
    "            \"orientation\": pose_cv[:3, :3].tolist(),\n",
    "            \"position\": pose_cv[:3, 3].tolist(),\n",
    "            \"focal_length\": float(fx),  # Assume square pixels for simplicity\n",
    "            \"principal_point\": [float(cx), float(cy)],\n",
    "            \"skew\": 0.0,\n",
    "            \"pixel_aspect_ratio\": float(fy / fx),\n",
    "            \"radial_distortion\": [0.0, 0.0, 0.0],\n",
    "            \"tangential_distortion\": [0.0, 0.0],\n",
    "            \"image_size\": [w, h]\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'camera', f'{frame_id}.json'), 'w') as f:\n",
    "            json.dump(camera_data, f, indent=2)\n",
    "        \n",
    "        # Save RGB image\n",
    "        frame = frames_source_target_np[i]\n",
    "        if frame.max() <= 1.0:\n",
    "            frame_uint8 = (frame * 255).astype(np.uint8)\n",
    "        else:\n",
    "            frame_uint8 = frame.astype(np.uint8)\n",
    "        \n",
    "        img = Image.fromarray(frame_uint8)\n",
    "        img.save(os.path.join(output_dir, 'rgb', '2x', f'{frame_id}.png'))\n",
    "        \n",
    "        # Save depth map\n",
    "        if i < len(depths_source_target) and depths_source_target is not None:\n",
    "            depth_np = depths_source_target[i, 0].cpu().numpy()\n",
    "            np.save(os.path.join(output_dir, 'depth', f'{frame_id}.npy'), depth_np)\n",
    "        \n",
    "        # Save point clouds if available\n",
    "        if i < len(global_pc) and global_pc[i] is not None:\n",
    "            \n",
    "            # downsample pc by 10x for saving\n",
    "            indices = torch.randperm(global_pc[i].shape[0], device=global_pc[i].device)[:len(global_pc[i])//10]\n",
    "            \n",
    "            pc_np = global_pc[i][indices].cpu().numpy()\n",
    "            colors_np = global_colors[i][indices].cpu().numpy()\n",
    "            \n",
    "            # Ensure colors are in [0,1] range\n",
    "            if colors_np.min() < -0.1:\n",
    "                colors_np = (colors_np + 1.0) / 2.0\n",
    "                \n",
    "            # normals = direction to camera\n",
    "            cam_pos = pose[:3, 3]\n",
    "            directions = cam_pos - pc_np  # (N, 3)\n",
    "            norms = np.linalg.norm(directions, axis=1, keepdims=True) + 1e-8\n",
    "            normals = directions / norms\n",
    "            \n",
    "            # Save as PLY format\n",
    "            ply_path = os.path.join(output_dir, 'point_clouds', f'{frame_id}.ply')\n",
    "            # with open(ply_path, 'w') as f:\n",
    "            #     f.write(\"ply\\n\")\n",
    "            #     f.write(\"format ascii 1.0\\n\")\n",
    "            #     f.write(f\"element vertex {len(pc_np)}\\n\")\n",
    "            #     f.write(\"property float x\\n\")\n",
    "            #     f.write(\"property float y\\n\")\n",
    "            #     f.write(\"property float z\\n\")\n",
    "            #     f.write(\"property float nx\\n\")\n",
    "            #     f.write(\"property float ny\\n\")\n",
    "            #     f.write(\"property float nz\\n\")\n",
    "            #     f.write(\"property uchar red\\n\")\n",
    "            #     f.write(\"property uchar green\\n\")\n",
    "            #     f.write(\"property uchar blue\\n\")\n",
    "            #     f.write(\"end_header\\n\")\n",
    "                \n",
    "            #     colors_uint8 = (colors_np * 255).astype(np.uint8)\n",
    "            #     for point, color in zip(pc_np, colors_uint8):\n",
    "            #         f.write(f\"{point[0]:.6f} {point[1]:.6f} {point[2]:.6f} {color[0]} {color[1]} {color[2]}\\n\")\n",
    "        \n",
    "            # Prepare vertex data\n",
    "            vertex_data = []\n",
    "            colors_uint8 = (colors_np * 255).astype(np.uint8)\n",
    "            \n",
    "            for j in range(len(pc_np)):\n",
    "                vertex_data.append((\n",
    "                    pc_np[j, 0], pc_np[j, 1], pc_np[j, 2],  # x, y, z\n",
    "                    normals[j, 0], normals[j, 1], normals[j, 2],  # nx, ny, nz\n",
    "                    colors_uint8[j, 0], colors_uint8[j, 1], colors_uint8[j, 2]  # r, g, b\n",
    "                ))\n",
    "            \n",
    "            # Create PLY element\n",
    "            vertex_array = np.array(vertex_data, dtype=[\n",
    "                ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "                ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),\n",
    "                ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')\n",
    "            ])\n",
    "            \n",
    "            vertex_element = PlyElement.describe(vertex_array, 'vertex')\n",
    "            PlyData([vertex_element]).write(ply_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Also save as numpy for convenience\n",
    "            # np.save(os.path.join(output_dir, 'point_clouds', f'{frame_id}_points.npy'), pc_np)\n",
    "            # np.save(os.path.join(output_dir, 'point_clouds', f'{frame_id}_colors.npy'), colors_np)\n",
    "        \n",
    "        # Save warped images and masks if available\n",
    "        if i < len(warped_images):\n",
    "            warped_np = ((warped_images[i][0].permute(1, 2, 0).cpu().numpy() + 1) / 2 * 255).astype(np.uint8)\n",
    "            Image.fromarray(warped_np).save(os.path.join(output_dir, 'warped', '2x', f'{frame_id}.png'))\n",
    "            \n",
    "            mask_np = (masks[i][0, 0].cpu().numpy() * 255).astype(np.uint8)\n",
    "            Image.fromarray(mask_np).save(os.path.join(output_dir, 'masks', '2x', f'{frame_id}.png'))\n",
    "    \n",
    "    # Save dataset.json\n",
    "    dataset_data = {\n",
    "        \"count\": n_frames,\n",
    "        \"num_exemplars\": n_frames,\n",
    "        \"ids\": ids,\n",
    "        \"train_ids\": train_ids,\n",
    "        \"val_ids\": val_ids\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'dataset.json'), 'w') as f:\n",
    "        json.dump(dataset_data, f, indent=2)\n",
    "    \n",
    "    # Save metadata.json\n",
    "    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save scene.json\n",
    "    # Calculate near/far planes based on depth statistics if available\n",
    "    if depths_source_target is not None:\n",
    "        all_depths = depths_source_target.cpu().numpy()\n",
    "        valid_depths = all_depths[all_depths > 0]\n",
    "        if len(valid_depths) > 0:\n",
    "            near_val = float(np.percentile(valid_depths, 1))\n",
    "            far_val = float(np.percentile(valid_depths, 99))\n",
    "        else:\n",
    "            near_val, far_val = 0.1, 100.0\n",
    "    else:\n",
    "        near_val, far_val = 0.1, 100.0\n",
    "    \n",
    "    scene_data = {\n",
    "        \"scale\": float(1.0 / scene_scale),\n",
    "        \"scene_to_metric\": float(1.0 / scene_scale),\n",
    "        \"center\": scene_center,\n",
    "        \"near\": near_val,\n",
    "        \"far\": far_val\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'scene.json'), 'w') as f:\n",
    "        json.dump(scene_data, f, indent=2)\n",
    "    \n",
    "    # Save additional TrajectoryCrafter-specific metadata\n",
    "    trajcrafter_metadata = {\n",
    "        \"dataset_name\": opts.exp_name,\n",
    "        \"video_length\": opts.video_length,\n",
    "        \"n_splits\": opts.n_splits,\n",
    "        \"target_pose\": opts.target_pose,\n",
    "        \"radius_scale\": opts.radius_scale,\n",
    "        \"near\": opts.near,\n",
    "        \"far\": opts.far,\n",
    "        \"fps\": opts.fps,\n",
    "        \"sample_size\": list(opts.sample_size),\n",
    "        \"creation_timestamp\": datetime.now().isoformat(),\n",
    "        \"format_version\": \"hypernerf_trajcrafter_v1.0\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'trajcrafter_metadata.json'), 'w') as f:\n",
    "        json.dump(trajcrafter_metadata, f, indent=2)\n",
    "    \n",
    "    # copy point cloud 0 to output_dir, name it \"points3D_downsample2.ply\"\n",
    "    if len(global_pc) > 0:\n",
    "        import shutil\n",
    "        src_ply = os.path.join(output_dir, 'point_clouds', 'frame_000022.ply')\n",
    "        dst_ply = os.path.join(output_dir, 'points3D_downsample2.ply')\n",
    "        if os.path.exists(src_ply):\n",
    "            shutil.copy(src_ply, dst_ply)\n",
    "    \n",
    "    \n",
    "    print(f\"HyperNeRF dataset saved to: {output_dir}\")\n",
    "    print(f\"- {n_frames} frames\")\n",
    "    print(f\"- {len(train_ids)} training frames\")\n",
    "    print(f\"- {len(val_ids)} validation frames\")\n",
    "    print(f\"- Scene center: {scene_center}\")\n",
    "    print(f\"- Scene scale: {scene_scale}\")\n",
    "    print(f\"- Depth range: {near_val:.3f} - {far_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive in 1 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_loop(\n",
    "    frames_source_np,\n",
    "    frames_target_np,\n",
    "    poses_source,\n",
    "    poses_target,\n",
    "    radius,\n",
    "):\n",
    "    \n",
    "    # --- determine output directory ---\n",
    "    n_subdirs = len([\n",
    "        name for name in os.listdir(opts.save_dir)\n",
    "        if os.path.isdir(os.path.join(opts.save_dir, name))\n",
    "    ])\n",
    "    segment_dir = os.path.join(opts.save_dir, f'stage_{n_subdirs + 1}')\n",
    "    os.makedirs(segment_dir, exist_ok=True)\n",
    "    print(f\"Saving to: {segment_dir}\")\n",
    "    \n",
    "           \n",
    "    # prompt\n",
    "    if vis_crafter.prompt is None:\n",
    "        vis_crafter.prompt = vis_crafter.get_caption(opts, frames_target_np[opts.video_length // 2])\n",
    "    \n",
    "    # concatenate with source\n",
    "    if frames_source_np is None:\n",
    "        frames_source_target_np = frames_target_np\n",
    "    else:\n",
    "        frames_source_target_np = np.concatenate([frames_source_np, frames_target_np], axis=0)\n",
    "       \n",
    "    # convert to tensors \n",
    "    frames_source_target_tensor = (\n",
    "        torch.from_numpy(frames_source_target_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "    )\n",
    "    frames_target_tensor = (\n",
    "        torch.from_numpy(frames_target_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "    )\n",
    "    \n",
    "    poses_source_target = torch.cat([poses_source, poses_target], dim=0)\n",
    "    \n",
    "    # save camera poses\n",
    "    torch.save(poses_source_target.cpu(), os.path.join(segment_dir, 'c2ws_source_target.pt'))\n",
    "    torch.save(poses_target.cpu(), os.path.join(segment_dir, 'c2ws_target.pt'))\n",
    "    torch.save(poses_source.cpu(), os.path.join(segment_dir, 'c2ws_source.pt'))\n",
    "    \n",
    "    # also save frames_source_np, frames_target_np frames_source_target_np, name as variables\n",
    "    \n",
    "    save_video(\n",
    "        frames_source_target_np,\n",
    "        os.path.join(segment_dir, 'frames_source_target.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    save_video(\n",
    "        frames_target_np,\n",
    "        os.path.join(segment_dir, 'frames_target.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    if frames_source_np is not None:\n",
    "        save_video(\n",
    "            frames_source_np,\n",
    "            os.path.join(segment_dir, 'frames_source.mp4'),\n",
    "            fps=opts.fps,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # depth estimation\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        depths_source_target = vis_crafter.depth_estimater.infer(\n",
    "            frames_source_target_np,\n",
    "            opts.near,\n",
    "            opts.far,\n",
    "            opts.depth_inference_steps,\n",
    "            opts.depth_guidance_scale,\n",
    "            window_size=opts.window_size,\n",
    "            overlap=opts.overlap,\n",
    "        ).to(opts.device)\n",
    "        \n",
    "    # align the depth to match the target cameras\n",
    "    radius_after = (\n",
    "        depths_source_target[opts.video_length - 1, 0, depths_source_target.shape[-2] // 2, depths_source_target.shape[-1] // 2].cpu()\n",
    "        * opts.radius_scale\n",
    "    )\n",
    "    depths_source_target = depths_source_target / (radius_after / radius)\n",
    "\n",
    "    # assert that the radius of aligned depth is equal to radius before\n",
    "    radius_after_2 = depths_source_target[opts.video_length - 1, 0, depths_source_target.shape[-2] // 2, depths_source_target.shape[-1] // 2].cpu() * opts.radius_scale\n",
    "    assert abs(radius_after_2 - radius) < 0.1\n",
    "    \n",
    "    \n",
    "    # visualize inverse depth\n",
    "    depths_vis = 1/depths_source_target  # Invert for better visualization\n",
    "    depths_vis_np = depths_vis.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    depths_vis_np = np.repeat(depths_vis_np, 3, axis=3)  # (T, H, W, 3)\n",
    "    save_video(\n",
    "        depths_vis_np,\n",
    "        os.path.join(segment_dir, 'depths_all.mp4'),\n",
    "        fps=opts.fps,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # extract point clouds\n",
    "    point_clouds_source_target = []\n",
    "    colors_list_source_target = []\n",
    "    for i in tqdm(range(frames_source_target_np.shape[0])):\n",
    "        with torch.no_grad():\n",
    "            points, colors, _ = funwarp.create_pointcloud_from_image(\n",
    "                frames_source_target_tensor[i:i+1],\n",
    "                None,\n",
    "                depths_source_target[i:i+1],\n",
    "                poses_source_target[i:i+1],\n",
    "                vis_crafter.K[0:1],\n",
    "                1,\n",
    "            )\n",
    "        point_clouds_source_target.append(points)\n",
    "        colors_list_source_target.append(colors)\n",
    "    \n",
    "    # concatenate and downsample the point clouds,\n",
    "    # we should have opts.video_length point clouds at the end\n",
    "    global_pc = []\n",
    "    global_colors = []\n",
    "    for i in range(opts.video_length):\n",
    "        # select 0 * opts.video_length + i, 1 * opts.video_length + i, ... \n",
    "        # from the full point cloud list\n",
    "        \n",
    "        # print(f'i = {i}')\n",
    "        \n",
    "        os.makedirs(os.path.join(segment_dir, f'local_pcs/{i:02d}'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(segment_dir, f'local_colors/{i:02d}'), exist_ok=True)\n",
    "        \n",
    "        pcs_to_merge = []\n",
    "        colors_to_merge = []\n",
    "        for j in range(0, len(point_clouds_source_target) // opts.video_length):\n",
    "            \n",
    "            # print(f'  j = {j}')\n",
    "            \n",
    "            # even = reverse, odd = normal\n",
    "            if j % 2 == 1:\n",
    "                pc_j = point_clouds_source_target[j * opts.video_length + i]\n",
    "                color_j = colors_list_source_target[j * opts.video_length + i]\n",
    "            else:\n",
    "                pc_j = point_clouds_source_target[(j + 1) * opts.video_length - i - 1]\n",
    "                color_j = colors_list_source_target[(j + 1) * opts.video_length - i - 1]\n",
    "                \n",
    "            pcs_to_merge.append(pc_j)\n",
    "            colors_to_merge.append(color_j)\n",
    "            \n",
    "            # save pc_j and color_j\n",
    "            torch.save(pc_j.cpu(), os.path.join(segment_dir, f'local_pcs/{i:02d}', f'pc_{j:02d}.pt'))\n",
    "            torch.save(color_j.cpu(), os.path.join(segment_dir, f'local_colors/{i:02d}', f'color_{j:02d}.pt'))\n",
    "\n",
    "        pc_merged = torch.cat(pcs_to_merge, dim=0)\n",
    "        color_merged = torch.cat(colors_to_merge, dim=0)\n",
    "        \n",
    "        # downsample by factor of len(pcs_to_merge) randomly\n",
    "        indices = torch.randperm(pc_merged.shape[0], device=pc_merged.device)[:len(pc_merged)//len(pcs_to_merge)]\n",
    "        pc_merged = pc_merged[indices]\n",
    "        color_merged = color_merged[indices]\n",
    "        \n",
    "        global_pc.append(pc_merged)\n",
    "        global_colors.append(color_merged)\n",
    "        \n",
    "        \n",
    "    if (len(point_clouds_source_target) // opts.video_length) % 2 == 0:\n",
    "        # reverse global pcs and colors\n",
    "        print('reversing the global PC')\n",
    "        global_pc = global_pc[::-1]\n",
    "        global_colors = global_colors[::-1]\n",
    "    \n",
    "    # save global pcs and colors to stage directory\n",
    "    os.makedirs(os.path.join(segment_dir, 'global_pc'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(segment_dir, 'global_colors'), exist_ok=True)\n",
    "    for i in range(opts.video_length):\n",
    "        torch.save(global_pc[i].cpu(), os.path.join(segment_dir, 'global_pc', f'pc_{i:02d}.pt'))\n",
    "        torch.save(global_colors[i].cpu(), os.path.join(segment_dir, 'global_colors', f'color_{i:02d}.pt'))    \n",
    "    \n",
    "    # render warped images and masks\n",
    "    warped_images = []\n",
    "    masks = []\n",
    "    \n",
    "    for i in tqdm(range(opts.video_length)):\n",
    "        warped_image, mask = funwarp.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "            global_pc[i],\n",
    "            global_colors[i],\n",
    "            poses_target[i:i+1],\n",
    "            vis_crafter.K[0:1].to(opts.device),\n",
    "            (576, 1024),\n",
    "            point_size=2,\n",
    "        )\n",
    "        \n",
    "        cleaned_mask = clean_single_mask_simple(\n",
    "            mask[0],\n",
    "            kernel_size=9,\n",
    "            n_erosion_steps=1,\n",
    "            n_dilation_steps=1\n",
    "            )\n",
    "        \n",
    "        cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "        \n",
    "        warped_image = warped_image * cleaned_mask\n",
    "        \n",
    "        warped_images.append(warped_image)\n",
    "        masks.append(cleaned_mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # plot the result\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.subplot(1,2,1)\n",
    "\n",
    "    plt.imshow((warped_images[10][0].permute(1,2,0).cpu().numpy() + 1) / 2)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    # plt.imshow(masks[10][0].permute(1,2,0).cpu().numpy(), cmap='gray')\n",
    "    plt.imshow((frames_target_tensor[10].permute(1,2,0).cpu().numpy() + 1) / 2, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    sample_autoreg, segment_dir_autoreg = sample_diffusion(\n",
    "        vis_crafter,\n",
    "        frames_tensor=frames_target_tensor,\n",
    "        warped_images=warped_images,\n",
    "        # frames 49 - 39\n",
    "        frames_ref=frames_source_target_tensor[:10],\n",
    "        masks=masks,\n",
    "        opts=opts,\n",
    "        segment_dir=segment_dir,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    frames_autoreg_np, _ = load_video_frames(\n",
    "        segment_dir_autoreg + '/gen.mp4', \n",
    "        opts.video_length, \n",
    "        opts.stride, \n",
    "        opts.max_res,\n",
    "        opts.device,\n",
    "        reverse=False\n",
    "    )\n",
    "    frames_source_target_autoreg_np = np.concatenate([frames_source_target_np, frames_autoreg_np], axis=0)\n",
    "    \n",
    "    # Add this in your autoregressive_loop function before the return\n",
    "    hypernerf_dir = os.path.join(segment_dir, 'hypernerf_dataset')\n",
    "    save_hypernerf_dataset(\n",
    "        frames_source_target_autoreg_np,\n",
    "        poses_source_target,\n",
    "        vis_crafter.K[0],\n",
    "        depths_source_target,\n",
    "        global_pc,\n",
    "        global_colors,\n",
    "        warped_images,\n",
    "        masks,\n",
    "        opts,\n",
    "        hypernerf_dir\n",
    "    )\n",
    "    \n",
    "    return segment_dir_autoreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target frames with optional reversal\n",
    "frames_input_np, _ = load_video_frames(\n",
    "    opts.video_path, \n",
    "    opts.video_length, \n",
    "    opts.stride, \n",
    "    opts.max_res,\n",
    "    opts.device,\n",
    "    reverse=True\n",
    ")\n",
    "# depth estimation\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    depths_input = vis_crafter.depth_estimater.infer(\n",
    "        frames_input_np,\n",
    "        opts.near,\n",
    "        opts.far,\n",
    "        opts.depth_inference_steps,\n",
    "        opts.depth_guidance_scale,\n",
    "        window_size=opts.window_size,\n",
    "        overlap=opts.overlap,\n",
    "    ).to(opts.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "# Cameras\n",
    "##########################################\n",
    "\n",
    "radius = (\n",
    "    depths_input[0, 0, depths_input.shape[-2] // 2, depths_input.shape[-1] // 2].cpu()\n",
    "    * opts.radius_scale\n",
    ")\n",
    "radius = min(radius, 5)\n",
    "\n",
    "# radius = 10\n",
    "\n",
    "\n",
    "c2ws_anchor = torch.tensor([ \n",
    "            [-1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, -1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "    ]).unsqueeze(0).to(opts.device)\n",
    "\n",
    "c2ws_target = generate_traj_specified(\n",
    "    c2ws_anchor, \n",
    "    opts.target_pose, \n",
    "    opts.video_length * opts.n_splits, \n",
    "    opts.device\n",
    ")\n",
    "c2ws_target[:, 2, 3] += radius\n",
    "\n",
    "c2ws_init = c2ws_target[0].repeat(opts.video_length, 1, 1)\n",
    "\n",
    "\n",
    "traj_segments = c2ws_target.view(opts.n_splits, opts.video_length, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_source_np = None\n",
    "frames_target_np = frames_input_np\n",
    "\n",
    "poses_source = c2ws_init\n",
    "poses_target = traj_segments[0]\n",
    "    \n",
    "for i in range(opts.n_splits):\n",
    "    segment_dir_autoreg = autoregressive_loop(\n",
    "        frames_source_np,\n",
    "        frames_target_np,\n",
    "        poses_source,\n",
    "        poses_target,\n",
    "        radius,\n",
    "    )\n",
    "    \n",
    "    # concatenate frames_source_np and frames_target_np, handling None initially\n",
    "    if frames_source_np is None:\n",
    "        frames_source_np = frames_target_np\n",
    "    else:\n",
    "        frames_source_np = np.concatenate([frames_source_np, frames_target_np], axis=0)\n",
    "    \n",
    "    frames_target_np, _ = load_video_frames(\n",
    "        segment_dir_autoreg + '/gen.mp4', \n",
    "        opts.video_length, \n",
    "        opts.stride, \n",
    "        opts.max_res,\n",
    "        opts.device,\n",
    "        reverse=False\n",
    "    )\n",
    "    \n",
    "    poses_source = torch.cat([poses_source, poses_target], dim=0)\n",
    "    poses_target = traj_segments[i+1] if i + 1 < opts.n_splits else None\n",
    "    \n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read /home/azhuravl/nobackup/hypernerf/broom2_no_mask/points3D_downsample2.ply\n",
    "import plyfile\n",
    "plydata = plyfile.PlyData.read('/home/azhuravl/nobackup/hypernerf/broom2_no_mask/points3D_downsample2.ply')\n",
    "plydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding /home/azhuravl/work/TrajectoryCrafter/experiments/13-10-2025/rhino_20251013_1601_90_0_0_0_1_auto_s4/stage_8/local_pcs/45/pc_00.pt\n",
      "adding /home/azhuravl/work/TrajectoryCrafter/experiments/13-10-2025/rhino_20251013_1601_90_0_0_0_1_auto_s4/stage_8/local_pcs/45/pc_01.pt\n",
      "adding /home/azhuravl/work/TrajectoryCrafter/experiments/13-10-2025/rhino_20251013_1601_90_0_0_0_1_auto_s4/stage_8/local_pcs/45/pc_02.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(viser)</span> Connection opened <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total<span style=\"font-weight: bold\">)</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> persistent messages\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1mviser\u001b[0m\u001b[1m)\u001b[0m Connection opened \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m total\u001b[1m)\u001b[0m, \u001b[1;36m11\u001b[0m persistent messages\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(viser)</span> Connection closed <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> total<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1mviser\u001b[0m\u001b[1m)\u001b[0m Connection closed \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m total\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(viser)</span> Connection opened <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total<span style=\"font-weight: bold\">)</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> persistent messages\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1mviser\u001b[0m\u001b[1m)\u001b[0m Connection opened \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m total\u001b[1m)\u001b[0m, \u001b[1;36m11\u001b[0m persistent messages\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "server.scene.reset()\n",
    "\n",
    "# add original point cloud #0, and all cameras from first segment\n",
    "\n",
    "stage_i = 8\n",
    "step_j = 45\n",
    "\n",
    "\n",
    "base_dir = '/home/azhuravl/work/TrajectoryCrafter/experiments/13-10-2025/rhino_20251013_1601_90_0_0_0_1_auto_s4/'\\\n",
    "            f'stage_{stage_i}'\n",
    "\n",
    "for k in range(3):\n",
    "    print('adding', f'{base_dir}/local_pcs/{step_j:02d}/pc_{k:02d}.pt')\n",
    "    add_points(\n",
    "        server,\n",
    "        torch.load(f'{base_dir}/local_pcs/{step_j:02d}/pc_{k:02d}.pt', weights_only=True).numpy(),\n",
    "        torch.load(f'{base_dir}/local_colors/{step_j:02d}/color_{k:02d}.pt', weights_only=True).numpy(),\n",
    "        name=f'pc_{k:02d}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/azhuravl/work/4DGaussians/render_gaussians.py --splat_paths /home/azhuravl/work/4DGaussians/output/dnerf/bouncingballs/gaussian_pertimestamp/time_00003.ply"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
