{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stereoanyvideo.datasets.video_datasets as video_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_monkaa = video_datasets.SequenceSceneFlowDatasetCamera(\n",
    "    aug_params=None,\n",
    "    root=\"/home/azhuravl/nobackup/SceneFlow\",\n",
    "    dstype=\"frames_cleanpass\",\n",
    "    sample_len=49,\n",
    "    things_test=False,\n",
    "    add_things=False,\n",
    "    add_monkaa=True,\n",
    "    add_driving=False,\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_monkaa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a PyTorch3D camera from your output_tensor\n",
    "viewpoint = data_0[\"viewpoint\"][0][0]  # First frame, left camera\n",
    "\n",
    "# Convert to OpenCV format\n",
    "opencv_params = video_datasets.pytorch3d_to_opencv_camera_general(viewpoint, (540, 960))\n",
    "\n",
    "# Access the parameters\n",
    "K = opencv_params['K']          # 3x3 intrinsic matrix\n",
    "R = opencv_params['R']          # 3x3 rotation matrix\n",
    "t = opencv_params['t']          # 3x1 translation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate depth from disparity using torch\n",
    "\n",
    "import torch\n",
    "disp = data_0['disp'][0][0]\n",
    "focal_length = K[0, 0]\n",
    "baseline = 1\n",
    "depth = -(focal_length * baseline) / disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rgb and disparity for the first frame\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(data_0['img'][0][0].permute(1, 2, 0).int().numpy())\n",
    "plt.title('RGB Frame 0')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(data_0['disp'][0][0].permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "plt.title('Disparity Frame 0')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(depth.permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "plt.title('Depth Frame 0')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "\n",
    "import warper_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warper = warper_point_cloud.GlobalPointCloudWarper(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_data(data, baseline=1):\n",
    "    \"\"\"\n",
    "    Extract frames, depths, poses, and camera intrinsics from data object.\n",
    "    \n",
    "    Args:\n",
    "        data: Data object containing 'img', 'disp', and 'viewpoint'\n",
    "        baseline: Baseline for depth calculation (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        frames_tensor: [T, 3, H, W] in [-1, 1] range\n",
    "        depths: [T, 1, H, W] depth maps\n",
    "        poses_tensor: [T, 4, 4] camera poses\n",
    "        K_tensor: [T, 3, 3] camera intrinsics\n",
    "    \"\"\"\n",
    "    # Convert to [-1, 1] range\n",
    "    frames_tensor = data['img'][:,0] / 127.5 - 1.0  # [T, 3, H, W]\n",
    "    disparity_tensor = data['disp'][:,0]  # [T, 1, H, W]\n",
    "    \n",
    "    poses_list = []\n",
    "    K_list = []\n",
    "    for i in range(frames_tensor.shape[0]):\n",
    "        viewpoint = data[\"viewpoint\"][i][0]\n",
    "        opencv_params = video_datasets.pytorch3d_to_opencv_camera_general(viewpoint, (540, 960))\n",
    "        R = opencv_params['R']\n",
    "        t = opencv_params['t']\n",
    "        pose = torch.eye(4)\n",
    "        pose[:3, :3] = R\n",
    "        pose[:3, 3] = t.squeeze()\n",
    "        poses_list.append(pose)\n",
    "        \n",
    "        K_list.append(opencv_params['K'])\n",
    "\n",
    "    poses_tensor = torch.stack(poses_list)  # [T, 4, 4]\n",
    "    K_tensor = torch.stack(K_list)  # [T, 3, 3]\n",
    "    \n",
    "    # Calculate focal length from K tensor\n",
    "    focal_length = K_tensor[0, 0, 0]\n",
    "    \n",
    "    depths = -(focal_length * baseline) / disparity_tensor  # [T, 1, H, W]\n",
    "    \n",
    "    return frames_tensor, depths, poses_tensor, K_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "frames_tensor, depths, poses_tensor, K_tensor = extract_video_data(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pc_list = []\n",
    "color_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(frames_tensor.shape[0])):\n",
    "        points, colors, _ = warper.create_pointcloud_from_image(\n",
    "            frames_tensor[i:i+1],\n",
    "            None,\n",
    "            depths[i:i+1],\n",
    "            poses_tensor[i:i+1],\n",
    "            K_tensor[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "        pc_list.append(points)\n",
    "        color_list.append(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_images = []\n",
    "masks = []        \n",
    "\n",
    "for i in tqdm(range(frames_tensor.shape[0])):\n",
    "\n",
    "    warped_image, mask = warper.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "        pc_list[i],\n",
    "        color_list[i],\n",
    "        poses_tensor[0:1].to('cuda'),\n",
    "        K_tensor[0:1].to('cuda'),\n",
    "        (540, 960),\n",
    "        point_size=2,\n",
    "    )\n",
    "    \n",
    "    cleaned_mask = utils_ar.clean_single_mask_simple(\n",
    "        mask[0],\n",
    "        kernel_size=9,\n",
    "        n_erosion_steps=1,\n",
    "        n_dilation_steps=1\n",
    "        )\n",
    "    # should stay in [-1, 1] range\n",
    "    \n",
    "    cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "    warped_image = warped_image * cleaned_mask\n",
    "    \n",
    "    warped_images.append(warped_image)\n",
    "    masks.append(cleaned_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(warped_image[0].cpu().permute(1, 2, 0).numpy())\n",
    "# show image and mask\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(warped_images[40][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "plt.title('Warped Image to Frame 10')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(masks[40][0].cpu().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images 0 and 10\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frames_tensor[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "plt.title('Original Image Frame 0')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(frames_tensor[40].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "plt.title('Original Image Frame 10')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "import utils_autoregressive as utils_ar\n",
    "from datetime import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/monkaa.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajcrafter = utils_ar.TrajCrafterAutoregressive(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# frames_tensor = (\n",
    "    # torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "# reverse this to get frames in numpy\n",
    "frames_np = ((frames_tensor.cpu().permute(0, 2, 3, 1).numpy() + 1.0) / 2.0).astype(np.float32)\n",
    "\n",
    "trajcrafter.prompt = trajcrafter.get_caption(opts, frames_np[opts.video_length // 2])\n",
    "print(trajcrafter.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, segment_dir = utils_ar.sample_diffusion(\n",
    "    trajcrafter,\n",
    "    frames_tensor,\n",
    "    warped_images,\n",
    "    frames_tensor[:10],\n",
    "    masks,\n",
    "    opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_features = trajcrafter.pipeline.collected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in collected_features['timestep_839'].keys():\n",
    "    print(keys, '           ', collected_features['timestep_839'][keys].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "384 * 672*49, 13104 * 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get size of collected features in MB\n",
    "total_size = 0\n",
    "for timestep in collected_features.keys():\n",
    "    for keys in collected_features[timestep].keys():\n",
    "        total_size += collected_features[timestep][keys].element_size() * collected_features[timestep][keys].nelement()\n",
    "print(f\"Total size of collected features: {total_size / (1024 ** 2):.2f} MB\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fix cameras - why it goes left-up?\n",
    "# how to get GT depth\n",
    "# why do we have so many features?\n",
    "# generate 100 features + depths samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
