{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/12_11_25_consistent_depth/autoregressive_alignment')\n",
    "\n",
    "from utils_autoregressive import load_video_frames, generate_traj_specified, TrajCrafterAutoregressive\n",
    "from parsing import get_parser\n",
    "from autoregressive_loop_alignment import autoregressive_loop, estimate_depth_without_alignment\n",
    "from autoregressive_loop_alignment import video_to_pcs, invert_depth_with_scale, imagenet_to_0_1\n",
    "from warper_point_cloud import GlobalPointCloudWarper\n",
    "\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/Video-Depth-Anything')\n",
    "sys.path.append('/home/azhuravl/work/Video-Depth-Anything/video_depth_anything/util')\n",
    "\n",
    "# Video Depth Anything imports\n",
    "from video_depth_anything.video_depth import VideoDepthAnything\n",
    "from utils.dc_utils import read_video_frames\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/12_11_25_consistent_depth/depth_alignment')\n",
    "\n",
    "# Depth trainer import\n",
    "from depth_trainer import DepthAlignmentTrainer\n",
    "from consistent_depth import prepare_frames, denormalize_rgb\n",
    "    \n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def setup_opts():\n",
    "    sys.argv = [\n",
    "        \"\",\n",
    "        \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/rhino.mp4\",\n",
    "        \"--n_splits\", \"4\",\n",
    "        \"--overlap_frames\", \"0\",\n",
    "        \"--radius\", \"0\",\n",
    "        \"--mode\", \"gradual\",\n",
    "        \"--video_length\", \"32\",\n",
    "        # \"--sample_size\", \"266\", \"462\"\n",
    "    ]\n",
    "\n",
    "    parser = get_parser()\n",
    "    opts_base = parser.parse_args()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "    # Setup\n",
    "    opts_base.weight_dtype = torch.bfloat16\n",
    "    opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "    opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "    # Create TrajCrafterVisualization instance for autoregressive generation\n",
    "    radius = opts_base.radius\n",
    "\n",
    "    pose = [90, 0, 0, 0, 1]\n",
    "    # name = '120_0_0_0_3', make it infer values from pose\n",
    "    name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Running Autoregressive {name} ===\")\n",
    "    opts = copy.deepcopy(opts_base)\n",
    "    opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "    opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "    opts.camera = \"target\"\n",
    "    opts.target_pose = pose\n",
    "    opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "    # Make directories\n",
    "    os.makedirs(opts.save_dir, exist_ok=True)\n",
    "\n",
    "    return opts\n",
    "\n",
    "\n",
    "def setup_vda():\n",
    "    \n",
    "    class ArgsVDA:\n",
    "        def __init__(self):\n",
    "            self.input_video = '/home/azhuravl/scratch/datasets_latents/monkaa_1000/000/videos/input_video.mp4'\n",
    "            self.output_dir = '/home/azhuravl/work/Video-Depth-Anything/outputs'\n",
    "            self.input_size = 256\n",
    "            self.max_res = 1280\n",
    "            self.encoder = 'vitl'\n",
    "            self.max_len = -1\n",
    "            self.target_fps = -1\n",
    "            self.metric = False\n",
    "            self.fp32 = False\n",
    "            self.grayscale = False\n",
    "            self.save_npz = False\n",
    "            self.save_exr = False\n",
    "            self.focal_length_x = 470.4\n",
    "            self.focal_length_y = 470.4\n",
    "\n",
    "    args_vda = ArgsVDA()\n",
    "    \n",
    "    model_configs = {\n",
    "        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    }\n",
    "    checkpoint_name = 'video_depth_anything'\n",
    "\n",
    "    video_depth_anything = VideoDepthAnything(**model_configs[args_vda.encoder], metric=args_vda.metric)\n",
    "    video_depth_anything.load_state_dict(torch.load(\n",
    "        f'/home/azhuravl/work/Video-Depth-Anything/checkpoints/{checkpoint_name}_{args_vda.encoder}.pth', \n",
    "        map_location='cpu', weights_only=True), strict=True)\n",
    "    video_depth_anything = video_depth_anything.to(DEVICE).eval()\n",
    "\n",
    "    # disable grad for video_depth_anything\n",
    "    for param in video_depth_anything.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return video_depth_anything, args_vda\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Initialize models\n",
    "#########################################\n",
    "\n",
    "opts = setup_opts()\n",
    "\n",
    "vis_crafter = TrajCrafterAutoregressive(opts)\n",
    "funwarp = GlobalPointCloudWarper(device=opts.device, max_points=2000000)\n",
    "\n",
    "# TODO: depth estimator + alignment\n",
    "video_depth_anything, args_vda = setup_vda()\n",
    "\n",
    "# from depth_trainer import DepthAlignmentTrainer\n",
    "depth_trainer = DepthAlignmentTrainer(\n",
    "    video_depth_anything,\n",
    "    lr=2e-3,\n",
    "    device=opts.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################\n",
    "# Read video, normalize to imagenet\n",
    "###########################################\n",
    "\n",
    "\n",
    "frames, target_fps = read_video_frames(\n",
    "    opts.video_path,\n",
    "    32, args_vda.target_fps, args_vda.max_res\n",
    "    ) # (32, 480, 854, 3) uint8 0 255\n",
    "# reverse the frames\n",
    "# frames = frames[::-1]\n",
    "\n",
    "\n",
    "frames_resized_im, orig_dims = prepare_frames(\n",
    "    frames, input_size=opts.sample_size, normalize_imagenet=True,\n",
    "    )  # torch.Size([32, 3, 266, 462]) torch.float32 tensor(-2.2437) tensor(2.6739)\n",
    "frames_resized_im = frames_resized_im.squeeze(0)\n",
    "\n",
    "\n",
    "print('frames_resized_im', frames_resized_im.shape, frames_resized_im.dtype, frames_resized_im.min(), frames_resized_im.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoregressive_loop_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(autoregressive_loop_alignment)\n",
    "\n",
    "from autoregressive_loop_alignment import autoregressive_loop, estimate_depth_with_padding, imagenet_to_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autoregressive_loop_alignment import estimate_depth_without_alignment\n",
    "\n",
    "###############################################\n",
    "# Estimate depth - will be used as anchor\n",
    "###############################################\n",
    "\n",
    "depth_scale = 10000.0  # set depth scale\n",
    "\n",
    "depths_input = estimate_depth_without_alignment(\n",
    "    frames_resized_im,\n",
    "    depth_trainer,\n",
    "    depth_scale,\n",
    ")\n",
    "\n",
    "depths_input_inv = invert_depth_with_scale(depths_input, depth_scale)\n",
    "\n",
    "print('depths_input_inv', depths_input_inv.shape, depths_input_inv.dtype, depths_input_inv.min(), depths_input_inv.max())\n",
    "print('depths_input', depths_input.shape, depths_input.dtype, depths_input.min(), depths_input.max())\n",
    "\n",
    "# save several depth frames using matplotlib, with colorbar\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(0, depths_input_inv.shape[0], 5):\n",
    "    plt.imshow(depths_input_inv[i, 0].cpu().numpy(), cmap='plasma')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(shrink=0.4)\n",
    "    plt.savefig(f'{opts.save_dir}/inv_depth_frame_{i}.png')\n",
    "    plt.clf()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##########################################\n",
    "# Camera trajectories\n",
    "##########################################\n",
    "\n",
    "radius = (\n",
    "    depths_input[0, 0, depths_input.shape[-2] // 2, depths_input.shape[-1] // 2].cpu()\n",
    "    * opts.radius_scale\n",
    ")\n",
    "# radius = min(radius, 5)\n",
    "\n",
    "print(f\"Estimated radius: {radius}\")\n",
    "\n",
    "c2ws_anchor = torch.tensor([ \n",
    "            [-1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, -1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "    ]).unsqueeze(0).to(opts.device)\n",
    "\n",
    "c2ws_target = generate_traj_specified(\n",
    "    c2ws_anchor, \n",
    "    opts.target_pose, \n",
    "    opts.video_length * opts.n_splits, \n",
    "    opts.device\n",
    ")\n",
    "c2ws_target[:, 2, 3] += radius\n",
    "\n",
    "c2ws_init = c2ws_target[0].repeat(opts.video_length, 1, 1)\n",
    "traj_segments = c2ws_target.view(opts.n_splits, opts.video_length, 4, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "# Initialize global point clouds\n",
    "###############################################\n",
    "\n",
    "frames_resized_tensor = imagenet_to_0_1(frames_resized_im).to(opts.device) * 2.0 - 1.0\n",
    "    \n",
    "\n",
    "global_pcs, global_colors = video_to_pcs(\n",
    "    frames_resized_tensor,\n",
    "    depths_input,\n",
    "    intrinsics_torch=vis_crafter.K,\n",
    "    extrinsics_torch=c2ws_init,\n",
    "    funwarp=funwarp,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "# Autoregressive loop\n",
    "###############################################\n",
    "\n",
    "frames_source_im = frames_resized_im\n",
    "\n",
    "poses_source = c2ws_init\n",
    "poses_target = traj_segments[0]\n",
    "    \n",
    "for i in range(opts.n_splits):\n",
    "    \n",
    "    segment_dir_autoreg, global_pcs, global_colors = autoregressive_loop(\n",
    "        frames_source_im,\n",
    "        poses_source,\n",
    "        poses_target,\n",
    "        global_pcs,\n",
    "        global_colors,\n",
    "        radius,\n",
    "        opts,  \n",
    "        vis_crafter,\n",
    "        funwarp,\n",
    "        video_depth_anything,\n",
    "        depth_trainer,\n",
    "        args_vda,\n",
    "        depth_scale,\n",
    "        i\n",
    "    )\n",
    "    \n",
    "    # next pose in sequence\n",
    "    poses_source = poses_target\n",
    "    poses_target = traj_segments[i+1] if i + 1 < opts.n_splits else None\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Read generated video frames for next iteration\n",
    "    ###############################################\n",
    "    \n",
    "    # TODO: remove reading video\n",
    "    \n",
    "    print('frames_source_im', frames_source_im.shape, frames_source_im.dtype, frames_source_im.min(), frames_source_im.max())\n",
    "    # print('frames_target_im', frames_target_im.shape, frames_target_im.dtype, frames_target_im.min(), frames_target_im.max())\n",
    "\n",
    "    frames, target_fps = read_video_frames(\n",
    "        segment_dir_autoreg + '/gen.mp4',\n",
    "        32, args_vda.target_fps, args_vda.max_res\n",
    "        ) # (32, 480, 854, 3) uint8 0 255\n",
    "\n",
    "\n",
    "    print('frames', frames.shape, frames.dtype, frames.min(), frames.max())\n",
    "\n",
    "    frames_resized_im, orig_dims = prepare_frames(\n",
    "        frames, input_size=opts.sample_size, normalize_imagenet=True, \n",
    "        )  # torch.Size([32, 3, 266, 462]) torch.float32 tensor(-2.2437) tensor(2.6739)\n",
    "    frames_resized_im = frames_resized_im.squeeze(0)\n",
    "    \n",
    "    print('frames_resized_im', frames_resized_im.shape, frames_resized_im.dtype, frames_resized_im.min(), frames_resized_im.max())\n",
    "\n",
    "    frames_source_im = frames_resized_im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depth_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(depth_trainer)\n",
    "\n",
    "from depth_trainer import DepthAlignmentTrainer\n",
    "depth_trainer = DepthAlignmentTrainer(\n",
    "    video_depth_anything,\n",
    "    lr=2e-3,\n",
    "    device=opts.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoregressive_loop_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(autoregressive_loop_alignment)\n",
    "\n",
    "from autoregressive_loop_alignment import autoregressive_loop, imagenet_to_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle frame / pc reversal\n",
    "# comparison videos rather than plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "i = 1   \n",
    "\n",
    "segment_dir_autoreg, global_pcs, global_colors = autoregressive_loop(\n",
    "    frames_source_im,\n",
    "    poses_source,\n",
    "    poses_target,\n",
    "    global_pcs,\n",
    "    global_colors,\n",
    "    radius,\n",
    "    opts,  \n",
    "    vis_crafter,\n",
    "    funwarp,\n",
    "    video_depth_anything,\n",
    "    depth_trainer,\n",
    "    args_vda,\n",
    "    depth_scale,\n",
    "    i\n",
    ")\n",
    "\n",
    "# next pose in sequence\n",
    "poses_source = poses_target\n",
    "poses_target = traj_segments[i+1] if i + 1 < opts.n_splits else None\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Read generated video frames for next iteration\n",
    "###############################################\n",
    "\n",
    "# TODO: remove reading video\n",
    "\n",
    "print('frames_source_im', frames_source_im.shape, frames_source_im.dtype, frames_source_im.min(), frames_source_im.max())\n",
    "# print('frames_target_im', frames_target_im.shape, frames_target_im.dtype, frames_target_im.min(), frames_target_im.max())\n",
    "\n",
    "frames, target_fps = read_video_frames(\n",
    "    segment_dir_autoreg + '/gen.mp4',\n",
    "    32, args_vda.target_fps, args_vda.max_res\n",
    "    ) # (32, 480, 854, 3) uint8 0 255\n",
    "\n",
    "\n",
    "print('frames', frames.shape, frames.dtype, frames.min(), frames.max())\n",
    "\n",
    "frames_resized_im, orig_dims = prepare_frames(\n",
    "    frames, input_size=opts.sample_size, normalize_imagenet=True, \n",
    "    )  # torch.Size([32, 3, 266, 462]) torch.float32 tensor(-2.2437) tensor(2.6739)\n",
    "frames_resized_im = frames_resized_im.squeeze(0)\n",
    "\n",
    "print('frames_resized_im', frames_resized_im.shape, frames_resized_im.dtype, frames_resized_im.min(), frames_resized_im.max())\n",
    "\n",
    "frames_source_im = frames_resized_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cuda memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
