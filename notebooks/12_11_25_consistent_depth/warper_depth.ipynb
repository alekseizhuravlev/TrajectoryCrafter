{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from demo import TrajCrafter\n",
    "from models.utils import Warper, read_video_frames, sphere2pose, save_video\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.infer import DepthCrafterDemo\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add core.py to path if needed\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/28_08_25_trajectories')\n",
    "from core import VisualizationWarper\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "\n",
    "\n",
    "class TrajCrafterAutoregressive(TrajCrafter):\n",
    "    def __init__(self, opts):\n",
    "        super().__init__(opts)\n",
    "\n",
    "        # self.funwarp = VisualizationWarper(device=opts.device)\n",
    "        self.prompt = None\n",
    "        \n",
    "        self.K = torch.tensor(\n",
    "            [[500, 0.0, 512.], [0.0, 500, 288.], [0.0, 0.0, 1.0]]\n",
    "            ).repeat(opts.video_length, 1, 1).to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/rhino.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "# name = \"right_90\"\n",
    "# pose = [0, 90, radius, 0, 0]\n",
    "\n",
    "# name = \"top_90\"\n",
    "# pose = [90, 0, radius, 0, 0]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "# name = '120_0_0_0_3', make it infer values from pose\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "\n",
    "print(f\"\\n=== Running Autoregressive {name} ===\")\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_crafter = TrajCrafterAutoregressive(opts_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(warper_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warper_point_cloud\n",
    "\n",
    "# funwarp = VisualizationWarper(device=opts.device)\n",
    "funwarp = warper_point_cloud.GlobalPointCloudWarper(device=opts.device, max_points=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.utils as utils\n",
    "from utils_autoregressive import pad_video, generate_traj_specified, clean_single_mask_simple\n",
    "\n",
    "# read input video\n",
    "\n",
    "frames_np = utils.read_video_frames(\n",
    "    opts.video_path, opts.video_length, opts.stride, opts.max_res,\n",
    "    # height=opts.sample_size[0], width=opts.sample_size[1],\n",
    ")\n",
    "\n",
    "# pad if too short\n",
    "frames_np = pad_video(frames_np, opts.video_length)\n",
    "# frames_np = frames_np[::-1, ...].copy()\n",
    "\n",
    "\n",
    "frames_tensor = (\n",
    "    torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    ")  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "assert frames_tensor.shape[0] == opts.video_length\n",
    "\n",
    "\n",
    "\n",
    "# prompt\n",
    "vis_crafter.prompt = vis_crafter.get_caption(opts, frames_np[opts.video_length // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Input Depth\n",
    "############################################\n",
    "\n",
    "# TODO: this takes frames as 1024 x 576? size\n",
    "# the sample will be about 640 x 360 - is it ok?\n",
    "depths = vis_crafter.depth_estimater.infer(\n",
    "    frames_np,\n",
    "    opts.near,\n",
    "    opts.far,\n",
    "    opts.depth_inference_steps,\n",
    "    opts.depth_guidance_scale,\n",
    "    window_size=opts.window_size,\n",
    "    overlap=opts.overlap,\n",
    ").to(opts.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Cameras\n",
    "##########################################\n",
    "\n",
    "radius = (\n",
    "    depths[0, 0, depths.shape[-2] // 2, depths.shape[-1] // 2].cpu()\n",
    "    * opts.radius_scale\n",
    ")\n",
    "radius = min(radius, 5)\n",
    "\n",
    "# radius = 10\n",
    "\n",
    "\n",
    "c2ws_anchor = torch.tensor([ \n",
    "            [-1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, -1.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "    ]).unsqueeze(0).to(opts.device)\n",
    "\n",
    "c2ws_target = generate_traj_specified(\n",
    "    c2ws_anchor, \n",
    "    opts.target_pose, \n",
    "    opts.video_length * opts.n_splits, \n",
    "    opts.device\n",
    ")\n",
    "c2ws_target[:, 2, 3] += radius\n",
    "\n",
    "c2ws_init = c2ws_target[0].repeat(opts.video_length, 1, 1)\n",
    "\n",
    "\n",
    "traj_segments = c2ws_target.view(opts.n_splits, opts.video_length, 4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "point_clouds = []\n",
    "colors_list = []\n",
    "weights_list = []\n",
    "\n",
    "global_pc = []\n",
    "global_colors = []\n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "    with torch.no_grad():\n",
    "        points, colors, weights = funwarp.create_pointcloud_from_image(\n",
    "            frames_tensor[i:i+1],\n",
    "            None,\n",
    "            depths[i:i+1],\n",
    "            c2ws_init[i:i+1],\n",
    "            vis_crafter.K[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "    point_clouds.append(points)\n",
    "    colors_list.append(colors)  \n",
    "    weights_list.append(weights)\n",
    "    \n",
    "    global_pc.append(points)\n",
    "    global_colors.append(colors)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_images = []\n",
    "warped_depths = []\n",
    "masks = []        \n",
    "\n",
    "for i in tqdm(range(opts.video_length)):\n",
    "\n",
    "    warped_image, mask, warped_depth = funwarp.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "        point_clouds[i],\n",
    "        colors_list[i],\n",
    "        c2ws_target[i:i+1],\n",
    "        vis_crafter.K[0:1].to(opts.device),\n",
    "        (576, 1024),\n",
    "        point_size=2,\n",
    "        return_depth=True\n",
    "    )\n",
    "    \n",
    "    # single_mask = masks[10][0]  # Shape: (1, H, W)\n",
    "    # print(single_mask.shape)\n",
    "    cleaned_mask = clean_single_mask_simple(\n",
    "        mask[0],\n",
    "        kernel_size=9,\n",
    "        n_erosion_steps=1,\n",
    "        n_dilation_steps=1\n",
    "        )\n",
    "    # should stay in [-1, 1] range\n",
    "    \n",
    "    cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "    \n",
    "    warped_image = warped_image * cleaned_mask\n",
    "    warped_depth = warped_depth * cleaned_mask\n",
    "    \n",
    "    warped_images.append(warped_image)\n",
    "    warped_depths.append(warped_depth)\n",
    "    masks.append(cleaned_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_mask.min(), cleaned_mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_depths[0].shape, warped_depths[10].min(), warped_depths[10].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,3,1)\n",
    "\n",
    "plt.imshow((warped_images[30][0].permute(1,2,0).cpu().numpy() + 1) / 2)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow((warped_depths[30][0].permute(1,2,0).cpu().numpy()))\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.5)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# plt.imshow(masks[10][0].permute(1,2,0).cpu().numpy(), cmap='gray')\n",
    "plt.imshow((frames_tensor[30].permute(1,2,0).cpu().numpy() + 1) / 2, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
