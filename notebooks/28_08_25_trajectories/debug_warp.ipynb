{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Path Setup\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"/home/azhuravl/work/TrajectoryCrafter\")\n",
    "# os.chdir(trajcrafter_path)\n",
    "\n",
    "import inference_orbits\n",
    "\n",
    "sys.path.insert(0, \"/home/azhuravl/work/TrajectoryCrafter/notebooks/28_08_25_trajectories\")\n",
    "\n",
    "import core, trajectory_generation, viser_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Argument Setup\n",
    "# Create opts manually for notebook use\n",
    "parser = inference_orbits.get_parser()\n",
    "opts_base = parser.parse_args([\n",
    "    '--video_path', './test/videos/0-NNvgaTcVzAG0-r.mp4',  # Change this path\n",
    "    '--radius', '1.0',\n",
    "    '--device', 'cuda:0'\n",
    "])\n",
    "\n",
    "# Set common parameters\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.camera = \"target\"\n",
    "opts_base.mode = \"gradual\"\n",
    "opts_base.mask = True\n",
    "opts_base.target_pose = [0, 90, opts_base.radius, 0, 0]  # right_90 example\n",
    "opts_base.exp_name = f\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d97d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Cell 4: Run Visualization\n",
    "# Initialize visualization TrajCrafter\n",
    "print(\"Initializing TrajCrafter for visualization...\")\n",
    "vis_crafter = core.TrajCrafterVisualization(opts_base)\n",
    "\n",
    "# Extract scene data\n",
    "print(\"Extracting scene data...\")\n",
    "scene_data = vis_crafter.extract_scene_data(opts_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class WarperDebug(core.VisualizationWarper):\n",
    "\n",
    "    def forward_warp(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        mask1: Optional[torch.Tensor],\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        transformation2: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "        intrinsic2: Optional[torch.Tensor],\n",
    "        mask=False,\n",
    "        twice=False,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Given a frame1 and global transformations transformation1 and transformation2, warps frame1 to next view using\n",
    "        bilinear splatting.\n",
    "        All arrays should be torch tensors with batch dimension and channel first\n",
    "        :param frame1: (b, 3, h, w). If frame1 is not in the range [-1, 1], either set is_image=False when calling\n",
    "                        bilinear_splatting on frame within this function, or modify clipping in bilinear_splatting()\n",
    "                        method accordingly.\n",
    "        :param mask1: (b, 1, h, w) - 1 for known, 0 for unknown. Optional\n",
    "        :param depth1: (b, 1, h, w)\n",
    "        :param transformation1: (b, 4, 4) extrinsic transformation matrix of first view: [R, t; 0, 1]\n",
    "        :param transformation2: (b, 4, 4) extrinsic transformation matrix of second view: [R, t; 0, 1]\n",
    "        :param intrinsic1: (b, 3, 3) camera intrinsic matrix\n",
    "        :param intrinsic2: (b, 3, 3) camera intrinsic matrix. Optional\n",
    "        \"\"\"\n",
    "        if self.resolution is not None:\n",
    "            assert frame1.shape[2:4] == self.resolution\n",
    "        b, c, h, w = frame1.shape\n",
    "        if mask1 is None:\n",
    "            mask1 = torch.ones(size=(b, 1, h, w)).to(frame1)\n",
    "        if intrinsic2 is None:\n",
    "            intrinsic2 = intrinsic1.clone()\n",
    "\n",
    "        assert frame1.shape == (b, 3, h, w)\n",
    "        assert mask1.shape == (b, 1, h, w)\n",
    "        assert depth1.shape == (b, 1, h, w)\n",
    "        assert transformation1.shape == (b, 4, 4)\n",
    "        assert transformation2.shape == (b, 4, 4)\n",
    "        assert intrinsic1.shape == (b, 3, 3)\n",
    "        assert intrinsic2.shape == (b, 3, 3)\n",
    "\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        mask1 = mask1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation1 = transformation1.to(self.device).to(self.dtype)\n",
    "        transformation2 = transformation2.to(self.device).to(self.dtype)\n",
    "        intrinsic1 = intrinsic1.to(self.device).to(self.dtype)\n",
    "        intrinsic2 = intrinsic2.to(self.device).to(self.dtype)\n",
    "\n",
    "        trans_points1 = self.compute_transformed_points(\n",
    "            depth1, transformation1, transformation2, intrinsic1, intrinsic2\n",
    "        )\n",
    "        trans_coordinates = (\n",
    "            trans_points1[:, :, :, :2, 0] / trans_points1[:, :, :, 2:3, 0]\n",
    "        )\n",
    "        trans_depth1 = trans_points1[:, :, :, 2, 0]\n",
    "        grid = self.create_grid(b, h, w).to(trans_coordinates)\n",
    "        flow12 = trans_coordinates.permute(0, 3, 1, 2) - grid\n",
    "        if not twice:\n",
    "            warped_frame2, mask2 = self.bilinear_splatting(\n",
    "                frame1, mask1, trans_depth1, flow12, None, is_image=True\n",
    "            )\n",
    "            if mask:\n",
    "                warped_frame2, mask2 = self.clean_points(warped_frame2, mask2)\n",
    "            return warped_frame2, mask2, None, flow12\n",
    "\n",
    "        else:\n",
    "            warped_frame2, mask2 = self.bilinear_splatting(\n",
    "                frame1, mask1, trans_depth1, flow12, None, is_image=True\n",
    "            )\n",
    "            # warped_frame2, mask2 = self.clean_points(warped_frame2, mask2)\n",
    "            warped_flow, _ = self.bilinear_splatting(\n",
    "                flow12, mask1, trans_depth1, flow12, None, is_image=False\n",
    "            )\n",
    "            twice_warped_frame1, _ = self.bilinear_splatting(\n",
    "                warped_frame2,\n",
    "                mask2,\n",
    "                depth1.squeeze(1),\n",
    "                -warped_flow,\n",
    "                None,\n",
    "                is_image=True,\n",
    "            )\n",
    "            return twice_warped_frame1, warped_frame2, None, None\n",
    "        \n",
    "        \n",
    "    def compute_transformed_points(\n",
    "        self,\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        transformation2: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "        intrinsic2: Optional[torch.Tensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes transformed position for each pixel location\n",
    "        \"\"\"\n",
    "        if self.resolution is not None:\n",
    "            assert depth1.shape[2:4] == self.resolution\n",
    "        b, _, h, w = depth1.shape\n",
    "        if intrinsic2 is None:\n",
    "            intrinsic2 = intrinsic1.clone()\n",
    "        transformation = torch.bmm(\n",
    "            transformation2, torch.linalg.inv(transformation1)\n",
    "        )  # (b, 4, 4)\n",
    "\n",
    "        x1d = torch.arange(0, w)[None]\n",
    "        y1d = torch.arange(0, h)[:, None]\n",
    "        x2d = x1d.repeat([h, 1]).to(depth1)  # (h, w)\n",
    "        y2d = y1d.repeat([1, w]).to(depth1)  # (h, w)\n",
    "        ones_2d = torch.ones(size=(h, w)).to(depth1)  # (h, w)\n",
    "        ones_4d = ones_2d[None, :, :, None, None].repeat(\n",
    "            [b, 1, 1, 1, 1]\n",
    "        )  # (b, h, w, 1, 1)\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[\n",
    "            None, :, :, :, None\n",
    "        ]  # (1, h, w, 3, 1)\n",
    "\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)  # (b, 3, 3)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        intrinsic2_4d = intrinsic2[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        depth_4d = depth1[:, 0][:, :, :, None, None]  # (b, h, w, 1, 1)\n",
    "        trans_4d = transformation[:, None, None]  # (b, 1, 1, 4, 4)\n",
    "\n",
    "        unnormalized_pos = torch.matmul(\n",
    "            intrinsic1_inv_4d, pos_vectors_homo\n",
    "        )  # (b, h, w, 3, 1)\n",
    "        \n",
    "        # world_point = actual 3D points in world space\n",
    "        world_points = depth_4d * unnormalized_pos  # (b, h, w, 3, 1)\n",
    "        world_points_homo = torch.cat([world_points, ones_4d], dim=3)  # (b, h, w, 4, 1)\n",
    "        trans_world_homo = torch.matmul(trans_4d, world_points_homo)  # (b, h, w, 4, 1)\n",
    "        trans_world = trans_world_homo[:, :, :, :3]  # (b, h, w, 3, 1)\n",
    "        trans_norm_points = torch.matmul(intrinsic2_4d, trans_world)  # (b, h, w, 3, 1)\n",
    "        return trans_norm_points, trans_world, world_points\n",
    "    \n",
    "    def extract_3d_points_with_colors(\n",
    "        self,\n",
    "        frame1: torch.Tensor,\n",
    "        depth1: torch.Tensor,\n",
    "        transformation1: torch.Tensor,\n",
    "        intrinsic1: torch.Tensor,\n",
    "        subsample_step: int = 10\n",
    "    ):\n",
    "        \"\"\"Extract 3D world points and their corresponding colors for visualization\"\"\"\n",
    "        b, c, h, w = frame1.shape\n",
    "        \n",
    "        # Move tensors to device\n",
    "        frame1 = frame1.to(self.device).to(self.dtype)\n",
    "        depth1 = depth1.to(self.device).to(self.dtype)\n",
    "        transformation1 = transformation1.to(self.device).to(self.dtype)\n",
    "        intrinsic1 = intrinsic1.to(self.device).to(self.dtype)\n",
    "        \n",
    "        # Create subsampled pixel coordinates for performance\n",
    "        x_coords = torch.arange(0, w, subsample_step, dtype=torch.float32)\n",
    "        y_coords = torch.arange(0, h, subsample_step, dtype=torch.float32)\n",
    "        x2d, y2d = torch.meshgrid(x_coords, y_coords, indexing='xy')\n",
    "        x2d = x2d.to(depth1.device)\n",
    "        y2d = y2d.to(depth1.device)\n",
    "        ones_2d = torch.ones_like(x2d)\n",
    "        \n",
    "        # Stack into homogeneous coordinates\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[None, :, :, :, None]\n",
    "        \n",
    "        # Subsample depth and colors\n",
    "        depth_sub = depth1[:, 0, ::subsample_step, ::subsample_step]\n",
    "        colors_sub = frame1[:, :, ::subsample_step, ::subsample_step]\n",
    "        \n",
    "        # Unproject to 3D camera coordinates\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]\n",
    "        depth_4d = depth_sub[:, :, :, None, None]\n",
    "        \n",
    "        unnormalized_pos = torch.matmul(intrinsic1_inv_4d, pos_vectors_homo)\n",
    "        camera_points = depth_4d * unnormalized_pos\n",
    "        \n",
    "        # Transform to world coordinates\n",
    "        ones_4d = torch.ones(b, camera_points.shape[1], camera_points.shape[2], 1, 1).to(depth1)\n",
    "        world_points_homo = torch.cat([camera_points, ones_4d], dim=3)\n",
    "        trans_4d = transformation1[:, None, None]\n",
    "        world_points_homo = torch.matmul(trans_4d, world_points_homo)\n",
    "        world_points = world_points_homo[:, :, :, :3, 0]  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # world_points = depth_4d * unnormalized_pos  # (b, h, w, 3, 1)\n",
    "        # world_points_homo = torch.cat([world_points, ones_4d], dim=3)  # (b, h, w, 4, 1)\n",
    "        trans_world_homo = torch.matmul(trans_4d, world_points_homo)  # (b, h, w, 4, 1)\n",
    "        trans_world = trans_world_homo[:, :, :, :3]  # (b, h, w, 3, 1)\n",
    "        trans_norm_points = torch.matmul(intrinsic2_4d, trans_world)  # (b, h, w, 3, 1)\n",
    "        \n",
    "        # Prepare colors\n",
    "        colors = colors_sub.permute(0, 2, 3, 1)  # (b, h_sub, w_sub, 3)\n",
    "        \n",
    "        # Filter valid points (positive depth)\n",
    "        valid_mask = depth_sub > 0  # (b, h_sub, w_sub)\n",
    "        \n",
    "        # Flatten and filter\n",
    "        points_3d = world_points[valid_mask]  # (N, 3)\n",
    "        colors_rgb = colors[valid_mask]       # (N, 3)\n",
    "        \n",
    "        return points_3d, colors_rgb, trans_world, trans_norm_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee77e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_warper = WarperDebug(device=opts_base.device)\n",
    "\n",
    "num_frames = scene_data['frames_tensor'].shape[0]\n",
    "\n",
    "all_trans_norm_points = []\n",
    "all_trans_world_points = []\n",
    "all_world_points = []\n",
    "\n",
    "for i in tqdm(range(num_frames), desc=\"Processing frames\"):\n",
    "\n",
    "    trans_norm_points, trans_world, world_points = vis_warper.compute_transformed_points(\n",
    "        depth1=scene_data['depths'][i:i+1].to(vis_warper.device),\n",
    "        transformation1=scene_data['pose_source'][i:i+1].to(vis_warper.device),\n",
    "        transformation2=scene_data['pose_target'][i:i+1].to(vis_warper.device),\n",
    "        intrinsic1=scene_data['intrinsics'][i:i+1].to(vis_warper.device),\n",
    "        intrinsic2=None\n",
    "    )\n",
    "\n",
    "    all_trans_norm_points.append(trans_norm_points)\n",
    "    all_trans_world_points.append(trans_world)\n",
    "    all_world_points.append(world_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44aa32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trans_norm_points[0].shape, all_trans_world_points[0].shape, all_world_points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91801f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Create Viser Server (run once)\n",
    "import viser\n",
    "\n",
    "# Check if server already exists and stop it\n",
    "try:\n",
    "    if 'viser_server' in globals() and viser_server is not None:\n",
    "        print(\"Stopping existing server...\")\n",
    "        viser_server.stop()\n",
    "        del viser_server\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new server\n",
    "print(\"Creating new Viser server on port 8080...\")\n",
    "viser_server = viser.ViserServer(port=8080)\n",
    "print(\"Server started successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing\n",
    "try:\n",
    "    viser_server.scene.remove(\"/points\")\n",
    "    viser_server.scene.remove(\"/camera_source\")\n",
    "    viser_server.scene.remove(\"/camera_target\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Show all_trans_points1[0] in viser with camera frustums\n",
    "i = 25\n",
    "\n",
    "\n",
    "def extract_xyz_points(points_raw):\n",
    "    # points_raw: (1, h, w, 3, 1)\n",
    "\n",
    "    subsample_factor = 100\n",
    "\n",
    "    points_3d = points_raw[0, :, :, :, 0].cpu().numpy()  # (h, w, 3)\n",
    "    points_3d = points_3d.reshape(-1, 3)  # (N, 3)\n",
    "    \n",
    "    points_3d = points_3d[::subsample_factor]  # Subsample for visualization\n",
    "    return points_3d\n",
    "\n",
    "# Extract points and poses\n",
    "world_points_3d = extract_xyz_points(all_world_points[i])\n",
    "trans_world_points_3d = extract_xyz_points(all_trans_world_points[i])\n",
    "\n",
    "\n",
    "pose_source = scene_data['pose_source'][i:i+1].cpu().numpy()[0]  # (4, 4)\n",
    "pose_target = scene_data['pose_target'][i:i+1].cpu().numpy()[0]  # (4, 4)\n",
    "\n",
    "\n",
    "\n",
    "# Add 3D points\n",
    "viser_server.scene.add_point_cloud(\n",
    "    \"/points\",\n",
    "    points=world_points_3d,\n",
    "    colors=(0.0, 0.0, 0.0),  # Red points\n",
    "    point_size=0.05\n",
    ")\n",
    "\n",
    "# Add transformed points\n",
    "viser_server.scene.add_point_cloud(\n",
    "    \"/transformed_points\",\n",
    "    points=trans_world_points_3d,\n",
    "    colors=(1.0, 0.0, 0.0),  # Blue points\n",
    "    point_size=0.05\n",
    ")\n",
    "\n",
    "# show world axes\n",
    "viser_server.scene.add_frame(\"/world\", axes_length=0.5, position=(0, 0, 0), wxyz=(1, 0, 0, 0))\n",
    "\n",
    "# Add source camera (green)\n",
    "viser_server.scene.add_camera_frustum(\n",
    "    \"/camera_source\",\n",
    "    fov=60, aspect=16/9, scale=0.2,\n",
    "    position=pose_source[:3, 3],\n",
    "    wxyz=viser.transforms.SO3.from_matrix(pose_source[:3, :3]).wxyz,\n",
    "    color=(0.0, 1.0, 0.0)\n",
    ")\n",
    "\n",
    "# Add target camera (blue)\n",
    "viser_server.scene.add_camera_frustum(\n",
    "    \"/camera_target\", \n",
    "    fov=60, aspect=16/9, scale=0.2,\n",
    "    position=pose_target[:3, 3],\n",
    "    wxyz=viser.transforms.SO3.from_matrix(pose_target[:3, :3]).wxyz,\n",
    "    color=(0.0, 0.0, 1.0)\n",
    ")\n",
    "\n",
    "print(f\"Showing {len(world_points_3d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all target cameras\n",
    "\n",
    "for i in range(num_frames):\n",
    "    pose_target = scene_data['pose_target'][i:i+1].cpu().numpy()[0]  # (4, 4)\n",
    "    \n",
    "    # invert the pose_target\n",
    "    pose_target_inv = np.linalg.inv(pose_target)\n",
    "    \n",
    "    \n",
    "    viser_server.scene.add_camera_frustum(\n",
    "        f\"/camera_target_{i}\", \n",
    "        fov=60, aspect=16/9, scale=0.1,\n",
    "        position=pose_target_inv[:3, 3],\n",
    "        wxyz=viser.transforms.SO3.from_matrix(pose_target_inv[:3, :3]).wxyz,\n",
    "        color=(0.0, 0.0, 1.0)\n",
    "    )\n",
    "    \n",
    "    # remove all target cameras\n",
    "    try:\n",
    "        viser_server.scene.remove(f\"/camera_target_{i}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create warper for 3D point extraction\n",
    "print(\"Creating 3D point cloud from all frames...\")\n",
    "vis_warper = WarperDebug(device=opts_base.device)\n",
    "\n",
    "# Extract points from all frames\n",
    "all_points_3d = []\n",
    "all_colors_rgb = []\n",
    "all_trans_world = []\n",
    "all_trans_norm_points = []\n",
    "\n",
    "warped_images = []\n",
    "masks = []\n",
    "\n",
    "num_frames = scene_data['frames_tensor'].shape[0]\n",
    "for i in tqdm(range(num_frames), desc=\"Processing frames\"):\n",
    "    frame_data = {\n",
    "        'frame': scene_data['frames_tensor'][i:i+1],\n",
    "        'depth': scene_data['depths'][i:i+1], \n",
    "        'pose_source': scene_data['pose_source'][i:i+1],\n",
    "        'intrinsics': scene_data['intrinsics'][i:i+1],\n",
    "    }\n",
    "\n",
    "    points_3d_frame, colors_rgb_frame, trans_world_frame, trans_norm_points_frame = vis_warper.extract_3d_points_with_colors(\n",
    "        frame_data['frame'],\n",
    "        frame_data['depth'], \n",
    "        frame_data['pose_source'],\n",
    "        frame_data['intrinsics'],\n",
    "        subsample_step=20  # Increased for performance with multiple frames\n",
    "    )\n",
    "    \n",
    "    if points_3d_frame.shape[0] > 0:  # Only add if we have valid points\n",
    "        all_points_3d.append(points_3d_frame)\n",
    "        all_colors_rgb.append(colors_rgb_frame)\n",
    "        all_trans_world.append(trans_world_frame)\n",
    "        all_trans_norm_points.append(trans_norm_points_frame)\n",
    "\n",
    "    warped_frame2, mask2, warped_depth2, flow12 = vis_warper.forward_warp(\n",
    "        scene_data['frames_tensor'][i:i+1],\n",
    "        None,\n",
    "        scene_data['depths'][i:i+1],\n",
    "        scene_data['pose_source'][i:i+1],\n",
    "        scene_data['pose_target'][i:i+1],\n",
    "        scene_data['intrinsics'][i:i+1],\n",
    "        None,\n",
    "        opts_base.mask,\n",
    "        twice=False,\n",
    "    )\n",
    "    warped_images.append(warped_frame2)\n",
    "    masks.append(mask2)\n",
    "\n",
    "# Concatenate all points\n",
    "if all_points_3d:\n",
    "    points_3d = torch.cat(all_points_3d, dim=0)\n",
    "    colors_rgb = torch.cat(all_colors_rgb, dim=0)\n",
    "    trans_world = torch.cat(all_trans_world, dim=0)\n",
    "    trans_norm_points = torch.cat(all_trans_norm_points, dim=0)\n",
    "    print(f\"Generated {points_3d.shape[0]} 3D points from {len(all_points_3d)} frames\")\n",
    "else:\n",
    "    print(\"No valid 3D points extracted!\")\n",
    "    points_3d = None\n",
    "    colors_rgb = None\n",
    "\n",
    "print(f\"Camera trajectory: {scene_data['pose_target'].shape[0]} poses\")\n",
    "\n",
    "\n",
    "cond_video = (torch.cat(warped_images) + 1.0) / 2.0\n",
    "cond_masks = torch.cat(masks)\n",
    "\n",
    "cond_video = F.interpolate(\n",
    "    cond_video, size=opts_base.sample_size, mode='bilinear', align_corners=False\n",
    ")\n",
    "cond_masks = F.interpolate(cond_masks, size=opts_base.sample_size, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6d393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
