{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "def read_tartanair_extrinsic(extrinsic_path, side='left'):\n",
    "    data = []\n",
    "    camera_id = {'left': 0, 'right': 1}\n",
    "    with open(extrinsic_path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for lineid, line in enumerate(lines):\n",
    "        frame = int(lineid)\n",
    "        values = line.rstrip().split(' ')\n",
    "        assert len(values) == 7, 'Pose must be quaterion format -- 7 params, but {} got'.format(len(values))\n",
    "        pose = np.array([float(values[i]) for i in range(len(values))])\n",
    "        tx, ty, tz, qx, qy, qz, qw = pose\n",
    "        R = Rotation.from_quat((qx, qy, qz, qw)).as_matrix()\n",
    "        t = np.array([tx, ty, tz])\n",
    "        T = np.eye(4)\n",
    "        T[:3, :3] = R.transpose()\n",
    "        T[:3, 3] = -R.transpose().dot(t)\n",
    "        # ned(z-axis down) to z-axis forward\n",
    "        m_correct = np.zeros_like(T)\n",
    "        m_correct[0, 1] = 1\n",
    "        m_correct[1, 2] = 1\n",
    "        m_correct[2, 0] = 1\n",
    "        m_correct[3, 3] = 1\n",
    "\n",
    "        # m_correct\n",
    "        T = np.matmul(m_correct, T)\n",
    "        data.append(T)\n",
    "        lineid += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_tartanair_sequence(sequence_path, max_frames=50):\n",
    "    \"\"\"\n",
    "    Read TartanAir sequence data for debugging\n",
    "    \n",
    "    Args:\n",
    "        sequence_path: Path to sequence directory (e.g., /path/to/abandonedfactory/Easy/P001)\n",
    "        max_frames: Maximum number of frames to read\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'images', 'depths', 'poses', 'intrinsics'\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Find RGB images (left camera only)\n",
    "    image_dir = os.path.join(sequence_path, \"image_left\")\n",
    "    image_files = sorted(glob.glob(os.path.join(image_dir, \"*_left.png\")))[:max_frames]\n",
    "    print(f\"Found {len(image_files)} RGB images\")\n",
    "    if len(image_files) > 0:\n",
    "        print(f\"First image: {image_files[0]}\")\n",
    "        print(f\"Last image: {image_files[-1]}\")\n",
    "    \n",
    "    # 2. Find depth files (left camera only)\n",
    "    depth_dir = os.path.join(sequence_path, \"depth_left\")\n",
    "    depth_files = sorted(glob.glob(os.path.join(depth_dir, \"*_left_depth.npy\")))[:max_frames]\n",
    "    print(f\"Found {len(depth_files)} depth files\")\n",
    "    if len(depth_files) > 0:\n",
    "        print(f\"First depth: {depth_files[0]}\")\n",
    "        print(f\"Last depth: {depth_files[-1]}\")\n",
    "    \n",
    "    # 3. Find pose file (left camera)\n",
    "    pose_file = os.path.join(sequence_path, \"pose_left.txt\")\n",
    "    print(f\"Pose file exists: {os.path.exists(pose_file)}\")\n",
    "    print(f\"Pose file path: {pose_file}\")\n",
    "    \n",
    "    # 4. Load RGB images\n",
    "    images = []\n",
    "    for img_file in image_files:\n",
    "        img = np.array(Image.open(img_file))\n",
    "        images.append(img)\n",
    "        print(f\"Image {len(images)}: shape={img.shape}, dtype={img.dtype}\")\n",
    "        if len(images) == 1:  # Show info for first image only\n",
    "            print(f\"  Min={img.min()}, Max={img.max()}\")\n",
    "    \n",
    "    # 5. Load depth files\n",
    "    depths = []\n",
    "    for depth_file in depth_files:\n",
    "        depth = np.load(depth_file)\n",
    "        depths.append(depth)\n",
    "        print(f\"Depth {len(depths)}: shape={depth.shape}, dtype={depth.dtype}\")\n",
    "        if len(depths) == 1:  # Show info for first depth only\n",
    "            print(f\"  Min={depth.min():.3f}, Max={depth.max():.3f}\")\n",
    "    \n",
    "    # 6. Load poses\n",
    "    poses = []\n",
    "    pose_data = read_tartanair_extrinsic(pose_file, side='left')\n",
    "    \n",
    "    poses = pose_data[:max_frames]\n",
    "    \n",
    "    # print(len(pose_data), pose_data)\n",
    "    \n",
    "    # 7. TartanAir intrinsics (fixed for all sequences)\n",
    "    K = np.array([[320.0, 0, 320.0],\n",
    "                  [0, 320.0, 240.0],\n",
    "                  [0, 0, 1]], dtype=np.float32)\n",
    "    \n",
    "    print(f\"\\nLoaded:\")\n",
    "    print(f\"  {len(images)} RGB images\")\n",
    "    print(f\"  {len(depths)} depth maps\") \n",
    "    print(f\"  {len(poses)} poses\")\n",
    "    print(f\"  Intrinsics K:\\n{K}\")\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'depths': depths, \n",
    "        'poses': poses,\n",
    "        'intrinsics': K,\n",
    "        'image_files': image_files,\n",
    "        'depth_files': depth_files,\n",
    "        'pose_file': pose_file\n",
    "    }\n",
    "\n",
    "def visualize_data(data, frame_idx=0):\n",
    "    \"\"\"Visualize a single frame\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Show RGB image\n",
    "    axes[0].imshow(data['images'][frame_idx])\n",
    "    axes[0].set_title(f'RGB Frame {frame_idx}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show depth map\n",
    "    depth = data['depths'][frame_idx]\n",
    "    im = axes[1].imshow(depth, cmap='viridis')\n",
    "    axes[1].set_title(f'Depth Frame {frame_idx}')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print pose info\n",
    "    if frame_idx < len(data['poses']):\n",
    "        pose = data['poses'][frame_idx]\n",
    "        print(f\"Frame {frame_idx} pose:\")\n",
    "        print(f\"  Camera position: {pose[:3, 3]}\")\n",
    "        print(f\"  Camera rotation matrix:\\n{pose[:3, :3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual path\n",
    "sequence_path = \"/home/azhuravl/scratch/tartanair/abandonedfactory/Easy/P001\"\n",
    "\n",
    "# Load data\n",
    "data = read_tartanair_sequence(sequence_path, max_frames=50)\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"\\nData consistency check:\")\n",
    "print(f\"  Images: {len(data['images'])}\")\n",
    "print(f\"  Depths: {len(data['depths'])}\")  \n",
    "print(f\"  Poses: {len(data['poses'])}\")\n",
    "\n",
    "if len(data['images']) == len(data['depths']) == len(data['poses']):\n",
    "    print(\"✓ All data lengths match!\")\n",
    "else:\n",
    "    print(\"✗ Data length mismatch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "\n",
    "depths = data['depths']\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "K = data['intrinsics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists of numpy arrays to tensors\n",
    "depths = np.stack(depths, axis=0)\n",
    "images = np.stack(images, axis=0)\n",
    "poses = np.stack(poses, axis=0)\n",
    "K = np.stack(K, axis=0)\n",
    "\n",
    "\n",
    "print(f\"Depths shape: {depths.shape}, dtype: {depths.dtype}\")\n",
    "print(f\"Images shape: {images.shape}, dtype: {images.dtype}\")\n",
    "print(f\"Poses shape: {poses.shape}, dtype: {poses.dtype}\")\n",
    "print(f\"K shape: {K.shape}, dtype: {K.dtype}\")\n",
    "\n",
    "depths_tensor = torch.from_numpy(depths).unsqueeze(1)  # Convert to (N, 1, H, W)\n",
    "frames_tensor = torch.from_numpy(images).permute(0, 3, 1, 2)  # Convert to (N, C, H, W)\n",
    "poses_tensor = torch.from_numpy(poses)\n",
    "K_tensor = torch.from_numpy(K)\n",
    "\n",
    "# normalize to -1, 1\n",
    "frames_tensor = frames_tensor.float() / 127.5 - 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try TartanAir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work')\n",
    "\n",
    "import stereoanyvideo.datasets.video_datasets as video_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [\n",
    "    # 'abandonedfactory/Easy/P001',\n",
    "    # 'abandonedfactory/Easy/P005', \n",
    "    # 'office/Easy/P001',\n",
    "    'office/Easy/P002',\n",
    "    # 'office2/Easy/P001'\n",
    "]\n",
    "\n",
    "dataset_tartanair = video_datasets.TartanAirDataset(\n",
    "        aug_params=None,\n",
    "        root=\"/home/azhuravl/scratch/tartanair\",\n",
    "        split=\"train\",\n",
    "        sample_len=59,\n",
    "        only_first_n_samples=-1,\n",
    "        sampling_stride=3,          # Starting frame stride (default 3)\n",
    "        min_temporal_step=1,        # Minimum temporal step (default 1)  \n",
    "        max_temporal_step=2,        # Maximum temporal step (default 6)\n",
    "        train_sequences=train_sequences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['RTK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tartanair(data):\n",
    "    \"\"\"\n",
    "    Extract frames, depths, poses, and camera intrinsics from data object.\n",
    "    \n",
    "    Args:\n",
    "        data: Data object containing 'img', 'disp', and 'viewpoint'\n",
    "        baseline: Baseline for depth calculation (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        frames_tensor: [T, 3, H, W] in [-1, 1] range\n",
    "        depths: [T, 1, H, W] depth maps\n",
    "        poses_tensor: [T, 4, 4] camera poses\n",
    "        K_tensor: [3, 3] camera intrinsics\n",
    "    \"\"\"\n",
    "    # Convert to [-1, 1] range\n",
    "    frames_tensor = data['img'][:,0] / 127.5 - 1.0  # [T, 3, H, W]\n",
    "    depths_tensor = data['depth'][:,0:1,...]  # [T, 1, H, W]\n",
    "    poses_tensor = data['RTK'][0]\n",
    "    \n",
    "    K_tensor = data['RTK'][1]  # [3, 3]\n",
    "    \n",
    "    return frames_tensor, depths_tensor, poses_tensor, K_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor, depths_tensor, poses_tensor, K_tensor = extract_tartanair(data_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter')\n",
    "\n",
    "import models.utils as utils\n",
    "\n",
    "warper_old = utils.Warper(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "warped_images = []\n",
    "masks = []\n",
    "warped_depths = []\n",
    "\n",
    "for i in tqdm(range(10, frames_tensor.shape[0])):\n",
    "    warped_frame2, mask2, warped_depth2, flow12 = warper_old.forward_warp(\n",
    "        frame1=frames_tensor[i:i+1],\n",
    "        mask1=None,\n",
    "        depth1=depths_tensor[i:i+1],\n",
    "        transformation1=poses_tensor[i:i+1],\n",
    "        transformation2=poses_tensor[10:11],\n",
    "        intrinsic1=K_tensor.unsqueeze(0),\n",
    "        intrinsic2=K_tensor.unsqueeze(0),\n",
    "        mask=False,\n",
    "        twice=True,\n",
    "    )\n",
    "    # print(warped_frame2[0])\n",
    "    warped_images.append(warped_frame2)\n",
    "    masks.append(mask2)\n",
    "    warped_depths.append(warped_depth2)\n",
    "    \n",
    "    # print(warped_frame2.shape, mask2.shape, warped_depth2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot warped image j, mask j, warped depth j\n",
    "j = 0\n",
    "k = 30\n",
    "\n",
    "frame = frames_tensor[j].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "warped_image = warped_images[k][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "target_frame = frames_tensor[k].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('Source Frame')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(warped_image)\n",
    "plt.title('Warped Image to Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(target_frame)\n",
    "plt.title('Target Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(depths[10+j].cpu().permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "# plt.title('Warped Image to Frame {}'.format(j))\n",
    "# plt.axis('off')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.imshow((warped_depths[j][0].cpu().permute(1, 2, 0).numpy() + 1e-2), cmap='plasma')\n",
    "# plt.title('Warped Image to Frame {}'.format(j))\n",
    "# plt.axis('off')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(masks[j][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5, cmap='gray')\n",
    "# plt.title('Mask')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import save_video\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def make_dimensions_even(tensor):\n",
    "    \"\"\"Pad tensor to make height and width even numbers\"\"\"\n",
    "    _, h, w, c = tensor.shape\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        # Pad bottom and right if needed\n",
    "        tensor = torch.nn.functional.pad(tensor, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "cond_video = (torch.cat(warped_images) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "cond_video_padded = make_dimensions_even(\n",
    "    cond_video.permute(0, 2, 3, 1)\n",
    ")\n",
    "\n",
    "save_video(\n",
    "    cond_video_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/warped_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "# --- save inputs for visualization ---\n",
    "\n",
    "input_video_padded = make_dimensions_even(\n",
    "    (frames_tensor[10:].permute(0, 2, 3, 1) + 1.0) / 2.0\n",
    ")\n",
    "\n",
    "save_video(\n",
    "    input_video_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/input_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "warped_depths_tensor = torch.cat(warped_depths)\n",
    "# Apply before saving\n",
    "warped_depths_padded = make_dimensions_even(\n",
    "    (warped_depths_tensor.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / warped_depths_tensor.max()\n",
    ")\n",
    "save_video(\n",
    "    warped_depths_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/warped_depths.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "depths_padded = make_dimensions_even(\n",
    "    (depths_tensor.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / depths_tensor.max()\n",
    ")\n",
    "save_video(\n",
    "    depths_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/input_depths.mp4',\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "import utils_autoregressive as utils_ar\n",
    "from datetime import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/monkaa.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajcrafter = utils_ar.TrajCrafterAutoregressive(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# frames_tensor = (\n",
    "    # torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "# reverse this to get frames in numpy\n",
    "frames_np = ((frames_tensor.cpu().permute(0, 2, 3, 1).numpy() + 1.0) / 2.0).astype(np.float32)\n",
    "\n",
    "trajcrafter.prompt = trajcrafter.get_caption(opts, frames_np[opts.video_length // 2])\n",
    "print(trajcrafter.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, segment_dir = utils_ar.sample_diffusion(\n",
    "    trajcrafter,\n",
    "    frames_tensor[10:],\n",
    "    warped_images,\n",
    "    frames_tensor[:10],\n",
    "    masks,\n",
    "    opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun data_0 = dataset_tartanair[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
