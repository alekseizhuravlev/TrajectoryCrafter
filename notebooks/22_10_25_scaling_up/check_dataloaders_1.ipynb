{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work')\n",
    "\n",
    "import stereoanyvideo.datasets.video_datasets as video_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_driving = video_datasets.SequenceSceneFlowDatasetCamera(\n",
    "    aug_params=None,\n",
    "    root=\"/home/azhuravl/scratch/SceneFlow\",\n",
    "    dstype=\"frames_cleanpass\",\n",
    "    sample_len=59,\n",
    "    things_test=False,\n",
    "    add_things=False,\n",
    "    add_monkaa=False,\n",
    "    add_driving=True,\n",
    "    split=\"test\",\n",
    "    stride=5,\n",
    ")\n",
    "len(dataset_driving)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[0]\n",
    "data_1 = dataset_tartanair[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot 0th frame of the first sequence and 2nd sequence\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data_0['img'][50][0].permute(1, 2, 0).int().numpy())\n",
    "# plt.imshow(data_0['img'][0][0].permute(1, 2, 0).int().numpy())\n",
    "plt.title('Sequence 0, Frame 0')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(data_1['img'][0][0].permute(1, 2, 0).int().numpy())\n",
    "plt.title('Sequence 1, Frame 0')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch3d.utils.camera_conversions import opencv_from_cameras_projection\n",
    "\n",
    "# If you have a PyTorch3D camera from your output_tensor\n",
    "viewpoint = data_0[\"viewpoint\"][0][0]  # First frame, left camera\n",
    "\n",
    "# Convert to OpenCV format\n",
    "# opencv_params = video_datasets.pytorch3d_to_opencv_camera_general(viewpoint, (540, 960))\n",
    "\n",
    "R, t, K = opencv_from_cameras_projection(\n",
    "    viewpoint,\n",
    "    # image_size=torch.tensor([[540, 960]])\n",
    "    # use the actual image shape\n",
    "    image_size=torch.tensor([[data_0['img'].shape[-2], data_0['img'].shape[-1]]])\n",
    ")\n",
    "R = R[0]\n",
    "t = t[0]\n",
    "K = K[0]\n",
    "\n",
    "# Access the parameters\n",
    "# K = opencv_params['K']          # 3x3 intrinsic matrix\n",
    "# R = opencv_params['R']          # 3x3 rotation matrix\n",
    "# t = opencv_params['t']          # 3x1 translation vector\n",
    "# calculate depth from disparity using torch\n",
    "\n",
    "import torch\n",
    "disp = data_0['disp'][0][0]\n",
    "focal_length = K[0, 0]\n",
    "baseline = 1\n",
    "depth = -(focal_length * baseline) / disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rgb and disparity for the first frame\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(data_0['img'][40][0].permute(1, 2, 0).int().numpy())\n",
    "plt.title('RGB Frame 0')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(data_0['disp'][40][0].permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "plt.title('Disparity Frame 0')\n",
    "plt.axis('off')\n",
    "plt.colorbar(fraction=0.03, pad=0.04)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(1 / depth.permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "plt.title('Depth Frame 0')\n",
    "plt.axis('off')\n",
    "plt.colorbar(fraction=0.03, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VKITTI 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [\n",
    "    'abandonedfactory/Easy/P001',\n",
    "    # 'abandonedfactory/Easy/P005', \n",
    "    # 'office/Easy/P001',\n",
    "    # 'office/Easy/P002',\n",
    "    # 'office2/Easy/P001'\n",
    "]\n",
    "\n",
    "dataset_tartanair = video_datasets.TartanAirDataset(\n",
    "        aug_params=None,\n",
    "        root=\"/home/azhuravl/scratch/tartanair\",\n",
    "        split=\"train\",\n",
    "        sample_len=59,\n",
    "        only_first_n_samples=-1,\n",
    "        sampling_stride=3,          # Starting frame stride (default 3)\n",
    "        min_temporal_step=1,        # Minimum temporal step (default 1)  \n",
    "        max_temporal_step=2,        # Maximum temporal step (default 6)\n",
    "        train_sequences=train_sequences\n",
    "    )\n",
    "\n",
    "# dataset_vkitti = video_datasets.VKITTI2Dataset(\n",
    "#     aug_params=None,\n",
    "#     root=\"/home/azhuravl/scratch/vkitti2\",\n",
    "#     split=\"train\",\n",
    "#     sample_len=59,\n",
    "#     only_first_n_samples=-1,\n",
    "#     sampling_stride=8,          # Starting frame stride (default 3)\n",
    "#     min_temporal_step=1,        # Minimum temporal step (default 1)  \n",
    "#     max_temporal_step=1,        # Maximum temporal step (default 6)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(collect_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/15_10_25_depth')\n",
    "\n",
    "import collect_dataset\n",
    "\n",
    "frames_tensor, depths, poses_tensor, K_tensor = collect_dataset.extract_video_data(\n",
    "    data_0,\n",
    "    baseline=0.25,\n",
    "    image_size=frames_tensor.shape[-2:-1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_tensor = data_0['RTK']\n",
    "depths = -data_0['depth2disp_scale'][0] / data_0['disp'][:,0,...] \n",
    "# depth = depth2disp_scale / disparity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/azhuravl/work/TrajectoryCrafter')\n",
    "\n",
    "# import models.utils as utils\n",
    "\n",
    "# warper_old = utils.Warper(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "warped_images = []\n",
    "masks = []\n",
    "warped_depths = []\n",
    "\n",
    "# poses_tensor_inv = torch.inverse(poses_tensor)\n",
    "\n",
    "# flip x axis\n",
    "# poses_tensor[:, 0:3, 3] = -poses_tensor[:, 0:3, 3]\n",
    "# flip y axis\n",
    "# poses_tensor[:, 1:3, 3] = -poses_tensor[:, 1:3, 3]\n",
    "# flip z axis\n",
    "# poses_tensor[:, 2:3, 3] = -poses_tensor[:, 2:3, 3]\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(10, frames_tensor.shape[0])):\n",
    "    \n",
    "    # transformation_1 = torch.inverse(poses_tensor[i:i+1])\n",
    "    # transformation_2 = torch.inverse(poses_tensor[10:11])\n",
    "    \n",
    "    transformation_1 = poses_tensor[i:i+1].clone()\n",
    "    transformation_2 = poses_tensor[10:11].clone()\n",
    "    \n",
    "    # multiply translation by 100\n",
    "    # transformation_1[:, 0:3, 3] = transformation_1[:, 0:3, 3] * 100\n",
    "    # transformation_2[:, 0:3, 3] = transformation_2[:, 0:3, 3] * 100\n",
    "    \n",
    "    # transformation_1 = torch.inverse(transformation_1)\n",
    "    # transformation_2 = torch.inverse(transformation_2)\n",
    "    \n",
    "    # print(transformation_1)\n",
    "        \n",
    "    warped_frame2, mask2, warped_depth2, flow12 = warper_old.forward_warp(\n",
    "        frame1=frames_tensor[i:i+1],\n",
    "        mask1=None,\n",
    "        depth1=depths[i:i+1],\n",
    "        transformation1=transformation_1,\n",
    "        transformation2=transformation_2,\n",
    "        intrinsic1=K_tensor[i:i+1],\n",
    "        intrinsic2=K_tensor[i:i+1],\n",
    "        mask=False,\n",
    "        twice=False,\n",
    "    )\n",
    "    # print(warped_frame2[0])\n",
    "    warped_images.append(warped_frame2)\n",
    "    masks.append(mask2)\n",
    "    warped_depths.append(warped_depth2)\n",
    "    \n",
    "    # print(warped_frame2.shape, mask2.shape, warped_depth2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# plot warped image j, mask j, warped depth j\n",
    "j = 0\n",
    "k = 30\n",
    "\n",
    "frame = frames_tensor[j+10].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "warped_image = warped_images[k][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "target_frame = frames_tensor[k+10].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('Source Frame')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(warped_image)\n",
    "plt.title('Warped Image to Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(target_frame)\n",
    "plt.title('Target Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(depths[10+j].cpu().permute(1, 2, 0).numpy(), cmap='plasma')\n",
    "# plt.title('Warped Image to Frame {}'.format(j))\n",
    "# plt.axis('off')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.imshow((warped_depths[j][0].cpu().permute(1, 2, 0).numpy() + 1e-2), cmap='plasma')\n",
    "# plt.title('Warped Image to Frame {}'.format(j))\n",
    "# plt.axis('off')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(masks[j][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5, cmap='gray')\n",
    "# plt.title('Mask')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import save_video\n",
    "import os\n",
    "\n",
    "def make_dimensions_even(tensor):\n",
    "    \"\"\"Pad tensor to make height and width even numbers\"\"\"\n",
    "    _, h, w, c = tensor.shape\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        # Pad bottom and right if needed\n",
    "        tensor = torch.nn.functional.pad(tensor, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "cond_video = (torch.cat(warped_images) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "cond_video_padded = make_dimensions_even(\n",
    "    cond_video.permute(0, 2, 3, 1)\n",
    ")\n",
    "\n",
    "save_video(\n",
    "    cond_video_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/warped_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "# --- save inputs for visualization ---\n",
    "\n",
    "input_video_padded = make_dimensions_even(\n",
    "    (frames_tensor[10:].permute(0, 2, 3, 1) + 1.0) / 2.0\n",
    ")\n",
    "\n",
    "save_video(\n",
    "    input_video_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/input_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "warped_depths_tensor = torch.cat(warped_depths)\n",
    "# Apply before saving\n",
    "warped_depths_padded = make_dimensions_even(\n",
    "    (warped_depths_tensor.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / warped_depths_tensor.max()\n",
    ")\n",
    "save_video(\n",
    "    warped_depths_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/warped_depths.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "depths_padded = make_dimensions_even(\n",
    "    (depths.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / depths.max()\n",
    ")\n",
    "save_video(\n",
    "    depths_padded,\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/input_depths.mp4',\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "\n",
    "import warper_point_cloud\n",
    "warper = warper_point_cloud.GlobalPointCloudWarper(device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pc_list = []\n",
    "color_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j in tqdm(range(frames_tensor.shape[0])):\n",
    "        i = j\n",
    "        points, colors, _ = warper.create_pointcloud_from_image(\n",
    "            frames_tensor[i:i+1],\n",
    "            None,\n",
    "            depths[i:i+1],\n",
    "            # torch.inverse(poses_tensor[i:i+1].to('cuda') @ transform_mat).to('cuda'),\n",
    "            torch.inverse(poses_tensor[i:i+1].to('cuda')),\n",
    "            K_tensor[i:i+1],\n",
    "            1,\n",
    "        )\n",
    "        pc_list.append(points)\n",
    "        color_list.append(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viser\n",
    "import numpy as np\n",
    "\n",
    "# Start viser server\n",
    "server = viser.ViserServer()\n",
    "import os\n",
    "\n",
    "# print slurm node name\n",
    "\n",
    "node_name=os.environ.get('SLURM_NODELIST', 'localhost')\n",
    "\n",
    "print(f'http://{node_name}:{server.get_port()}')\n",
    "\n",
    "\n",
    "def add_points(\n",
    "    server,\n",
    "    points: np.ndarray,  # (N, 3)\n",
    "    colors: np.ndarray,  # (N, 3)\n",
    "    name: str,\n",
    "):\n",
    "    # ensure colors are in [0, 1]\n",
    "    if colors.min() < -0.1:\n",
    "        colors = (colors + 1.0) / 2.0\n",
    "    \n",
    "    server.scene.add_point_cloud(\n",
    "        name=name,\n",
    "        points=points,\n",
    "        colors=colors,\n",
    "        point_size=0.01\n",
    "    )\n",
    "\n",
    "def add_camera(\n",
    "    server,\n",
    "    pose: np.ndarray,  # (4, 4)\n",
    "    name: str,\n",
    "    color: tuple = (0.2, 0.8, 0.2),\n",
    "):\n",
    "    pose = np.linalg.inv(pose)\n",
    "    \n",
    "    position = pose[:3, 3]\n",
    "    rotation_matrix = pose[:3, :3]\n",
    "    \n",
    "    # Convert rotation to quaternion\n",
    "    wxyz = viser.transforms.SO3.from_matrix(rotation_matrix).wxyz\n",
    "    \n",
    "    server.scene.add_camera_frustum(\n",
    "        name,\n",
    "        fov=60, aspect=4/3, scale=0.1,\n",
    "        position=position, wxyz=wxyz,\n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "# Add this after creating your server and adding point clouds\n",
    "@server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    yaw_slider = client.gui.add_slider(\"Camera Yaw\", min=-180, max=180, step=1, initial_value=0)\n",
    "    \n",
    "    @yaw_slider.on_update\n",
    "    def _(_):\n",
    "        angle_rad = np.deg2rad(yaw_slider.value)\n",
    "        radius = 8.0\n",
    "        \n",
    "        position = np.array([\n",
    "            radius * np.sin(angle_rad),\n",
    "            0,  # height\n",
    "            radius * np.cos(angle_rad)\n",
    "        ])\n",
    "        \n",
    "        client.camera.position = position\n",
    "        client.camera.look_at = np.array([0, 0, 0])\n",
    "        client.camera.up_direction = np.array([0, -1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.scene.reset()\n",
    "\n",
    "# add original point cloud #0, and all cameras from first segment\n",
    "\n",
    "j_list = [0, 15, 30, 45]\n",
    "\n",
    "for j in j_list:    \n",
    "    add_points(\n",
    "        server,\n",
    "        pc_list[j].cpu().numpy(),\n",
    "        color_list[j].cpu().numpy(),\n",
    "        name=f'input_pc_{j}'\n",
    "    )\n",
    "    \n",
    "for j in range(frames_tensor.shape[0]):\n",
    "    add_camera(\n",
    "        server,\n",
    "        torch.inverse(poses_tensor[j]).cpu().numpy(),\n",
    "        name=f'input_cam_{j:02d}',\n",
    "        color=(0.2, 0.8, 0.2)\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "import utils_autoregressive as utils_ar\n",
    "from datetime import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/monkaa.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajcrafter = utils_ar.TrajCrafterAutoregressive(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# frames_tensor = (\n",
    "    # torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "# reverse this to get frames in numpy\n",
    "frames_np = ((frames_tensor.cpu().permute(0, 2, 3, 1).numpy() + 1.0) / 2.0).astype(np.float32)\n",
    "\n",
    "trajcrafter.prompt = trajcrafter.get_caption(opts, frames_np[opts.video_length // 2])\n",
    "print(trajcrafter.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, segment_dir = utils_ar.sample_diffusion(\n",
    "    trajcrafter,\n",
    "    frames_tensor[10:],\n",
    "    warped_images,\n",
    "    frames_tensor[:10],\n",
    "    masks,\n",
    "    opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TartanAir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [\n",
    "    'abandonedfactory/Easy/P001',\n",
    "    # 'abandonedfactory/Easy/P005', \n",
    "    # 'office/Easy/P001',\n",
    "    # 'office/Easy/P002',\n",
    "    # 'office2/Easy/P001'\n",
    "]\n",
    "\n",
    "dataset_tartanair = video_datasets.TartanAirDataset(\n",
    "        aug_params=None,\n",
    "        root=\"/home/azhuravl/scratch/tartanair\",\n",
    "        split=\"train\",\n",
    "        sample_len=59,\n",
    "        only_first_n_samples=-1,\n",
    "        sampling_stride=3,          # Starting frame stride (default 3)\n",
    "        min_temporal_step=1,        # Minimum temporal step (default 1)  \n",
    "        max_temporal_step=2,        # Maximum temporal step (default 6)\n",
    "        train_sequences=train_sequences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[0]\n",
    "data_1 = dataset_tartanair[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix TartanAir cameras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
