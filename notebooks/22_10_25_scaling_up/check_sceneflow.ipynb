{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work')\n",
    "\n",
    "import stereoanyvideo.datasets.video_datasets_custom as video_datasets_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_driving = video_datasets_custom.SequenceSceneFlowDatasetCamera(\n",
    "    aug_params=None,\n",
    "    root=\"/home/azhuravl/scratch/SceneFlow\",\n",
    "    dstype=\"frames_cleanpass\",\n",
    "    sample_len=59,\n",
    "    things_test=False,\n",
    "    add_things=False,\n",
    "    add_monkaa=False,\n",
    "    add_driving=True,\n",
    "    split=\"test\",\n",
    "    stride=5,\n",
    ")\n",
    "len(dataset_driving)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_driving[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/15_10_25_depth')\n",
    "\n",
    "import collect_dataset\n",
    "\n",
    "frames_tensor, depths, poses_tensor, K_tensor = collect_dataset.extract_video_data(\n",
    "    data_0,\n",
    "    # baseline=0.532725,\n",
    "    # image_size=data_0['metadata'][0][0][1],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter')\n",
    "\n",
    "import models.utils as utils\n",
    "\n",
    "warper_old = utils.Warper(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "warped_images = []\n",
    "masks = []\n",
    "warped_depths = []\n",
    "\n",
    "warped_images_once = []\n",
    "\n",
    "for i in tqdm(range(10, frames_tensor.shape[0])):\n",
    "    \n",
    "    transformation_1 = poses_tensor[i:i+1].clone()\n",
    "    transformation_2 = poses_tensor[10:11].clone()\n",
    "    \n",
    "    warped_frame2, mask2, warped_depth2, flow12 = warper_old.forward_warp(\n",
    "        frame1=frames_tensor[i:i+1],\n",
    "        mask1=None,\n",
    "        depth1=depths[i:i+1],\n",
    "        transformation1=transformation_1,\n",
    "        transformation2=transformation_2,\n",
    "        intrinsic1=K_tensor[0].unsqueeze(0),\n",
    "        intrinsic2=K_tensor[0].unsqueeze(0),\n",
    "        mask=False,\n",
    "        twice=True,\n",
    "    )\n",
    "    # depth returned is incorrect, multiply input depth by mask\n",
    "    warped_depth2 = depths[i:i+1] * mask2\n",
    "\n",
    "    warped_frame_once, _, _, _ = warper_old.forward_warp(\n",
    "        frame1=frames_tensor[i:i+1],\n",
    "        mask1=None,\n",
    "        depth1=depths[i:i+1],\n",
    "        transformation1=transformation_1,\n",
    "        transformation2=transformation_2,\n",
    "        intrinsic1=K_tensor[0].unsqueeze(0),\n",
    "        intrinsic2=K_tensor[0].unsqueeze(0),\n",
    "        mask=False,\n",
    "        twice=False,\n",
    "    )\n",
    "    \n",
    "    warped_images.append(warped_frame2)\n",
    "    masks.append(mask2)\n",
    "    warped_depths.append(warped_depth2)\n",
    "    \n",
    "    warped_images_once.append(warped_frame_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# plot warped image j, mask j, warped depth j\n",
    "j = 0\n",
    "k = 30\n",
    "\n",
    "frame = frames_tensor[j+10].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "warped_image = warped_images[k][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "target_frame = frames_tensor[k+10].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('Source Frame')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(warped_image)\n",
    "plt.title('Warped Image to Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(target_frame)\n",
    "plt.title('Target Frame {}'.format(j))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_resize_strategy(original_height, original_width, target_height=384, target_width=672):\n",
    "    \"\"\"Determine the best resize strategy based on aspect ratios\"\"\"\n",
    "    original_aspect = original_width / original_height\n",
    "    target_aspect = target_width / target_height  # 672/384 = 1.75\n",
    "    \n",
    "    if original_aspect > target_aspect * 1.2:\n",
    "        return \"crop_width\"\n",
    "    # If original is much taller, crop height  \n",
    "    elif original_aspect < target_aspect * 0.8:\n",
    "        return \"crop_height\"\n",
    "    else:\n",
    "        # For remaining cases, just use resize (may cause some distortion)\n",
    "        return \"resize\"\n",
    "\n",
    "\n",
    "def smart_video_resize(video, target_height=384, target_width=672, interpolation_mode='bilinear'):\n",
    "    \"\"\"\n",
    "    Smart resize for videos with various aspect ratios\n",
    "    \n",
    "    Args:\n",
    "        video: tensor of shape [T, C, H, W]\n",
    "        target_height: target height (default 384)\n",
    "        target_width: target width (default 672)\n",
    "        interpolation_mode: 'bilinear' for RGB videos, 'nearest' for masks\n",
    "        \n",
    "    Returns:\n",
    "        video: tensor of shape [T, C, target_height, target_width]\n",
    "    \"\"\"\n",
    "    T, C, H, W = video.shape\n",
    "    \n",
    "    strategy = get_resize_strategy(H, W, target_height, target_width)\n",
    "    \n",
    "    print(f\"resize strategy: '{strategy}' for original size ({H}, {W}), aspect ratio {W/H:.2f}, target ratio {target_width/target_height:.2f}\")\n",
    "    \n",
    "    if strategy == \"resize\":\n",
    "        # Simple resize when aspect ratios are similar or as fallback\n",
    "        video = F.interpolate(\n",
    "            video, \n",
    "            size=(target_height, target_width), \n",
    "            mode=interpolation_mode, \n",
    "            align_corners=False if interpolation_mode == 'bilinear' else None\n",
    "        )\n",
    "        \n",
    "    elif strategy == \"crop_width\":\n",
    "        # Video is too wide - crop width first, then resize\n",
    "        target_aspect = target_width / target_height\n",
    "        new_width = int(H * target_aspect)\n",
    "        \n",
    "        # Center crop width\n",
    "        start_w = (W - new_width) // 2\n",
    "        video = video[:, :, :, start_w:start_w + new_width]\n",
    "        \n",
    "        # Then resize to target\n",
    "        video = F.interpolate(\n",
    "            video,\n",
    "            size=(target_height, target_width),\n",
    "            mode=interpolation_mode,\n",
    "            align_corners=False if interpolation_mode == 'bilinear' else None\n",
    "        )\n",
    "        \n",
    "    elif strategy == \"crop_height\":\n",
    "        # Video is too tall - crop height first, then resize\n",
    "        target_aspect = target_width / target_height\n",
    "        new_height = int(W / target_aspect)\n",
    "        \n",
    "        # Center crop height\n",
    "        start_h = (H - new_height) // 2\n",
    "        video = video[:, :, start_h:start_h + new_height, :]\n",
    "        \n",
    "        # Then resize to target\n",
    "        video = F.interpolate(\n",
    "            video,\n",
    "            size=(target_height, target_width),\n",
    "            mode=interpolation_mode,\n",
    "            align_corners=False if interpolation_mode == 'bilinear' else None\n",
    "        )\n",
    "    \n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import save_video\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_dimensions_even(tensor):\n",
    "    \"\"\"Pad tensor to make height and width even numbers\"\"\"\n",
    "    _, h, w, c = tensor.shape\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        # Pad bottom and right if needed\n",
    "        tensor = torch.nn.functional.pad(tensor, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "def apply_colormap_to_depth(depth_tensor, colormap='viridis', inverse=True):\n",
    "    \"\"\"Apply colormap to depth tensor for better visualization\"\"\"\n",
    "    # Create mask for zero values\n",
    "    zero_mask = (depth_tensor == 0)\n",
    "    \n",
    "    if inverse:\n",
    "        # Compute inverse depth, avoid division by zero\n",
    "        depth_processed = torch.where(depth_tensor > 0, 1.0 / depth_tensor, torch.zeros_like(depth_tensor))\n",
    "    else:\n",
    "        depth_processed = depth_tensor\n",
    "    \n",
    "    # Normalize non-zero values to [0, 1]\n",
    "    if depth_processed[~zero_mask].numel() > 0:\n",
    "        depth_norm = depth_processed / depth_processed[~zero_mask].max()\n",
    "    else:\n",
    "        depth_norm = depth_processed\n",
    "    \n",
    "    # Convert to numpy and apply colormap\n",
    "    depth_np = depth_norm.cpu().numpy()\n",
    "    colormap_func = matplotlib.colormaps.get_cmap(colormap)\n",
    "    depth_colored = colormap_func(depth_np)  # Returns RGBA\n",
    "    \n",
    "    # Convert back to tensor, drop alpha channel\n",
    "    depth_colored_tensor = torch.from_numpy(depth_colored[..., :3]).to(depth_tensor.device)\n",
    "    \n",
    "    # Set zero depth areas to black\n",
    "    zero_mask_expanded = zero_mask.unsqueeze(-1).expand_as(depth_colored_tensor)\n",
    "    depth_colored_tensor[zero_mask_expanded] = 0.0  # Black for zero depth\n",
    "    \n",
    "    return depth_colored_tensor\n",
    "\n",
    "\n",
    "\n",
    "save_dir = '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/driving_resized'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "### Save videos\n",
    "\n",
    "# for saving: [T, H, W, C] in [0,1]\n",
    "# for diffusion: [C, T, H, W] in [0,1]\n",
    "\n",
    "\n",
    "# input and ref videos\n",
    "frames_tensor_resized = (frames_tensor + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "frames_tensor_resized = smart_video_resize(frames_tensor_resized)\n",
    "save_video(\n",
    "    frames_tensor_resized[10:].permute(0, 2, 3, 1),\n",
    "    f'{save_dir}/input_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "save_video(\n",
    "    frames_tensor_resized[:10].permute(0, 2, 3, 1),\n",
    "    f'{save_dir}/ref_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "# masks\n",
    "masks_tensor = torch.cat(masks)\n",
    "masks_tensor_resized = smart_video_resize(masks_tensor, interpolation_mode='nearest')\n",
    "save_video(\n",
    "    masks_tensor_resized.permute(0, 2, 3, 1).repeat(1, 1, 1, 3),\n",
    "    f'{save_dir}/masks.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "# twice warped video\n",
    "# cond_video_twice = (torch.cat(warped_images) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "# cond_video_twice_resized = smart_video_resize(\n",
    "#     cond_video_twice,\n",
    "#     interpolation_mode='nearest'\n",
    "#     )\n",
    "cond_video_twice_resized = frames_tensor_resized[10:] * masks_tensor_resized\n",
    "save_video(\n",
    "    cond_video_twice_resized.permute(0, 2, 3, 1),\n",
    "    f'{save_dir}/warped_video_twice.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "# once warped\n",
    "cond_video_once = (torch.cat(warped_images_once) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "cond_video_once_resized = smart_video_resize(\n",
    "    cond_video_once,\n",
    "    interpolation_mode='nearest'\n",
    ")\n",
    "save_video(\n",
    "    cond_video_once_resized.permute(0, 2, 3, 1),\n",
    "    f'{save_dir}/warped_video_once.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "\n",
    "### Save depths\n",
    "\n",
    "# input and ref depths\n",
    "depths_resized = smart_video_resize(depths)\n",
    "depths_colored = apply_colormap_to_depth(depths_resized.squeeze(1), inverse=True)\n",
    "save_video(\n",
    "    depths_colored[10:],\n",
    "    f'{save_dir}/input_depths.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "save_video(\n",
    "    depths_colored[:10],\n",
    "    f'{save_dir}/ref_depths.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "\n",
    "# warped depths\n",
    "# warped_depths_tensor = torch.cat(warped_depths)\n",
    "# warped_depths_tensor_resized = smart_video_resize(\n",
    "#     warped_depths_tensor,\n",
    "#     interpolation_mode='nearest'\n",
    "# )\n",
    "warped_depths_tensor_resized = depths_resized[10:] * masks_tensor_resized\n",
    "warped_depths_colored = apply_colormap_to_depth(warped_depths_tensor_resized.squeeze(1), inverse=True)\n",
    "save_video(\n",
    "    warped_depths_colored,\n",
    "    f'{save_dir}/warped_depths.mp4',\n",
    "    fps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_colored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "\n",
    "# extrinsics\n",
    "with open(f\"{save_dir}/extrinsics.json\", \"w\") as f:\n",
    "    json.dump(poses_tensor.cpu().tolist(), f, indent=2)\n",
    "\n",
    "# intrinsics\n",
    "with open(f\"{save_dir}/intrinsics.json\", \"w\") as f:\n",
    "    json.dump(K_tensor[0].cpu().tolist(), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frames_np = ((frames_tensor.cpu().permute(0, 2, 3, 1).numpy() + 1.0) / 2.0).astype(np.float32)\n",
    "\n",
    "caption = trajcrafter.get_caption(opts, frames_np[opts.video_length // 2])\n",
    "with open(f\"{save_dir}/caption.txt\", \"w\") as f:\n",
    "    f.write(caption)\n",
    "print(\"Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths.min(), depths.max(), warped_depths_tensor.min(), warped_depths_tensor.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save:\n",
    "\n",
    "### 1,000 for visualization\n",
    "- 49 input RGB + 10 ref\n",
    "- 49 warped RGB 2x\n",
    "- 49 warped RGB 1x\n",
    "- 49 input depth + 10 ref\n",
    "- 49 warped depth\n",
    "- 49 masks\n",
    "- caption\n",
    "- camera extr\n",
    "- camera intr\n",
    "\n",
    "\n",
    "### 10,000 for training\n",
    "- 49 GT RGB latents\n",
    "- 10 ref latents\n",
    "- 49 warped RGB 2x latents\n",
    "- 49 masks\n",
    "- caption latents\n",
    "\n",
    "- 49 GT depth latents\n",
    "- 10 ref depth latents\n",
    "- 49 warped depth latents\n",
    "\n",
    "\n",
    "### TODO\n",
    "- ++ implement saving for visualization, 1 subdir per sample\n",
    "- ++ crop or interpolate videos to fixed size\n",
    "- ++ rename resized variables\n",
    "- encode training data to latents\n",
    "- check them by running inference\n",
    "- implement saving for training, to a tar directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "def encode_inputs_to_latents(\n",
    "    pipeline,\n",
    "    video=None,\n",
    "    reference=None,\n",
    "    mask_video=None,\n",
    "    masked_video_latents=None,\n",
    "    prompt=None,\n",
    "    negative_prompt=None,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    device=\"cuda\",\n",
    "    batch_size=1,\n",
    "    noise_aug_strength=0.0563,\n",
    "    max_sequence_length=226,\n",
    "    do_classifier_free_guidance=True,\n",
    "    # New parameters for training\n",
    "    ground_truth_video=None,  # GT video for training\n",
    "    encode_for_training=False  # Flag to indicate training vs inference\n",
    "):\n",
    "    \"\"\"\n",
    "    Encode all inputs (video, reference, mask, prompts) to latents for training dataset preparation.\n",
    "    \n",
    "    Args:\n",
    "        video: Conditioning video (warped/masked video for inference)\n",
    "        ground_truth_video: Ground truth video for training targets\n",
    "        encode_for_training: If True, encodes GT video as training targets\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Encode text prompts (same for both training and inference)\n",
    "        if prompt is not None:\n",
    "            prompt_embeds, negative_prompt_embeds = pipeline.encode_prompt(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "                num_videos_per_prompt=1,\n",
    "                prompt_embeds=None,\n",
    "                negative_prompt_embeds=None,\n",
    "                max_sequence_length=max_sequence_length,\n",
    "                device=device,\n",
    "            )\n",
    "            \n",
    "            results['prompt_embeds'] = prompt_embeds.cpu()\n",
    "            if negative_prompt_embeds is not None:\n",
    "                results['negative_prompt_embeds'] = negative_prompt_embeds.cpu()\n",
    "        \n",
    "        # 2. Process reference video (same for both training and inference)\n",
    "        if reference is not None:\n",
    "            ref_length = reference.shape[2]\n",
    "            ref_video = pipeline.image_processor.preprocess(\n",
    "                rearrange(reference, \"b c f h w -> (b f) c h w\"), \n",
    "                height=height, \n",
    "                width=width\n",
    "            )\n",
    "            ref_video = rearrange(ref_video, \"(b f) c h w -> b c f h w\", f=ref_length)\n",
    "            ref_video = ref_video.to(device=device, dtype=pipeline.vae.dtype)\n",
    "            \n",
    "            # Encode reference video\n",
    "            bs = 1\n",
    "            new_ref_video = []\n",
    "            for i in range(0, ref_video.shape[0], bs):\n",
    "                video_bs = ref_video[i : i + bs]\n",
    "                video_bs = pipeline.vae.encode(video_bs)[0]\n",
    "                video_bs = video_bs.sample()\n",
    "                new_ref_video.append(video_bs)\n",
    "            new_ref_video = torch.cat(new_ref_video, dim=0)\n",
    "            new_ref_video = new_ref_video * pipeline.vae.config.scaling_factor\n",
    "            ref_latents = new_ref_video.repeat(batch_size // new_ref_video.shape[0], 1, 1, 1, 1)\n",
    "            \n",
    "            # Rearrange ONLY for final storage\n",
    "            ref_latents_final = rearrange(ref_latents, \"b c f h w -> b f c h w\")\n",
    "            results['ref_latents'] = ref_latents_final.cpu()\n",
    "        \n",
    "        # 3. Encode ground truth video for training targets\n",
    "        if encode_for_training and ground_truth_video is not None:\n",
    "            gt_video_length = ground_truth_video.shape[2]\n",
    "            gt_processed = pipeline.image_processor.preprocess(\n",
    "                rearrange(ground_truth_video, \"b c f h w -> (b f) c h w\"), \n",
    "                height=height, \n",
    "                width=width\n",
    "            )\n",
    "            gt_processed = rearrange(gt_processed, \"(b f) c h w -> b c f h w\", f=gt_video_length)\n",
    "            gt_processed = gt_processed.to(device=device, dtype=pipeline.vae.dtype)\n",
    "            \n",
    "            # Encode ground truth video\n",
    "            bs = 1\n",
    "            new_gt_video = []\n",
    "            for i in range(0, gt_processed.shape[0], bs):\n",
    "                video_bs = gt_processed[i : i + bs]\n",
    "                video_bs = pipeline.vae.encode(video_bs)[0]\n",
    "                video_bs = video_bs.sample()\n",
    "                new_gt_video.append(video_bs)\n",
    "            gt_encoded = torch.cat(new_gt_video, dim=0)\n",
    "            gt_encoded = gt_encoded * pipeline.vae.config.scaling_factor\n",
    "            \n",
    "            # Store GT latents for training (in transformer format)\n",
    "            gt_latents_final = rearrange(gt_encoded, \"b c f h w -> b f c h w\")\n",
    "            results['gt_video_latents'] = gt_latents_final.cpu()\n",
    "        \n",
    "        # 4. Process conditioning video (if provided)\n",
    "        init_video = None\n",
    "        video_latents_bcfhw = None  # Keep in original format for mask processing\n",
    "        \n",
    "        if video is not None:\n",
    "            video_length = video.shape[2]\n",
    "            init_video = pipeline.image_processor.preprocess(\n",
    "                rearrange(video, \"b c f h w -> (b f) c h w\"), \n",
    "                height=height, \n",
    "                width=width\n",
    "            )\n",
    "            init_video = init_video.to(dtype=torch.float32)\n",
    "            init_video = rearrange(init_video, \"(b f) c h w -> b c f h w\", f=video_length)\n",
    "            init_video = init_video.to(device=device, dtype=pipeline.vae.dtype)\n",
    "            \n",
    "            # Encode conditioning video\n",
    "            bs = 1\n",
    "            new_video = []\n",
    "            for i in range(0, init_video.shape[0], bs):\n",
    "                video_bs = init_video[i : i + bs]\n",
    "                video_bs = pipeline.vae.encode(video_bs)[0]\n",
    "                video_bs = video_bs.sample()\n",
    "                new_video.append(video_bs)\n",
    "            video_encoded = torch.cat(new_video, dim=0)\n",
    "            video_encoded = video_encoded * pipeline.vae.config.scaling_factor\n",
    "            \n",
    "            # Keep in [b, c, f, h, w] format for mask processing\n",
    "            video_latents_bcfhw = video_encoded\n",
    "            \n",
    "            # Rearrange ONLY for final storage\n",
    "            video_latents_final = rearrange(video_encoded, \"b c f h w -> b f c h w\")\n",
    "            results['cond_video_latents'] = video_latents_final.cpu()  # Renamed for clarity\n",
    "        \n",
    "        # 5. Get model configuration\n",
    "        num_channels_transformer = pipeline.transformer.config.in_channels\n",
    "        num_channels_latents = pipeline.vae.config.latent_channels\n",
    "        \n",
    "        # 6. Process mask video (same as before but use video_latents_bcfhw or gt if available)\n",
    "        reference_latents_bcfhw = video_latents_bcfhw\n",
    "        if encode_for_training and ground_truth_video is not None:\n",
    "            # For training, use GT video shape as reference\n",
    "            reference_latents_bcfhw = rearrange(gt_encoded, \"b f c h w -> b c f h w\")\n",
    "        \n",
    "        if mask_video is not None and reference_latents_bcfhw is not None:\n",
    "            video_length = mask_video.shape[2]\n",
    "            \n",
    "            if (mask_video == 255).all():\n",
    "                # All mask case\n",
    "                mask_latents = torch.zeros_like(reference_latents_bcfhw)[:, :, :1]\n",
    "                masked_video_latents = torch.zeros_like(reference_latents_bcfhw)\n",
    "                \n",
    "                results['mask_latents'] = rearrange(mask_latents, \"b c f h w -> b f c h w\").cpu()\n",
    "                results['masked_video_latents'] = rearrange(masked_video_latents, \"b c f h w -> b f c h w\").cpu()\n",
    "                results['mask'] = None\n",
    "            else:\n",
    "                # Process mask condition\n",
    "                mask_condition = pipeline.mask_processor.preprocess(\n",
    "                    rearrange(mask_video, \"b c f h w -> (b f) c h w\"),\n",
    "                    height=height,\n",
    "                    width=width,\n",
    "                )\n",
    "                mask_condition = mask_condition.to(dtype=torch.float32)\n",
    "                mask_condition = rearrange(mask_condition, \"(b f) c h w -> b c f h w\", f=video_length)\n",
    "                \n",
    "                if num_channels_transformer != num_channels_latents:\n",
    "                    # Inpainting model case\n",
    "                    mask_condition_tile = torch.tile(mask_condition, [1, 3, 1, 1, 1])\n",
    "                    \n",
    "                    # Create masked video (use init_video as base)\n",
    "                    if masked_video_latents is None and init_video is not None:\n",
    "                        masked_video = init_video  # Use the conditioning video\n",
    "                    else:\n",
    "                        masked_video = masked_video_latents or init_video\n",
    "                    \n",
    "                    # Encode masked video using prepare_mask_latents\n",
    "                    _, masked_video_latents = pipeline.prepare_mask_latents(\n",
    "                        None,\n",
    "                        masked_video,\n",
    "                        batch_size,\n",
    "                        height,\n",
    "                        width,\n",
    "                        pipeline.vae.dtype,\n",
    "                        device,\n",
    "                        None,  # generator\n",
    "                        do_classifier_free_guidance=False,\n",
    "                        noise_aug_strength=noise_aug_strength,\n",
    "                    )\n",
    "                    \n",
    "                    # Resize mask to latent size\n",
    "                    mask_latents = resize_mask(1 - mask_condition, masked_video_latents)\n",
    "                    mask_latents = mask_latents * pipeline.vae.config.scaling_factor\n",
    "                    \n",
    "                    # FIX: Ensure mask_latents matches the dtype of other latents\n",
    "                    mask_latents = mask_latents.to(dtype=masked_video_latents.dtype)\n",
    "\n",
    "                    \n",
    "                    # Create mask at latent resolution\n",
    "                    mask = torch.tile(mask_condition, [1, num_channels_latents, 1, 1, 1])\n",
    "                    mask = F.interpolate(\n",
    "                        mask,\n",
    "                        size=reference_latents_bcfhw.size()[-3:],\n",
    "                        mode='trilinear', \n",
    "                        align_corners=True\n",
    "                    )\n",
    "                    \n",
    "                    # FIX: Ensure mask matches the dtype of other latents  \n",
    "                    mask = mask.to(dtype=reference_latents_bcfhw.dtype)\n",
    "\n",
    "                    \n",
    "                    # Rearrange for final storage\n",
    "                    mask_input = rearrange(mask_latents, \"b c f h w -> b f c h w\")\n",
    "                    masked_video_latents_input = rearrange(masked_video_latents, \"b c f h w -> b f c h w\")\n",
    "                    mask_final = rearrange(mask, \"b c f h w -> b f c h w\")\n",
    "                    \n",
    "                    results['mask_latents'] = mask_input.cpu()\n",
    "                    results['masked_video_latents'] = masked_video_latents_input.cpu()\n",
    "                    results['mask'] = mask_final.cpu()\n",
    "                else:\n",
    "                    # Non-inpainting model case\n",
    "                    mask = torch.tile(mask_condition, [1, num_channels_latents, 1, 1, 1])\n",
    "                    mask = F.interpolate(\n",
    "                        mask,\n",
    "                        size=reference_latents_bcfhw.size()[-3:],\n",
    "                        mode='trilinear',\n",
    "                        align_corners=True\n",
    "                    )\n",
    "                    mask_final = rearrange(mask, \"b c f h w -> b f c h w\")\n",
    "                    results['mask'] = mask_final.cpu()\n",
    "                    results['mask_latents'] = None\n",
    "                    results['masked_video_latents'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def resize_mask(mask, latent, process_first_frame_only=True):\n",
    "    \"\"\"Helper function from the pipeline - resize mask to match latent dimensions\"\"\"\n",
    "    latent_size = latent.size()\n",
    "    batch_size, channels, num_frames, height, width = mask.shape\n",
    "\n",
    "    if process_first_frame_only:\n",
    "        target_size = list(latent_size[2:])\n",
    "        target_size[0] = 1\n",
    "        first_frame_resized = F.interpolate(\n",
    "            mask[:, :, 0:1, :, :],\n",
    "            size=target_size,\n",
    "            mode='trilinear',\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        target_size = list(latent_size[2:])\n",
    "        target_size[0] = target_size[0] - 1\n",
    "        if target_size[0] != 0:\n",
    "            remaining_frames_resized = F.interpolate(\n",
    "                mask[:, :, 1:, :, :],\n",
    "                size=target_size,\n",
    "                mode='trilinear',\n",
    "                align_corners=False,\n",
    "            )\n",
    "            resized_mask = torch.cat(\n",
    "                [first_frame_resized, remaining_frames_resized], dim=2\n",
    "            )\n",
    "        else:\n",
    "            resized_mask = first_frame_resized\n",
    "    else:\n",
    "        target_size = list(latent_size[2:])\n",
    "        resized_mask = F.interpolate(\n",
    "            mask, size=target_size, mode='trilinear', align_corners=False\n",
    "        )\n",
    "    return resized_mask\n",
    "\n",
    "\n",
    "def prepare_encoded_inputs_for_inference(encoded_data, do_classifier_free_guidance=True):\n",
    "    \"\"\"\n",
    "    Helper function to prepare pre-encoded inputs for inference.\n",
    "    \"\"\"\n",
    "    prepared_inputs = {}\n",
    "    \n",
    "    # Prepare prompt embeddings with CFG\n",
    "    if 'prompt_embeds' in encoded_data:\n",
    "        prompt_embeds = encoded_data['prompt_embeds']\n",
    "        if do_classifier_free_guidance and 'negative_prompt_embeds' in encoded_data:\n",
    "            negative_prompt_embeds = encoded_data['negative_prompt_embeds']\n",
    "            # Concatenate for CFG as done in the pipeline\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "        prepared_inputs['prompt_embeds'] = prompt_embeds\n",
    "    \n",
    "    # Prepare reference latents with CFG\n",
    "    if 'ref_latents' in encoded_data:\n",
    "        ref_latents = encoded_data['ref_latents']\n",
    "        if do_classifier_free_guidance:\n",
    "            ref_input = torch.cat([ref_latents] * 2)\n",
    "        else:\n",
    "            ref_input = ref_latents\n",
    "        prepared_inputs['ref_input'] = ref_input\n",
    "    \n",
    "    # Prepare inpaint latents with CFG\n",
    "    if 'mask_latents' in encoded_data and 'masked_video_latents' in encoded_data:\n",
    "        mask_latents = encoded_data['mask_latents']\n",
    "        masked_video_latents = encoded_data['masked_video_latents']\n",
    "        \n",
    "        if mask_latents is not None and masked_video_latents is not None:\n",
    "            if do_classifier_free_guidance:\n",
    "                mask_input = torch.cat([mask_latents] * 2)\n",
    "                masked_video_latents_input = torch.cat([masked_video_latents] * 2)\n",
    "            else:\n",
    "                mask_input = mask_latents\n",
    "                masked_video_latents_input = masked_video_latents\n",
    "            \n",
    "            # Channel concatenation for inpainting\n",
    "            inpaint_latents = torch.cat([mask_input, masked_video_latents_input], dim=2)\n",
    "            prepared_inputs['inpaint_latents'] = inpaint_latents\n",
    "    \n",
    "    # Load other encoded inputs\n",
    "    for key in ['video_latents', 'mask']:\n",
    "        if key in encoded_data and encoded_data[key] is not None:\n",
    "            prepared_inputs[key] = encoded_data[key]\n",
    "    \n",
    "    return prepared_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_tensor_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor_resized.shape, cond_video_twice_resized.shape, masks_tensor_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_dict['video_latents'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_video = (1.0 - masks_tensor_resized.permute(1, 0, 2, 3).unsqueeze(0)) * 255.0\n",
    "\n",
    "\n",
    "latents_dict = encode_inputs_to_latents(\n",
    "    trajcrafter.pipeline,\n",
    "    video=cond_video_twice_resized.permute(1, 0, 2, 3).unsqueeze(0).to('cuda'),\n",
    "    reference=frames_tensor_resized[:10].permute(1, 0, 2, 3).unsqueeze(0).to('cuda'),\n",
    "    mask_video=mask_video.to('cuda'),\n",
    "    masked_video_latents=None,\n",
    "    prompt=caption,\n",
    "    negative_prompt=opts.negative_prompt,\n",
    "    height=384,\n",
    "    width=672,\n",
    "    device=\"cuda\",\n",
    "    batch_size=1,\n",
    "    noise_aug_strength=0.0563,\n",
    "    max_sequence_length=226,\n",
    "    do_classifier_free_guidance=True,\n",
    "    ground_truth_video=frames_tensor_resized[10:].permute(1, 0, 2, 3).unsqueeze(0).to('cuda'),  # GT video for training\n",
    "    encode_for_training=True  # Flag to indicate training vs inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each key, print shape\n",
    "\n",
    "save_dir_latents = '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/latents'\n",
    "for key, value in latents_dict.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape}, dtype={value.dtype}\")\n",
    "        \n",
    "        # save it to disk as .pt file\n",
    "        # torch.save(value, f\"{save_dir_latents}/{key}.pt\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save video_latents as pt to /home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/latents\n",
    "torch.save(\n",
    "    latents_dict['video_latents'],\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/latents/video_latents.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    latents_dict['gt_video_latents'][0,12,10].cpu().float(),\n",
    "    cmap='gray'\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.colorbar(fraction=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "import time\n",
    "\n",
    "def run_inference_from_cached_latents(\n",
    "    pipeline,\n",
    "    cached_latents,\n",
    "    height=480,\n",
    "    width=720,\n",
    "    num_frames=49,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=6.0,\n",
    "    use_dynamic_cfg=False,\n",
    "    eta=0.0,\n",
    "    generator=None,\n",
    "    latents=None,\n",
    "    output_type=\"numpy\",\n",
    "    return_dict=False,\n",
    "    strength=1.0,\n",
    "    noise_aug_strength=0.0563,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run TrajectoryCrafter inference using pre-encoded cached latents.\n",
    "    Uses only conditioning inputs, NOT ground truth latents.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Move cached latents to device\n",
    "    for key, value in cached_latents.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            cached_latents[key] = value.to(device)\n",
    "    \n",
    "    # Setup basic parameters\n",
    "    batch_size = 1\n",
    "    num_videos_per_prompt = 1\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    \n",
    "    pipeline._guidance_scale = guidance_scale\n",
    "    pipeline._interrupt = False\n",
    "    \n",
    "    # 1. Prepare prompt embeddings (from cache)\n",
    "    if 'prompt_embeds' in cached_latents:\n",
    "        prompt_embeds = cached_latents['prompt_embeds']\n",
    "        if do_classifier_free_guidance and 'negative_prompt_embeds' in cached_latents:\n",
    "            negative_prompt_embeds = cached_latents['negative_prompt_embeds']\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"prompt_embeds not found in cached_latents\")\n",
    "    \n",
    "    # 2. Set timesteps\n",
    "    pipeline.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps, num_inference_steps = pipeline.get_timesteps(\n",
    "        num_inference_steps=num_inference_steps, \n",
    "        strength=strength, \n",
    "        device=device\n",
    "    )\n",
    "    pipeline._num_timesteps = len(timesteps)\n",
    "    \n",
    "    latent_timestep = timesteps[:1].repeat(batch_size * num_videos_per_prompt)\n",
    "    is_strength_max = strength == 1.0\n",
    "    \n",
    "    # 3. Prepare reference latents (from cache)\n",
    "    if 'ref_latents' in cached_latents:\n",
    "        ref_latents = cached_latents['ref_latents']\n",
    "        ref_input = torch.cat([ref_latents] * 2) if do_classifier_free_guidance else ref_latents\n",
    "    else:\n",
    "        raise ValueError(\"ref_latents not found in cached_latents\")\n",
    "    \n",
    "    # 4. Prepare initial latents for denoising\n",
    "    num_channels_latents = pipeline.vae.config.latent_channels\n",
    "    num_channels_transformer = pipeline.transformer.config.in_channels\n",
    "    \n",
    "    # For img2vid, use conditioning video latents if available (NOT gt_video_latents!)\n",
    "    init_video_latents = None\n",
    "    if 'cond_video_latents' in cached_latents:\n",
    "        # Use conditioning video latents for img2vid initialization\n",
    "        cond_latents = cached_latents['cond_video_latents']  # [b, f, c, h, w]\n",
    "        init_video_latents = rearrange(cond_latents, \"b f c h w -> b c f h w\")\n",
    "    \n",
    "    # Prepare latents shape\n",
    "    shape = (\n",
    "        batch_size,\n",
    "        (num_frames - 1) // pipeline.vae_scale_factor_temporal + 1,\n",
    "        num_channels_latents,\n",
    "        height // pipeline.vae_scale_factor_spatial,\n",
    "        width // pipeline.vae_scale_factor_spatial,\n",
    "    )\n",
    "    \n",
    "    if latents is None:\n",
    "        noise = randn_tensor(shape, generator=generator, device=device, dtype=prompt_embeds.dtype)\n",
    "        if init_video_latents is not None and not is_strength_max:\n",
    "            # Convert to transformer format for noise addition\n",
    "            video_latents = rearrange(init_video_latents, \"b c f h w -> b f c h w\")\n",
    "            latents = pipeline.scheduler.add_noise(video_latents, noise, latent_timestep)\n",
    "        else:\n",
    "            # Pure text2video generation - start from pure noise\n",
    "            latents = noise\n",
    "        latents = latents * pipeline.scheduler.init_noise_sigma if is_strength_max else latents\n",
    "    else:\n",
    "        noise = latents.to(device)\n",
    "        latents = noise * pipeline.scheduler.init_noise_sigma\n",
    "    \n",
    "    # 5. Prepare inpainting latents (from cache)\n",
    "    inpaint_latents = None\n",
    "    mask = None\n",
    "    \n",
    "    if num_channels_transformer != num_channels_latents:\n",
    "        # Inpainting model - need mask_latents and masked_video_latents\n",
    "        if 'mask_latents' in cached_latents and 'masked_video_latents' in cached_latents:\n",
    "            mask_latents = cached_latents['mask_latents']\n",
    "            masked_video_latents = cached_latents['masked_video_latents']\n",
    "            \n",
    "            if mask_latents is not None and masked_video_latents is not None:\n",
    "                mask_input = torch.cat([mask_latents] * 2) if do_classifier_free_guidance else mask_latents\n",
    "                masked_video_latents_input = torch.cat([masked_video_latents] * 2) if do_classifier_free_guidance else masked_video_latents\n",
    "                \n",
    "                # Channel concatenation for inpainting\n",
    "                inpaint_latents = torch.cat([mask_input, masked_video_latents_input], dim=2)\n",
    "        \n",
    "        # Also get the mask if available\n",
    "        if 'mask' in cached_latents and cached_latents['mask'] is not None:\n",
    "            mask = cached_latents['mask']\n",
    "    else:\n",
    "        # Non-inpainting model - just need mask\n",
    "        if 'mask' in cached_latents and cached_latents['mask'] is not None:\n",
    "            mask = cached_latents['mask']\n",
    "        else:\n",
    "            # Create zero mask if none provided\n",
    "            mask = torch.zeros_like(latents)\n",
    "    \n",
    "    # 6. Prepare extra step kwargs\n",
    "    extra_step_kwargs = pipeline.prepare_extra_step_kwargs(generator, eta)\n",
    "    \n",
    "    # 7. Create rotary embeddings if required\n",
    "    image_rotary_emb = (\n",
    "        pipeline._prepare_rotary_positional_embeddings(height, width, latents.size(1), device)\n",
    "        if pipeline.transformer.config.use_rotary_positional_embeddings\n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    # 8. Denoising loop\n",
    "    num_warmup_steps = max(len(timesteps) - num_inference_steps * pipeline.scheduler.order, 0)\n",
    "    \n",
    "    with pipeline.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        old_pred_original_sample = None\n",
    "        for i, t in enumerate(timesteps):\n",
    "            if pipeline.interrupt:\n",
    "                continue\n",
    "                \n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "            timestep = t.expand(latent_model_input.shape[0])\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = pipeline.transformer(\n",
    "                hidden_states=latent_model_input,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                timestep=timestep,\n",
    "                image_rotary_emb=image_rotary_emb,\n",
    "                return_dict=False,\n",
    "                inpaint_latents=inpaint_latents,\n",
    "                cross_latents=ref_input,\n",
    "            )[0]\n",
    "            noise_pred = noise_pred.float()\n",
    "            \n",
    "            # Apply guidance\n",
    "            if use_dynamic_cfg:\n",
    "                import math\n",
    "                pipeline._guidance_scale = 1 + guidance_scale * (\n",
    "                    (1 - math.cos(math.pi * ((num_inference_steps - t.item()) / num_inference_steps) ** 5.0)) / 2\n",
    "                )\n",
    "            \n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + pipeline.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            # Compute previous sample\n",
    "            if not hasattr(pipeline.scheduler, 'step') or 'DPM' not in pipeline.scheduler.__class__.__name__:\n",
    "                latents = pipeline.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "            else:\n",
    "                latents, old_pred_original_sample = pipeline.scheduler.step(\n",
    "                    noise_pred, old_pred_original_sample, t,\n",
    "                    timesteps[i - 1] if i > 0 else None, latents,\n",
    "                    **extra_step_kwargs, return_dict=False,\n",
    "                )\n",
    "            \n",
    "            latents = latents.to(prompt_embeds.dtype)\n",
    "            \n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipeline.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "    \n",
    "    # 9. Decode latents\n",
    "    if output_type == \"numpy\":\n",
    "        video = pipeline.decode_latents(latents)\n",
    "    elif output_type != \"latent\":\n",
    "        video = pipeline.decode_latents(latents)\n",
    "        video = pipeline.video_processor.postprocess_video(video=video, output_type=output_type)\n",
    "    else:\n",
    "        video = latents\n",
    "    \n",
    "    # Clean up\n",
    "    pipeline.maybe_free_model_hooks()\n",
    "    \n",
    "    return video\n",
    "    \n",
    "    # if not return_dict:\n",
    "    #     if output_type == \"numpy\":\n",
    "    #         video = torch.from_numpy(video)\n",
    "    \n",
    "    # return CogVideoX_Fun_PipelineOutput(videos=video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = run_inference_from_cached_latents(\n",
    "        trajcrafter.pipeline,\n",
    "        latents_dict,\n",
    "        height=opts.sample_size[0],\n",
    "        width=opts.sample_size[1],\n",
    "        num_frames=opts.video_length,\n",
    "        guidance_scale=opts.diffusion_guidance_scale,\n",
    "        num_inference_steps=opts.diffusion_inference_steps,\n",
    "        generator=generator,\n",
    "        device=torch.device(\"cuda\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.from_numpy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_tensor_resized = (frames_tensor + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "# frames_tensor_resized = smart_video_resize(frames_tensor_resized)\n",
    "save_video(\n",
    "    output[0].permute(1, 2, 3, 0),\n",
    "    f'{save_dir}/gen.mp4',\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_depths_tensor_resized.min(), warped_depths_tensor_resized.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_min = depths_resized.min()\n",
    "depths_max = depths_resized.max()\n",
    "\n",
    "depths_resized_norm = (depths_resized - depths_min) / (depths_max - depths_min + 1e-8)\n",
    "warped_depths_tensor_resized_norm = depths_resized_norm[10:] * masks_tensor_resized\n",
    "\n",
    "# permute, repeat, unsqueeze, to cuda\n",
    "\n",
    "\n",
    "latents_dict_depths = encode_inputs_to_latents(\n",
    "    trajcrafter.pipeline,\n",
    "    video=warped_depths_tensor_resized_norm.permute(1, 0, 2, 3).repeat(3, 1, 1, 1).unsqueeze(0).to('cuda'),\n",
    "    reference=depths_resized_norm[:10].permute(1, 0, 2, 3).repeat(3, 1, 1, 1).unsqueeze(0).to('cuda'),\n",
    "    mask_video=mask_video.to('cuda'),\n",
    "    masked_video_latents=None,\n",
    "    prompt=caption,\n",
    "    negative_prompt=opts.negative_prompt,\n",
    "    height=384,\n",
    "    width=672,\n",
    "    device=\"cuda\",\n",
    "    batch_size=1,\n",
    "    noise_aug_strength=0.0563,\n",
    "    max_sequence_length=226,\n",
    "    do_classifier_free_guidance=True,\n",
    "    ground_truth_video=depths_resized_norm[10:].permute(1, 0, 2, 3).repeat(3, 1, 1, 1).unsqueeze(0).to('cuda'),  # GT video for training\n",
    "    encode_for_training=True  # Flag to indicate training vs inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in latents_dict_depths.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape}, dtype={value.dtype}\")\n",
    "        \n",
    "        # save it to disk as .pt file\n",
    "        # torch.save(value, f\"{save_dir_latents}/{key}.pt\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    latents_dict_depths['cond_video_latents'][0,12,0].cpu().float(),\n",
    "    cmap='gray'\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.colorbar(fraction=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=opts.device).manual_seed(opts.seed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_depths = run_inference_from_cached_latents(\n",
    "        trajcrafter.pipeline,\n",
    "        latents_dict_depths,\n",
    "        height=opts.sample_size[0],\n",
    "        width=opts.sample_size[1],\n",
    "        num_frames=opts.video_length,\n",
    "        guidance_scale=opts.diffusion_guidance_scale,\n",
    "        num_inference_steps=opts.diffusion_inference_steps,\n",
    "        generator=generator,\n",
    "        device=torch.device(\"cuda\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_depths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_depths_colored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_depths_unnorm = torch.from_numpy(output_depths)\n",
    "\n",
    "# take mean along 1th axis\n",
    "output_depths_unnorm = output_depths_unnorm.mean(dim=1, keepdim=True)\n",
    "\n",
    "print(output_depths_unnorm.min(), output_depths_unnorm.max(), output_depths_unnorm.shape)\n",
    "\n",
    "# # unnormalize depths back to original range\n",
    "output_depths_unnorm = output_depths_unnorm * (depths_max - depths_min + 1e-8) + depths_min\n",
    "\n",
    "print(output_depths_unnorm.min(), output_depths_unnorm.max(), output_depths_unnorm.shape)\n",
    "\n",
    "\n",
    "output_depths_colored = apply_colormap_to_depth(\n",
    "    output_depths_unnorm[0].squeeze(1),\n",
    "    inverse=True\n",
    "    )\n",
    "\n",
    "print(output_depths_colored.min(), output_depths_colored.max(), output_depths_colored.shape)\n",
    "\n",
    "\n",
    "save_video(\n",
    "    output_depths_colored[0],\n",
    "    f'{save_dir}/gen_depth.mp4',\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud Warper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "\n",
    "import warper_point_cloud\n",
    "warper = warper_point_cloud.GlobalPointCloudWarper(device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pc_list = []\n",
    "color_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j in tqdm(range(frames_tensor.shape[0])):\n",
    "        i = 0\n",
    "        points, colors, _ = warper.create_pointcloud_from_image(\n",
    "            frames_tensor[i:i+1],\n",
    "            None,\n",
    "            depths[i:i+1],\n",
    "            # torch.inverse(poses_tensor[i:i+1]).to('cuda'),\n",
    "            poses_tensor[i:i+1].to('cuda'),\n",
    "            # K_tensor[i:i+1],\n",
    "            K_tensor.unsqueeze(0),\n",
    "            1,\n",
    "        )\n",
    "        pc_list.append(points)\n",
    "        color_list.append(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_autoregressive as utils_ar\n",
    "\n",
    "warped_images = []\n",
    "masks = []        \n",
    "\n",
    "for i in tqdm(range(frames_tensor.shape[0])):\n",
    "\n",
    "    # warped_image, mask = warper.render_pointcloud_zbuffer_vectorized_point_size(\n",
    "    warped_image, mask = warper.render_pointcloud_zbuffer_vectorized_fixed(\n",
    "        pc_list[i],\n",
    "        color_list[i],\n",
    "        poses_tensor[i:i+1].to('cuda'),\n",
    "        K_tensor.unsqueeze(0).to('cuda'),\n",
    "        image_size=frames_tensor.shape[-2:],\n",
    "        point_size=2,\n",
    "    )\n",
    "\n",
    "    \n",
    "    cleaned_mask = utils_ar.clean_single_mask_simple(\n",
    "        mask[0],\n",
    "        kernel_size=9,\n",
    "        n_erosion_steps=1,\n",
    "        n_dilation_steps=1\n",
    "        )\n",
    "    # should stay in [-1, 1] range\n",
    "    \n",
    "    cleaned_mask = cleaned_mask.unsqueeze(0)\n",
    "    warped_image = warped_image * cleaned_mask\n",
    "    \n",
    "    warped_images.append(warped_image)\n",
    "    masks.append(cleaned_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# plot warped image j, mask j, warped depth j\n",
    "j = 0\n",
    "k = 30\n",
    "\n",
    "frame = frames_tensor[j].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "warped_image = warped_images[k][0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "target_frame = frames_tensor[k].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('Source Frame')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(warped_image)\n",
    "plt.title('Warped Image to Frame {}'.format(j))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(target_frame)\n",
    "plt.title('Target Frame {}'.format(j))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import save_video\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def make_dimensions_even(tensor):\n",
    "    \"\"\"Pad tensor to make height and width even numbers\"\"\"\n",
    "    _, h, w, c = tensor.shape\n",
    "    pad_h = h % 2\n",
    "    pad_w = w % 2\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        # Pad bottom and right if needed\n",
    "        tensor = torch.nn.functional.pad(tensor, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "cond_video = (torch.cat(warped_images) + 1.0) / 2.0  # [T, 3, H, W] in [0,1]\n",
    "cond_video_padded = make_dimensions_even(\n",
    "    cond_video.permute(0, 2, 3, 1)\n",
    ")\n",
    "save_dir = '/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up/vkitti2_pc_warper'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_video(\n",
    "    cond_video_padded,\n",
    "    f'{save_dir}/warped_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "# --- save inputs for visualization ---\n",
    "\n",
    "input_video_padded = make_dimensions_even(\n",
    "    (frames_tensor.permute(0, 2, 3, 1) + 1.0) / 2.0\n",
    ")\n",
    "\n",
    "save_video(\n",
    "    input_video_padded,\n",
    "    f'{save_dir}/input_video.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "warped_depths_tensor = torch.cat(warped_depths)\n",
    "# Apply before saving\n",
    "warped_depths_padded = make_dimensions_even(\n",
    "    (warped_depths_tensor.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / warped_depths_tensor.max()\n",
    ")\n",
    "save_video(\n",
    "    warped_depths_padded,\n",
    "    f'{save_dir}/warped_depths.mp4',\n",
    "    fps=10,\n",
    ")\n",
    "\n",
    "depths_padded = make_dimensions_even(\n",
    "    (depths.permute(0, 2, 3, 1).repeat(1, 1, 1, 3)) / depths.max()\n",
    ")\n",
    "save_video(\n",
    "    depths_padded,\n",
    "    f'{save_dir}/input_depths.mp4',\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viser\n",
    "import numpy as np\n",
    "\n",
    "# Start viser server\n",
    "server = viser.ViserServer()\n",
    "import os\n",
    "\n",
    "# print slurm node name\n",
    "\n",
    "node_name=os.environ.get('SLURM_NODELIST', 'localhost')\n",
    "\n",
    "print(f'http://{node_name}:{server.get_port()}')\n",
    "\n",
    "\n",
    "def add_points(\n",
    "    server,\n",
    "    points: np.ndarray,  # (N, 3)\n",
    "    colors: np.ndarray,  # (N, 3)\n",
    "    name: str,\n",
    "):\n",
    "    # ensure colors are in [0, 1]\n",
    "    if colors.min() < -0.1:\n",
    "        colors = (colors + 1.0) / 2.0\n",
    "    \n",
    "    server.scene.add_point_cloud(\n",
    "        name=name,\n",
    "        points=points,\n",
    "        colors=colors,\n",
    "        point_size=0.01\n",
    "    )\n",
    "\n",
    "def add_camera(\n",
    "    server,\n",
    "    pose: np.ndarray,  # (4, 4)\n",
    "    name: str,\n",
    "    color: tuple = (0.2, 0.8, 0.2),\n",
    "):\n",
    "    pose = np.linalg.inv(pose)\n",
    "    \n",
    "    position = pose[:3, 3]\n",
    "    rotation_matrix = pose[:3, :3]\n",
    "    \n",
    "    # Convert rotation to quaternion\n",
    "    wxyz = viser.transforms.SO3.from_matrix(rotation_matrix).wxyz\n",
    "    \n",
    "    server.scene.add_camera_frustum(\n",
    "        name,\n",
    "        fov=60, aspect=4/3, scale=0.1,\n",
    "        position=position, wxyz=wxyz,\n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "# Add this after creating your server and adding point clouds\n",
    "@server.on_client_connect\n",
    "def _(client: viser.ClientHandle) -> None:\n",
    "    yaw_slider = client.gui.add_slider(\"Camera Yaw\", min=-180, max=180, step=1, initial_value=0)\n",
    "    \n",
    "    @yaw_slider.on_update\n",
    "    def _(_):\n",
    "        angle_rad = np.deg2rad(yaw_slider.value)\n",
    "        radius = 8.0\n",
    "        \n",
    "        position = np.array([\n",
    "            radius * np.sin(angle_rad),\n",
    "            0,  # height\n",
    "            radius * np.cos(angle_rad)\n",
    "        ])\n",
    "        \n",
    "        client.camera.position = position\n",
    "        client.camera.look_at = np.array([0, 0, 0])\n",
    "        client.camera.up_direction = np.array([0, -1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.scene.reset()\n",
    "\n",
    "# add original point cloud #0, and all cameras from first segment\n",
    "\n",
    "j_list = [10]\n",
    "\n",
    "for j in j_list:    \n",
    "    add_points(\n",
    "        server,\n",
    "        pc_list[j].cpu().numpy()[::10,:],\n",
    "        color_list[j].cpu().numpy()[::10,:],\n",
    "        name=f'input_pc_{j}'\n",
    "    )\n",
    "    \n",
    "for j in range(frames_tensor.shape[0]):\n",
    "    add_camera(\n",
    "        server,\n",
    "        torch.inverse(poses_tensor[j]).cpu().numpy(),\n",
    "        name=f'input_cam_{j:02d}',\n",
    "        color=(0.2, 0.8, 0.2)\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths.min(), depths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/06_10_25_vggt')\n",
    "from parsing import get_parser\n",
    "import utils_autoregressive as utils_ar\n",
    "from datetime import datetime\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "sys.argv = [\n",
    "    \"\",\n",
    "    \"--video_path\", \"/home/azhuravl/nobackup/DAVIS_testing/trainval/vkitti2.mp4\",\n",
    "    \"--n_splits\", \"4\",\n",
    "    \"--overlap_frames\", \"0\",\n",
    "    \"--radius\", \"0\",\n",
    "    \"--mode\", \"gradual\",\n",
    "]\n",
    "\n",
    "parser = get_parser()\n",
    "opts_base = parser.parse_args()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "video_basename = os.path.splitext(os.path.basename(opts_base.video_path))[0]\n",
    "\n",
    "# Setup\n",
    "opts_base.weight_dtype = torch.bfloat16\n",
    "opts_base.exp_name = f\"{video_basename}_{timestamp}_autoregressive\"\n",
    "opts_base.save_dir = os.path.join(opts_base.out_dir, opts_base.exp_name)\n",
    "\n",
    "# Create TrajCrafterVisualization instance for autoregressive generation\n",
    "radius = opts_base.radius\n",
    "\n",
    "variants = [\n",
    "    (\"right_90\", [0, 90, radius, 0, 0]),\n",
    "]\n",
    "\n",
    "pose = [90, 0, 0, 0, 1]\n",
    "name = f\"{pose[0]}_{pose[1]}_{pose[2]}_{pose[3]}_{pose[4]}\"\n",
    "\n",
    "opts = copy.deepcopy(opts_base)\n",
    "opts.exp_name = f\"{video_basename}_{timestamp}_{name}_auto_s{opts_base.n_splits}\"\n",
    "opts.save_dir = os.path.join(opts.out_dir, opts.exp_name)\n",
    "opts.camera = \"target\"\n",
    "opts.target_pose = pose\n",
    "opts.traj_txt = 'test/trajs/loop2.txt'\n",
    "\n",
    "# Make directories\n",
    "os.makedirs(opts.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajcrafter = utils_ar.TrajCrafterAutoregressive(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# frames_tensor = (\n",
    "    # torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(opts.device) * 2.0 - 1.0\n",
    "  # 49 576 1024 3 -> 49 3 576 1024, [-1,1]\n",
    "# reverse this to get frames in numpy\n",
    "frames_np = ((frames_tensor.cpu().permute(0, 2, 3, 1).numpy() + 1.0) / 2.0).astype(np.float32)\n",
    "\n",
    "trajcrafter.prompt = trajcrafter.get_caption(opts, frames_np[opts.video_length // 2])\n",
    "print(trajcrafter.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, segment_dir = utils_ar.sample_diffusion(\n",
    "    trajcrafter,\n",
    "    frames_tensor[10:],\n",
    "    warped_images,\n",
    "    frames_tensor[:10],\n",
    "    masks,\n",
    "    opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TartanAir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(video_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [\n",
    "    'abandonedfactory/Easy/P001',\n",
    "    # 'abandonedfactory/Easy/P005', \n",
    "    # 'office/Easy/P001',\n",
    "    # 'office/Easy/P002',\n",
    "    # 'office2/Easy/P001'\n",
    "]\n",
    "\n",
    "dataset_tartanair = video_datasets.TartanAirDataset(\n",
    "        aug_params=None,\n",
    "        root=\"/home/azhuravl/scratch/tartanair\",\n",
    "        split=\"train\",\n",
    "        sample_len=59,\n",
    "        only_first_n_samples=-1,\n",
    "        sampling_stride=3,          # Starting frame stride (default 3)\n",
    "        min_temporal_step=1,        # Minimum temporal step (default 1)  \n",
    "        max_temporal_step=2,        # Maximum temporal step (default 6)\n",
    "        train_sequences=train_sequences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset_tartanair[0]\n",
    "data_1 = dataset_tartanair[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix TartanAir cameras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
