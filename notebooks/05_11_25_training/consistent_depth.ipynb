{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716aed6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef008983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/lora_utils_ours')\n",
    "\n",
    "# from dataset_videos import SimpleValidationDataset  # Add this import\n",
    "\n",
    "\n",
    "# dataset = SimpleValidationDataset(\n",
    "#     validation_dir='/home/azhuravl/scratch/datasets_latents/monkaa_1000',\n",
    "#     use_depth=True,\n",
    "#     max_samples=100,\n",
    "#     num_ref_frames=49,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_0 = dataset[0]\n",
    "# data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.memory._record_memory_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30dd31",
   "metadata": {},
   "source": [
    "## Video Depth Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ee81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/Video-Depth-Anything')\n",
    "\n",
    "from video_depth_anything.video_depth import VideoDepthAnything\n",
    "from utils.dc_utils import read_video_frames, save_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsVDA:\n",
    "    input_video = '/home/azhuravl/scratch/datasets_latents/monkaa_1000/000/videos/input_video.mp4'\n",
    "    output_dir = '/home/azhuravl/work/Video-Depth-Anything/outputs'\n",
    "    input_size = 256\n",
    "    max_res = 1280\n",
    "    encoder = 'vitl'\n",
    "    max_len = -1\n",
    "    target_fps = -1\n",
    "    metric = False\n",
    "    fp32 = False\n",
    "    grayscale = False\n",
    "    save_npz = False\n",
    "    save_exr = False\n",
    "    focal_length_x = 470.4\n",
    "    focal_length_y = 470.4\n",
    "    \n",
    "args_vda = ArgsVDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "}\n",
    "checkpoint_name = 'video_depth_anything'\n",
    "\n",
    "video_depth_anything = VideoDepthAnything(**model_configs[args_vda.encoder], metric=args_vda.metric)\n",
    "video_depth_anything.load_state_dict(torch.load(\n",
    "    f'/home/azhuravl/work/Video-Depth-Anything/checkpoints/{checkpoint_name}_{args_vda.encoder}.pth', \n",
    "    map_location='cpu'), strict=True)\n",
    "video_depth_anything = video_depth_anything.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_depth_anything to bf16\n",
    "\n",
    "# video_depth_anything = video_depth_anything.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f05b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable grad for video_depth_anything\n",
    "for param in video_depth_anything.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_depth_anything.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in video_depth_anything.head.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters in VideoDepthAnything: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames, target_fps = read_video_frames(args_vda.input_video, args_vda.max_len, args_vda.target_fps, args_vda.max_res)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    depths, fps = video_depth_anything.infer_video_depth(\n",
    "        frames,\n",
    "        target_fps, input_size=args_vda.input_size, device=DEVICE, fp32=args_vda.fp32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colormap = np.array(cm.get_cmap(\"inferno\").colors)\n",
    "d_min, d_max = depths.min(), depths.max()\n",
    "\n",
    "depth_vis_list = []\n",
    "for i in range(depths.shape[0]):\n",
    "    depth = depths[i]\n",
    "    depth_norm = ((depth - d_min) / (d_max - d_min) * 255).astype(np.uint8)\n",
    "    depth_vis = (colormap[depth_norm] * 255).astype(np.uint8) if not args_vda.grayscale else depth_norm\n",
    "    depth_vis_list.append(depth_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(depths[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e0a0d",
   "metadata": {},
   "source": [
    "## Test Time Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84290eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import tqdm\n",
    "import warnings\n",
    "import argparse\n",
    "import torch.optim \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module \n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class Arguments:\n",
    "    gpu = '0'\n",
    "    random_seed = 2025\n",
    "    epochs = 50\n",
    "    exp_name = 'base'\n",
    "    mode = 'VP'  # choices=['VP', 'FT']\n",
    "    dataset = 'ibims'  # choices=['ibims', 'ddad']\n",
    "    dataset_path = '/workspace/data_all'\n",
    "    \n",
    "args_ttt = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50489161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unidepth_custom.models import UniDepthV2\n",
    "\n",
    "\n",
    "def compute_scale_and_shift(predicted_depth, sparse_depth):\n",
    "\n",
    "    valid_mask = (sparse_depth > 0)\n",
    "    \n",
    "    pred_valid = predicted_depth[valid_mask]   \n",
    "    sparse_valid = sparse_depth[valid_mask]    \n",
    "    \n",
    "    if pred_valid.numel() == 0:\n",
    "        device = predicted_depth.device\n",
    "        dtype = predicted_depth.dtype\n",
    "        return torch.tensor(1.0, device=device, dtype=dtype), torch.tensor(0.0, device=device, dtype=dtype)\n",
    "    \n",
    "    X = torch.stack([pred_valid, torch.ones_like(pred_valid)], dim=1)\n",
    "    \n",
    "    a = torch.pinverse(X) @ sparse_valid \n",
    "    scale = a[0]\n",
    "    shift = a[1]\n",
    "    \n",
    "    return scale, shift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11902e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_gt = torch.load('/home/azhuravl/scratch/datasets_latents/monkaa_1000/000/videos/input_depths.pt', weights_only=True).squeeze(1) \n",
    "depth_gt_inv = 1.0 / (depth_gt + 1e-8)\n",
    "\n",
    "depth_warped = torch.load('/home/azhuravl/scratch/datasets_latents/monkaa_1000/000/videos/warped_depths.pt', weights_only=True).squeeze(1)\n",
    "# areas that are 0 should stay 0 after inversion\n",
    "depth_warped_inv = torch.where(depth_warped > 0, 1.0 / depth_warped, torch.zeros_like(depth_warped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671aeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show depths[0] and depth_gt[0] side by side with colorbars\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n",
    "im1 = axs[0].imshow(depths[40], cmap='inferno')\n",
    "axs[0].set_title('Predicted Depth[0]')\n",
    "fig.colorbar(im1, ax=axs[0])\n",
    "im2 = axs[1].imshow(depth_gt_inv[40], cmap='inferno')\n",
    "axs[1].set_title('Ground Truth Depth[0]')\n",
    "fig.colorbar(im2, ax=axs[1])  \n",
    "\n",
    "im3 = axs[2].imshow(depth_warped_inv[40], cmap='inferno')\n",
    "axs[2].set_title('Warped Depth[0]')\n",
    "fig.colorbar(im3, ax=axs[2])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7aac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/azhuravl/work/Video-Depth-Anything/video_depth_anything/util')\n",
    "\n",
    "from transform import Resize, NormalizeImage, PrepareForNet\n",
    "from torchvision.transforms import Compose\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# infer settings, do not change\n",
    "INFER_LEN = 32\n",
    "OVERLAP = 10\n",
    "KEYFRAMES = [0,12,24,25,26,27,28,29,30,31]\n",
    "INTERP_LEN = 8\n",
    "\n",
    "\n",
    "def prepare_frames(frames, input_size=518):\n",
    "    \"\"\"\n",
    "    Prepare frames for inference by resizing and normalizing.\n",
    "    \n",
    "    Args:\n",
    "        frames: numpy array of shape [T, H, W, C] containing video frames\n",
    "        input_size: target input size for the model\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: processed frames ready for model input [1, T, C, H, W]\n",
    "        tuple: original frame dimensions (height, width)\n",
    "    \"\"\"\n",
    "    if frames.shape[0] != INFER_LEN:\n",
    "        raise ValueError(f\"Expected {INFER_LEN} frames, but got {frames.shape[0]} frames\")\n",
    "    \n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    ratio = max(frame_height, frame_width) / min(frame_height, frame_width)\n",
    "    \n",
    "    # Adjust input size for very wide/tall videos\n",
    "    if ratio > 1.78:\n",
    "        input_size = int(input_size * 1.777 / ratio)\n",
    "        input_size = round(input_size / 14) * 14\n",
    "\n",
    "    transform = Compose([\n",
    "        Resize(\n",
    "            width=input_size,\n",
    "            height=input_size,\n",
    "            resize_target=False,\n",
    "            keep_aspect_ratio=True,\n",
    "            ensure_multiple_of=14,\n",
    "            resize_method='lower_bound',\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        PrepareForNet(),\n",
    "    ])\n",
    "\n",
    "    # Process all frames\n",
    "    processed_frames = []\n",
    "    for i in range(INFER_LEN):\n",
    "        frame_tensor = torch.from_numpy(\n",
    "            transform({'image': frames[i].astype(np.float32) / 255.0})['image']\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        processed_frames.append(frame_tensor)\n",
    "    \n",
    "    input_tensor = torch.cat(processed_frames, dim=1)\n",
    "    \n",
    "    return input_tensor, (frame_height, frame_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_resized, orig_dims = prepare_frames(frames[:32], input_size=args_vda.input_size)\n",
    "\n",
    "# now interpolate depths to shape of frames_resized\n",
    "depths_gt_inv_resized = F.interpolate(\n",
    "    depth_gt_inv[:32].unsqueeze(1),\n",
    "    size=frames_resized.shape[3:],\n",
    "    mode='bilinear',\n",
    ").unsqueeze(0)\n",
    "\n",
    "depths_warped_inv_resized = F.interpolate(\n",
    "    depth_warped_inv[:32].unsqueeze(1),\n",
    "    size=frames_resized.shape[3:],\n",
    "    mode='nearest',\n",
    ").unsqueeze(0)\n",
    "\n",
    "\n",
    "frames_resized.shape, depths_gt_inv_resized.shape, depths_warped_inv_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91366fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show depths[0] and depth_gt[0] side by side with colorbars\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n",
    "im1 = axs[0].imshow(frames_resized[0, 20].permute(1, 2, 0), cmap='inferno')\n",
    "axs[0].set_title('Predicted Depth[0]')\n",
    "fig.colorbar(im1, ax=axs[0], shrink=0.3)\n",
    "im2 = axs[1].imshow(depths_gt_inv_resized[0, 20, 0], cmap='inferno')\n",
    "axs[1].set_title('Ground Truth Depth[0]')\n",
    "fig.colorbar(im2, ax=axs[1], shrink=0.3)  \n",
    "\n",
    "im3 = axs[2].imshow(depths_warped_inv_resized[0, 20, 0], cmap='inferno')\n",
    "axs[2].set_title('Warped Depth[0]')\n",
    "fig.colorbar(im3, ax=axs[2], shrink=0.3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rmse_mean_ft, mae_mean_ft = 0.0, 0.0\n",
    "# rmse_mean_vp, mae_mean_vp = 0.0, 0.0\n",
    "\n",
    "\n",
    "# rgb = frames_resized.cuda()\n",
    "# depth = depths_gt_inv_resized.cuda()\n",
    "# sparse_depth = depths_warped_inv_resized.cuda()\n",
    "\n",
    "\n",
    "# gt_mask = depth > 0\n",
    "# sparse_mask = sparse_depth > 0\n",
    "\n",
    "# # Visual Prompt\n",
    "# # visual_prompt = torch.nn.Parameter(torch.zeros_like(rgb, device='cuda'))\n",
    "# # optimizer = torch.optim.AdamW([{'params': visual_prompt, 'lr': 2e-3}])\n",
    "\n",
    "# pbar = tqdm.tqdm(total=args_ttt.epochs)\n",
    "\n",
    "# # Create prompt for single frame and repeat across time\n",
    "# single_frame_prompt = torch.nn.Parameter(torch.zeros_like(rgb[:, :1], device='cuda'))  # [1, 1, C, H, W]\n",
    "# visual_prompt = single_frame_prompt.repeat(1, rgb.shape[1], 1, 1, 1)  # Repeat across time dimension\n",
    "# optimizer = torch.optim.AdamW([{'params': single_frame_prompt, 'lr': 2e-3}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle\n",
    "\n",
    "# snapshot = torch.cuda.memory_snapshot()\n",
    "# with open(\"/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/snapshot.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(snapshot, f)\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/snapshot_before.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c46e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mean_ft, mae_mean_ft = 0.0, 0.0\n",
    "rmse_mean_vp, mae_mean_vp = 0.0, 0.0\n",
    "\n",
    "# Keep model in original precision, don't convert to bf16\n",
    "# video_depth_anything = video_depth_anything.to(torch.bfloat16)\n",
    "\n",
    "# Convert data to bf16\n",
    "rgb = frames_resized.to(torch.bfloat16).cuda()\n",
    "depth = depths_gt_inv_resized.to(torch.bfloat16).cuda()\n",
    "sparse_depth = depths_warped_inv_resized.squeeze(2).to(torch.bfloat16).cuda()\n",
    "\n",
    "single_frame_prompt = torch.nn.Parameter(torch.zeros_like(rgb[:, :1], dtype=torch.bfloat16, device='cuda'))\n",
    "\n",
    "gt_mask = depth > 0\n",
    "sparse_mask = sparse_depth > 0\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': single_frame_prompt, 'lr': 2e-3}])\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "pbar = tqdm.tqdm(total=args_ttt.epochs)\n",
    "\n",
    "for epoch in range(args_ttt.epochs):\n",
    "    visual_prompt = single_frame_prompt.repeat(1, rgb.shape[1], 1, 1, 1)\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        new_rgb = rgb + visual_prompt\n",
    "        pre_depth_ = video_depth_anything.forward(new_rgb)\n",
    "        scale, shift = compute_scale_and_shift(pre_depth_, sparse_depth)\n",
    "        pre_depth = pre_depth_ * scale + shift\n",
    "        loss_l1 = F.l1_loss(pre_depth[sparse_mask], sparse_depth[sparse_mask])\n",
    "        loss_rmse = torch.sqrt(((pre_depth[sparse_mask] - sparse_depth[sparse_mask]) ** 2).mean())\n",
    "        loss = loss_l1 + loss_rmse\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    pbar.set_description(f'exp: {args_ttt.exp_name} l1: {loss_l1.item():.4f} rmse: {loss_rmse.item():.4f}')\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_depth_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for epoch in range(args_ttt.epochs):               \n",
    "#     new_rgb = rgb + visual_prompt\n",
    "    \n",
    "    \n",
    "#     # pre_depth_ = foundation_model({'image': new_rgb, 'depth': sparse_depth}, {})['depth']\n",
    "#     # Run model inference\n",
    "    \n",
    "#     with torch.autocast(device_type='cuda', enabled=(not args_vda.fp32)):\n",
    "#         depth = video_depth_anything.forward(new_rgb)  # depth shape: [1, T, H, W]\n",
    "\n",
    "    \n",
    "    \n",
    "#     scale, shift = compute_scale_and_shift(pre_depth_, sparse_depth)\n",
    "    \n",
    "#     print(scale, shift)    \n",
    "    \n",
    "#     pre_depth = pre_depth_ * scale + shift    \n",
    "        \n",
    "#     loss_l1 = F.l1_loss(pre_depth[sparse_mask], sparse_depth[sparse_mask])\n",
    "#     loss_rmse = torch.sqrt(((pre_depth[sparse_mask] - sparse_depth[sparse_mask]) ** 2).mean())\n",
    "#     loss = loss_l1 + loss_rmse\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()          \n",
    "#     optimizer.step()\n",
    "\n",
    "#     pbar.set_description(f'exp: {args_ttt.exp_name} l1: {loss_l1.item():.4f} rmse: {loss_rmse.item():.4f}')\n",
    "#     pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714afb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._dump_snapshot(\"/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/snapshot_after.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cuda cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ee3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    rmse_vp, mae_vp = torch.sqrt(((pre_depth[gt_mask] - depth[gt_mask]) ** 2).mean()), torch.abs(pre_depth[gt_mask] - depth[gt_mask]).mean()\n",
    "    rmse_mean_vp += rmse_vp.item()\n",
    "    mae_mean_vp += mae_vp.item()\n",
    "            \n",
    "pbar.close()\n",
    "    \n",
    "print(f'RMSE: {rmse_mean_vp}, MAE: {mae_mean_vp} idx: {idx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
