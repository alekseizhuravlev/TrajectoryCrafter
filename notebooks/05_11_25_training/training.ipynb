{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b07eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3382711/4235522030.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  depth = torch.load('/home/azhuravl/scratch/datasets_latents/driving_1000/002/videos/ref_depths.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 384, 672])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "depth = torch.load('/home/azhuravl/scratch/datasets_latents/driving_1000/002/videos/ref_depths.pt')\n",
    "depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a9b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/22_10_25_scaling_up')\n",
    "from generate_sceneflow import apply_colormap_to_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48637827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 validation samples (depth mode)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/lora_utils_ours')\n",
    "\n",
    "from dataset_videos import SimpleValidationDataset\n",
    "\n",
    "dataset = SimpleValidationDataset(\n",
    "    '/home/azhuravl/scratch/datasets_latents/driving_1000',\n",
    "    max_samples=5, use_depth=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7189af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset[0]\n",
    "# data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6521f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 49, 384, 672])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0['warped_video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a737b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_depth_sample_to_rgb(depth_tensor):\n",
    "    # sample (1, C, T, H, W)\n",
    "    # input [T, H, W]\n",
    "    # output [T, H, W, 3]\n",
    "    \n",
    "    depth_thw = depth_tensor.mean(dim=1, keepdim=False)[0]  # [T, H, W]\n",
    "    \n",
    "    # unnormalize depth to min 1, max 100, \n",
    "    # set values where depth_thw is zero to zero\n",
    "    depth_thw_unnorm = depth_thw * (100.0 - 1.0) + 1.0\n",
    "    depth_thw_unnorm = torch.where(depth_thw > 0, depth_thw_unnorm, torch.zeros_like(depth_thw_unnorm))\n",
    "    \n",
    "    depth_colormap = apply_colormap_to_depth(depth_thw_unnorm, inverse=True)  # [T, H, W, 3]\n",
    "    depth_bcthw = depth_colormap.permute(3, 0, 1, 2).unsqueeze(0)  # [1, 3, T, H, W]\n",
    "    return depth_bcthw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4acc5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bcthw = convert_depth_sample_to_rgb(\n",
    "    data_0['warped_video'].unsqueeze(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29e4d809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 49, 384, 672])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_bcthw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4ccc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from videox_fun.utils.utils import save_videos_grid\n",
    "\n",
    "save_videos_grid(\n",
    "    depth_bcthw, \n",
    "    '/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/test_depth_vis.mp4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908c32c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lora_unet__norm_out_linear.alpha', 'lora_unet__norm_out_linear.lora_down.weight', 'lora_unet__norm_out_linear.lora_up.weight', 'lora_unet__perceiver_cross_attention_0_to_kv.alpha', 'lora_unet__perceiver_cross_attention_0_to_kv.lora_down.weight', 'lora_unet__perceiver_cross_attention_0_to_kv.lora_up.weight', 'lora_unet__perceiver_cross_attention_0_to_q.alpha', 'lora_unet__perceiver_cross_attention_0_to_q.lora_down.weight', 'lora_unet__perceiver_cross_attention_0_to_q.lora_up.weight', 'lora_unet__proj_out.alpha', 'lora_unet__proj_out.lora_down.weight', 'lora_unet__proj_out.lora_up.weight', 'lora_unet__transformer_blocks_0_attn1_to_q.alpha', 'lora_unet__transformer_blocks_0_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_0_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_0_attn1_to_v.alpha', 'lora_unet__transformer_blocks_0_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_0_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_10_attn1_to_q.alpha', 'lora_unet__transformer_blocks_10_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_10_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_10_attn1_to_v.alpha', 'lora_unet__transformer_blocks_10_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_10_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_12_attn1_to_q.alpha', 'lora_unet__transformer_blocks_12_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_12_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_12_attn1_to_v.alpha', 'lora_unet__transformer_blocks_12_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_12_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_13_attn1_to_q.alpha', 'lora_unet__transformer_blocks_13_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_13_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_13_attn1_to_v.alpha', 'lora_unet__transformer_blocks_13_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_13_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_15_attn1_to_q.alpha', 'lora_unet__transformer_blocks_15_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_15_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_15_attn1_to_v.alpha', 'lora_unet__transformer_blocks_15_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_15_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_17_attn1_to_q.alpha', 'lora_unet__transformer_blocks_17_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_17_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_17_attn1_to_v.alpha', 'lora_unet__transformer_blocks_17_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_17_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_18_attn1_to_q.alpha', 'lora_unet__transformer_blocks_18_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_18_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_18_attn1_to_v.alpha', 'lora_unet__transformer_blocks_18_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_18_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_1_attn1_to_q.alpha', 'lora_unet__transformer_blocks_1_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_1_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_1_attn1_to_v.alpha', 'lora_unet__transformer_blocks_1_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_1_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_20_attn1_to_q.alpha', 'lora_unet__transformer_blocks_20_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_20_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_20_attn1_to_v.alpha', 'lora_unet__transformer_blocks_20_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_20_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_21_attn1_to_q.alpha', 'lora_unet__transformer_blocks_21_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_21_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_21_attn1_to_v.alpha', 'lora_unet__transformer_blocks_21_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_21_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_23_attn1_to_q.alpha', 'lora_unet__transformer_blocks_23_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_23_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_23_attn1_to_v.alpha', 'lora_unet__transformer_blocks_23_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_23_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_24_attn1_to_q.alpha', 'lora_unet__transformer_blocks_24_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_24_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_24_attn1_to_v.alpha', 'lora_unet__transformer_blocks_24_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_24_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_26_attn1_to_q.alpha', 'lora_unet__transformer_blocks_26_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_26_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_26_attn1_to_v.alpha', 'lora_unet__transformer_blocks_26_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_26_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_27_attn1_to_q.alpha', 'lora_unet__transformer_blocks_27_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_27_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_27_attn1_to_v.alpha', 'lora_unet__transformer_blocks_27_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_27_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_29_attn1_to_q.alpha', 'lora_unet__transformer_blocks_29_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_29_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_29_attn1_to_v.alpha', 'lora_unet__transformer_blocks_29_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_29_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_2_attn1_to_q.alpha', 'lora_unet__transformer_blocks_2_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_2_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_2_attn1_to_v.alpha', 'lora_unet__transformer_blocks_2_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_2_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_30_attn1_to_q.alpha', 'lora_unet__transformer_blocks_30_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_30_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_30_attn1_to_v.alpha', 'lora_unet__transformer_blocks_30_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_30_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_31_attn1_to_q.alpha', 'lora_unet__transformer_blocks_31_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_31_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_31_attn1_to_v.alpha', 'lora_unet__transformer_blocks_31_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_31_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_33_attn1_to_q.alpha', 'lora_unet__transformer_blocks_33_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_33_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_33_attn1_to_v.alpha', 'lora_unet__transformer_blocks_33_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_33_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_34_attn1_to_q.alpha', 'lora_unet__transformer_blocks_34_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_34_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_34_attn1_to_v.alpha', 'lora_unet__transformer_blocks_34_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_34_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_36_attn1_to_q.alpha', 'lora_unet__transformer_blocks_36_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_36_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_36_attn1_to_v.alpha', 'lora_unet__transformer_blocks_36_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_36_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_37_attn1_to_q.alpha', 'lora_unet__transformer_blocks_37_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_37_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_37_attn1_to_v.alpha', 'lora_unet__transformer_blocks_37_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_37_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_39_attn1_to_q.alpha', 'lora_unet__transformer_blocks_39_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_39_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_39_attn1_to_v.alpha', 'lora_unet__transformer_blocks_39_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_39_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_3_attn1_to_q.alpha', 'lora_unet__transformer_blocks_3_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_3_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_3_attn1_to_v.alpha', 'lora_unet__transformer_blocks_3_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_3_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_40_attn1_to_q.alpha', 'lora_unet__transformer_blocks_40_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_40_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_40_attn1_to_v.alpha', 'lora_unet__transformer_blocks_40_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_40_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_4_attn1_to_q.alpha', 'lora_unet__transformer_blocks_4_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_4_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_4_attn1_to_v.alpha', 'lora_unet__transformer_blocks_4_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_4_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_5_attn1_to_q.alpha', 'lora_unet__transformer_blocks_5_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_5_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_5_attn1_to_v.alpha', 'lora_unet__transformer_blocks_5_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_5_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_6_attn1_to_q.alpha', 'lora_unet__transformer_blocks_6_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_6_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_6_attn1_to_v.alpha', 'lora_unet__transformer_blocks_6_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_6_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_7_attn1_to_q.alpha', 'lora_unet__transformer_blocks_7_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_7_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_7_attn1_to_v.alpha', 'lora_unet__transformer_blocks_7_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_7_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_8_attn1_to_q.alpha', 'lora_unet__transformer_blocks_8_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_8_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_8_attn1_to_v.alpha', 'lora_unet__transformer_blocks_8_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_8_attn1_to_v.lora_up.weight', 'lora_unet__transformer_blocks_9_attn1_to_q.alpha', 'lora_unet__transformer_blocks_9_attn1_to_q.lora_down.weight', 'lora_unet__transformer_blocks_9_attn1_to_q.lora_up.weight', 'lora_unet__transformer_blocks_9_attn1_to_v.alpha', 'lora_unet__transformer_blocks_9_attn1_to_v.lora_down.weight', 'lora_unet__transformer_blocks_9_attn1_to_v.lora_up.weight'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load /home/azhuravl/work/TrajectoryCrafter/experiments/07-11-2025/13-33-14/checkpoint-1.safetensors\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "state_dict = load_file(\n",
    "    '/home/azhuravl/work/TrajectoryCrafter/experiments/07-11-2025/13-33-14/checkpoint-1.safetensors'\n",
    "    )   \n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139c771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 780 directories with both video and depth latents\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/azhuravl/work/TrajectoryCrafter/notebooks/05_11_25_training/lora_utils_ours')\n",
    "\n",
    "from dataset_latents_double import LatentsDataset\n",
    "\n",
    "dataset = LatentsDataset(\n",
    "    data_dir='/home/azhuravl/scratch/datasets_latents/monkaa_1000',\n",
    "    use_depth_latents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017d582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbde3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 16, 48, 84])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0['depth_latents']['cond_video_latents'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc33468f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  1.4118,  2.8235,  4.2353,  5.6471,  7.0588,  8.4706,  9.8824,\n",
       "        11.2941, 12.7059, 14.1176, 15.5294, 16.9412, 18.3529, 19.7647, 21.1765,\n",
       "        22.5882, 24.0000, 25.4118, 26.8235, 28.2353, 29.6471, 31.0588, 32.4706,\n",
       "        33.8824, 35.2941, 36.7059, 38.1176, 39.5294, 40.9412, 42.3529, 43.7647,\n",
       "        45.1765, 46.5882, 48.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 49-1, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7e0588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_latents_10_frames torch.Size([1, 3, 16, 48, 84])\n",
      "ref_latents_20_frames torch.Size([1, 5, 16, 48, 84])\n",
      "ref_latents_30_frames torch.Size([1, 8, 16, 48, 84])\n",
      "ref_latents_35_frames torch.Size([1, 9, 16, 48, 84])\n",
      "ref_latents_40_frames torch.Size([1, 10, 16, 48, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1395950/3041759288.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latents = torch.load('/home/azhuravl/scratch/datasets_latents/monkaa_ref_latents/001/ref_videos_latents.pt')\n"
     ]
    }
   ],
   "source": [
    "# read and print the keys with shapes of /home/azhuravl/scratch/datasets_latents/monkaa_ref_latents/001/ref_videos_latents.pt\n",
    "import torch\n",
    "\n",
    "latents = torch.load('/home/azhuravl/scratch/datasets_latents/monkaa_ref_latents/001/ref_videos_latents.pt')\n",
    "for k, v in latents.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832f9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4849fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load this /home/azhuravl/work/TrajectoryCrafter/experiments/08-11-2025/12-11-14/snapshot_0030.pickle\n",
    "\n",
    "import pickle\n",
    "with open('/home/azhuravl/work/TrajectoryCrafter/experiments/08-11-2025/12-11-14/snapshot_0030.pickle', 'rb') as f:\n",
    "    snapshot = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6b649e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# save snapshot as json\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/azhuravl/work/TrajectoryCrafter/experiments/08-11-2025/12-11-14/snapshot_0030.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/CT/video_4d_recon/nobackup/conda_envs/trajcrafter/lib/python3.10/json/encoder.py:381\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    379\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m item_separator\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m _encoder(key)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m _key_separator\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "# save snapshot as json\n",
    "with open('/home/azhuravl/work/TrajectoryCrafter/experiments/08-11-2025/12-11-14/snapshot_0030.json', 'w') as f:\n",
    "    json.dump(snapshot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f867c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.linspace(0, 49-1, 49).long()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
